{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "So far, we've looked at a ton of neat things that we can do with images: filtering, edge detection, stitching, segmentation, resizing, detection, and optical flow. Look at how far we've come!\n",
    "\n",
    "In this assignment, we'll explore some of the geometry that underlies how these images are formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def hash_numpy(x):\n",
    "    import hashlib\n",
    "\n",
    "    return hashlib.sha1(x.view(np.uint8)).hexdigest()\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transformations in 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sense of how objects in our world are rendered in a camera, we typically need to understand how they are located relative to the camera. In this question, we'll examine some properties of the transformations that formalize this process by expressing coordinates with respect to multiple frames. \n",
    "\n",
    "We'll be considering a scene with two frames: a world frame ($W$) and a camera frame ($C$).\n",
    "\n",
    "Notice that:\n",
    "- We have 3D points $p$, $q$, $r$, and $s$ that define a square, which is parallel to the world $zy$ plane\n",
    "- $C_z$ and $C_x$ belong to the plane defined by $W_z$ and $W_x$\n",
    "- $C_y$ is parallel to $W_y$\n",
    "\n",
    "<!-- into camera space as we translate a simple shape from world coordinates to camera coordinates. We will take this square in world coordinates and transform it into the camera coordinates.-->\n",
    "\n",
    "<img src=\"images/projection_geometry.png\" alt=\"projection geometry figure\" width=\"640\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reference Frame Definitions\n",
    "\n",
    "First, we'll take a moment to validate your understanding of 3D reference frames.\n",
    "\n",
    "Consider creating:\n",
    "- A point $w$ at the origin of the world frame ($O_w$)\n",
    "- A point $c$ at the origin of the camera frame ($O_c$)\n",
    "\n",
    "Examine the $x$, $y$, and $z$ axes of each frame, then express these points with respect to the world and camera frames. Fill in **`w_wrt_camera`**, **`w_wrt_world`**, and **`c_wrt_camera`**.\n",
    "\n",
    "You can consider the length $d = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1.0\n",
    "\n",
    "# Abbreviation note:\n",
    "# - \"wrt\" stands for \"with respect to\", which is ~synonymous with \"relative to\"\n",
    "\n",
    "w_wrt_world = np.array([0.0, 0.0, 0.0])  # Done for you\n",
    "w_wrt_camera = None  # Assign me!\n",
    "\n",
    "c_wrt_world = None  # Assign me!\n",
    "c_wrt_camera = None  # Assign me!\n",
    "\n",
    "### YOUR CODE HERE\n",
    "pass\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check your answers!\n",
    "assert (\n",
    "    (3,)\n",
    "    == w_wrt_world.shape\n",
    "    == w_wrt_camera.shape\n",
    "    == c_wrt_world.shape\n",
    "    == c_wrt_camera.shape\n",
    "), \"Wrong shape!\"\n",
    "assert (\n",
    "    hash_numpy(w_wrt_world) == \"d3399b7262fb56cb9ed053d68db9291c410839c4\"\n",
    "), \"Double check your w_wrt_world!\"\n",
    "assert (\n",
    "    hash_numpy(w_wrt_camera) == \"6248a1dcfe0c8822ba52527f68f7f98955584277\"\n",
    "), \"Double check your w_wrt_camera!\"\n",
    "assert (\n",
    "    hash_numpy(c_wrt_camera) == \"d3399b7262fb56cb9ed053d68db9291c410839c4\"\n",
    "), \"Double check your c_wrt_camera!\"\n",
    "assert (\n",
    "    hash_numpy(c_wrt_world) == \"a4c525cd853a072d96cade8b989a9eaf1e13ed3d\"\n",
    "), \"Double check your c_wrt_world!\"\n",
    "\n",
    "print(\"Looks correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 World ⇨ Camera Transforms\n",
    "\n",
    "Derive the homogeneous transformation matrix needed to convert a point expressed with respect to the world frame $W$ in the camera frame $C$.\n",
    "\n",
    "**Discuss the rotation and translation terms in this matrix and how you determined them, then implement it in `camera_from_world_transform()`**.\n",
    "\n",
    "We've also supplied a set of `assert` statements below to help you check your work.\n",
    "\n",
    "---\n",
    "\n",
    "*Hint #1:*\n",
    "With rotation matrix $R \\in \\mathbb{R}^{3\\times 3}$ and translation vector $t \\in \\mathbb{R}^{3\\times 1}$, you can write transformations as $4 \\times 4$ matrices: \n",
    "$$\n",
    "\\begin{bmatrix}{x_C} \\\\ {y_C} \\\\ {z_C} \\\\ 1\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    R & t \\\\\n",
    "    \\vec{0}^\\top & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}{x_W} \\\\ {y_W} \\\\ {z_W} \\\\ 1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "*Hint #2: Remember our 2D transformation matrix for rotations in the $xy$ plane.*\n",
    "\n",
    "$$\\begin{bmatrix}{x}' \\\\ {y}'\\end{bmatrix} = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta)\\end{bmatrix} \\begin{bmatrix}{x} \\\\ {y}\\end{bmatrix}$$ \n",
    "\n",
    "To apply this to 3D rotations, you might think of this $xy$ plane rotation as holding the $z$ coordinate constant, since that's the axis you're rotating around, and transforming the $x$ and $y$ coordinates as described in the 2D formulation:\n",
    "\n",
    "$$\\begin{bmatrix}{x}' \\\\ {y}' \\\\ {z}'\\end{bmatrix}  = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) & 0 \\\\ \\sin(\\theta) & \\cos(\\theta) & 0 \\\\ 0 & 0 & 1\\end{bmatrix} \\begin{bmatrix}{x} \\\\ {y} \\\\ {z}\\end{bmatrix}$$\n",
    "\n",
    "(Alternatively you could simply take the rotation matrix from the [Wikipedia](https://en.wikipedia.org/wiki/Rotation_matrix) page)\n",
    "\n",
    "*Hint #3: In a homogeneous transform, the translation is applied after the rotation.*\n",
    "\n",
    "As a result, you can visualize the translation as an offset in the output frame.\n",
    "\n",
    "The order matters! You'll end up with a different transformation if you translate and then rotate versus if you rotate first and then translate with the same offsets. In lecture 2 we discussed a formulation for a combinated scaling, rotating, and translating matrix (in that order), which can be a useful starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Your response here:** Write your answer in this markdown cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your answer against 1.1!\n",
    "from cameras import camera_from_world_transform\n",
    "\n",
    "T_camera_from_world = camera_from_world_transform()\n",
    "\n",
    "# Check c_wrt_camera against T_camera_from_world @ w_wrt_world\n",
    "w_wrt_camera_computed = (T_camera_from_world @ np.append(w_wrt_world, 1.0))[:3]\n",
    "print(f\"w_wrt camera: expected {w_wrt_camera}, computed {w_wrt_camera_computed}\")\n",
    "assert np.allclose(\n",
    "    w_wrt_camera, w_wrt_camera_computed\n",
    "), \"Error! (likely bad translation)\"\n",
    "print(\"Translation components look reasonable!\")\n",
    "\n",
    "# Check w_wrt_camera against T_camera_from_world @ c_wrt_world\n",
    "c_wrt_camera_computed = (T_camera_from_world @ np.append(c_wrt_world, 1.0))[:3]\n",
    "print(f\"c_wrt camera: expected {c_wrt_camera}, computed {c_wrt_camera_computed}\")\n",
    "assert np.allclose(\n",
    "    c_wrt_camera, c_wrt_camera_computed\n",
    "), \"Error! (likely bad rotation)\"\n",
    "print(\"Rotation components looks reasonable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preserving Edge Orientations (Geometric Intuition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the translation and rotation transformation from world coordinates to camera coordinates, which, if any, of the edges of the square retain their orientation and why? \n",
    "\n",
    "For those that change orientation, how do they change? (e.g. translation x,y,z and rotation in one of our planes). \n",
    "\n",
    "A sentence or two of geometric intuition is sufficient for each question, such as reasoning about the orientation of the edges and which axes we're rotating and translating about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Your response here:** Write your answer in this markdown cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Preserving Edge Orientations (Mathematical Proof)\n",
    "\n",
    "We'll now connect this geometric intuition to your transformation matrix. Framing transformations as matrix multiplication is useful because it allows us to rewrite the difference between two transformed points as the transformation of the difference between the original points. For example, take points $a$ and $b$ and a transformation matrix $T$: $Ta - Tb = T(a-b)$.\n",
    "\n",
    "All of the edges in the $p,q,r,s$ square are axis-aligned, which means each edge has a nonzero component on only one axis. Assume that the square is 1 by 1, and apply your transformation to the edge vectors $bottom=q-p$ and $left=s-p$ to show which of these edges rotate and how.\n",
    "\n",
    "*Notation:*\n",
    "You can apply the transformation to vectors representing the direction of each edge. If we transform all 4 corners, then the vector representing the direction of the transformed square's bottom is:\n",
    "$$\\begin{bmatrix}{bottom_x}' \\\\ {bottom_y}' \\\\ {bottom_z}' \\\\ 0\\end{bmatrix} = T\\begin{bmatrix}{q_x} \\\\ {q_y} \\\\ {q_z} \\\\ 1\\end{bmatrix} -T\\begin{bmatrix}{p_x} \\\\ {p_y} \\\\ {p_z} \\\\ 1\\end{bmatrix}$$\n",
    "\n",
    "Using matrix rules, we can rewrite this in terms of the edges of the original square\n",
    "$$\\begin{bmatrix}{bottom_x}' \\\\ {bottom_y}' \\\\ {bottom_z}' \\\\ 0\\end{bmatrix} = T\\begin{bmatrix}{q_x}-p_x \\\\ {q_y}-p_y \\\\ {q_z}-p_z \\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "**Eliminate the $q - p$ components that you know to be 0, and then apply your transformation to obtain the vector $bottom' = q' - p'$ defined above. Do the same for $left' = s' -p'$. Which edge rotated, and which one didn't?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Your response here:** Write your answer in this markdown cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interesting note:* This may remind you of eigenvectors: one of these edges (the one that doesn't rotate) is an eigenvector of our transformation matrix!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement **`apply_transform()`** to help us apply a homogeneous transformation to a batch of points.\n",
    "\n",
    "Then, run the cell below to start visualizing our frames and the world square in PyPlot!\n",
    "\n",
    "Using your code, we can animate a GIF that shows the transition of the square from its position in world coordinates to a new position in camera coordinates. We transform the perspective continuously from the world coordinate system to the camera coordinate system. Analogous to a homogeneous transform, you can see that we first rotate to match the orientation of the camera coordinate system, then translate to match the position of the camera origin. \n",
    "\n",
    "If you want to see how the animation was computed or if you want to play around with its configuration, then check out **`animate_transformation`** in **`utils.py`**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cameras import apply_transform\n",
    "from utils import (\n",
    "    animate_transformation,\n",
    "    configure_ax,\n",
    "    plot_frame,\n",
    "    plot_square,\n",
    ")\n",
    "\n",
    "# Vertices per side of the square\n",
    "N = 2\n",
    "\n",
    "# Compute vertices corresponding to each side of the square\n",
    "vertices_wrt_world = np.concatenate(\n",
    "    [\n",
    "        np.vstack([np.zeros(N), np.linspace(1, 2, N), np.ones(N)]),\n",
    "        np.vstack([np.zeros(N), np.ones(N) + 1, np.linspace(1, 2, N)]),\n",
    "        np.vstack([np.zeros(N), np.linspace(2, 1, N), np.ones(N) + 1]),\n",
    "        np.vstack([np.zeros(N), np.ones(N), np.linspace(1, 2, N)]),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Visualize our rotation!\n",
    "animate_transformation(\n",
    "    \"transformation.gif\",\n",
    "    vertices_wrt_world,\n",
    "    camera_from_world_transform,\n",
    "    apply_transform,\n",
    ")\n",
    "\n",
    "import IPython.display\n",
    "\n",
    "with open(\"transformation.gif\", \"rb\") as file:\n",
    "    display(IPython.display.Image(file.read()))\n",
    "\n",
    "# Uncomment to compare to staff animation\n",
    "# with open(\"solution_transformation.gif\", \"rb\") as file:\n",
    "#     display(IPython.display.Image(file.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Camera Intrinsics & Vanishing Points\n",
    "\n",
    "In a pinhole camera, lines that are parallel in 3D rarely remain parallel when projected to the image plane. Instead, parallel lines will meet at a **vanishing point**:\n",
    "\n",
    "<img alt=\"vanishing point\" src=\"images/vanishing_point.jpg\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Homogeneous coordinates (5 points)\n",
    "\n",
    "Consider a line that is parallel to a world-space direction vector in the set $\\{ d \\in \\mathbb{R}^3 : d^\\top d = 1\\}$. Show that the image coordinates $v$ of the vanishing point can be be written as $v = KRd$.\n",
    "\n",
    "*Hints:*\n",
    "- As per the lecture slides, $K$ is the camera calibration matrix and $R$ is the camera extrinsic rotation.\n",
    "- As in the diagram above, the further a point on a 3D line is from the camera origin, the closer its projection will be to the line's 2D vanishing point.\n",
    "- Given a line with direction vector $d$, you can write a point that's infinitely far away from the camera via a limit: $\\lim_{\\alpha \\to \\infty} \\alpha d$.\n",
    "- The 3D homogeneous coordinate definition is:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    x & y & z & w\n",
    "\\end{bmatrix}^\\top\n",
    "\\iff\n",
    "\\begin{bmatrix}\n",
    "    x/w & y/w & z/w & 1\n",
    "\\end{bmatrix}^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You answer here:** Write your answer in this markdown cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Calibration from vanishing points (5 points)\n",
    "\n",
    "Let $d_0, d_1, \\dots$ represent directional vectors for 3D lines in a scene, and $v_0, v_1, \\dots$ represent their corresponding vanishing points. \n",
    "\n",
    "Consider the situtation when these lines are orthogonal:\n",
    "$$\n",
    "d_i^\\top d_j = 0, \\text{for each } i \\neq j\n",
    "$$\n",
    "\n",
    "Show that:\n",
    "$$\n",
    "(K^{-1}v_i)^\\top(K^{-1}v_j) = 0, \\text{for each } i \\neq j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You answer here:** Write your answer in this markdown cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Short Response (5 points)\n",
    "\n",
    "Respond to the following using bullet points:\n",
    "\n",
    "- In the section above, we eliminated the extrinsic rotation matrix $R$. Why might this simplify camera calibration?\n",
    "\n",
    "- Assuming square pixels and no skew, how many vanishing points with mutually orthogonal directions do we now need to solve for our camera's focal length and optical center?\n",
    "\n",
    "- Assuming square pixels and no skew, how many vanishing points with mutually orthogonal directions do we now need to solve for our camera's focal length when the optical center is known?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**You answer here:** Write your answer in this markdown cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Intrinsic Calibration\n",
    "\n",
    "Using the vanishing point math from above, we can solve for a camera matrix $K$!\n",
    "\n",
    "First, let's load in an image. To make life easier for you, we've hand labeled a set of coordinates on it that we'll use to compute vanishing points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and annotated points; note that:\n",
    "# > Our image is a PIL image type; you can convert this to NumPy with `np.asarray(img)`\n",
    "# > Points are in (x, y) format, which corresponds to (col, row)!\n",
    "img = Image.open(\"images/pressure_cooker.jpg\")\n",
    "print(f\"Image is {img.width} x {img.height}\")\n",
    "points = np.array(\n",
    "    [\n",
    "        [270.0, 327.0],  # [0]\n",
    "        [356.0, 647.0],  # [1]\n",
    "        [610.0, 76.0],  # [2]\n",
    "        [706.0, 857.0],  # [3]\n",
    "        [780.0, 585.0],  # [4]\n",
    "        [1019.0, 226.0],  # [5]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Visualize image & annotated points\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.imshow(img)\n",
    "ax.scatter(points[:, 0], points[:, 1], color=\"white\", marker=\"x\")\n",
    "for i in range(len(points)):\n",
    "    ax.annotate(\n",
    "        f\"points[{i}]\",\n",
    "        points[i] + np.array([15.0, 5.0]),\n",
    "        color=\"white\",\n",
    "        backgroundcolor=(0, 0, 0, 0.15),\n",
    "        zorder=0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Finding Vanishing Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2D, notice that a vanishing point can be computing by finding the intersection of two lines that we know are parallel in 3D.\n",
    "\n",
    "To find the vanishing points in the image, implement **`intersection_from_lines()`**.\n",
    "\n",
    "Then, run the cell below to check that it's working.\n",
    "\n",
    "Note that later parts of this homework will fail if you choose the side face instead of the front face for producing the leftmost vanishing point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cameras import intersection_from_lines\n",
    "\n",
    "# Python trivia: the following two assert statements are the same.\n",
    "# > https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists\n",
    "# > https://numpy.org/doc/stable/reference/arrays.indexing.html#integer-array-indexing\n",
    "assert np.allclose(\n",
    "    intersection_from_lines(points[0], points[1], points[4], points[0],),\n",
    "    points[0],\n",
    ")\n",
    "assert np.allclose(intersection_from_lines(*points[[0, 1, 4, 0]]), points[0])\n",
    "print(\"Looks correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the constraint we derived above, we need to find vanishing points that correspond to three orthogonal direction vectors.\n",
    "\n",
    "Populate `v0_indices`, `v1_indices`, and `v2_indices`, then run the cell below to compute `v`.\n",
    "\n",
    "You should be able to get an output that looks like this (color ordering does not matter):\n",
    "\n",
    "![vanishing point reference](images/vanishing_point_reference.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select points used to compute each vanishing point\n",
    "#\n",
    "# Each `v*_indices` list should contain four integers, corresponding to\n",
    "# indices into the `points` array; the first two ints define one line and\n",
    "# the second two define another line.\n",
    "v0_indices = None\n",
    "v1_indices = None\n",
    "v2_indices = None\n",
    "\n",
    "### YOUR CODE HERE\n",
    "pass\n",
    "### END YOUR CODE\n",
    "\n",
    "# Validate indices\n",
    "assert (\n",
    "    len(v0_indices) == len(v1_indices) == len(v2_indices) == 4\n",
    "), \"Invalid length!\"\n",
    "for i, j, k in zip(v0_indices, v1_indices, v2_indices):\n",
    "    assert type(i) == type(j) == type(k) == int, \"Invalid type!\"\n",
    "\n",
    "# Compute vanishing points\n",
    "v = np.zeros((3, 2))\n",
    "v[:, :2] = np.array(\n",
    "    [\n",
    "        intersection_from_lines(*points[v0_indices]),\n",
    "        intersection_from_lines(*points[v1_indices]),\n",
    "        intersection_from_lines(*points[v2_indices]),\n",
    "    ]\n",
    ")\n",
    "assert v.shape == (3, 2)\n",
    "\n",
    "# Display image\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.imshow(img)\n",
    "\n",
    "# Display annotated points\n",
    "ax.scatter(points[:, 0], points[:, 1], color=\"white\")\n",
    "\n",
    "# Visualize vanishing points\n",
    "colors = [\"red\", \"green\", \"blue\"]\n",
    "for indices, color in zip((v0_indices, v1_indices, v2_indices), colors):\n",
    "    ax.axline(*points[indices[:2]], zorder=0.1, c=color, alpha=0.4)\n",
    "    ax.axline(*points[indices[2:]], zorder=0.1, c=color, alpha=0.4)\n",
    "ax.scatter(v[:, 0], v[:, 1], c=colors)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Computing Optical Centers\n",
    "\n",
    "Next, implement **`optical_center_from_vanishing_points()`** to compute the 2D optical center from our vanishing points. Then, run the cell below to compute a set of optical center coordinates from our vanishing points.\n",
    "\n",
    "*Hint:* Property 3 from [1] may be useful. (Try connecting to Stanford campus network otherwise the paper link might not work for you.)\n",
    "> [1] Caprile, B., Torre, V. **Using vanishing points for camera calibration.** *Int J Comput Vision 4, 127–139 (1990)*. https://doi.org/10.1007/BF00127813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cameras import optical_center_from_vanishing_points\n",
    "\n",
    "optical_center = optical_center_from_vanishing_points(v[0], v[1], v[2],)\n",
    "\n",
    "assert np.allclose(np.mean(optical_center), 583.4127277436276)\n",
    "assert np.allclose(np.mean(optical_center ** 2), 343524.39942528843)\n",
    "print(\"Looks correct!\")\n",
    "\n",
    "# Display image\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.imshow(img)\n",
    "\n",
    "# Display optical center\n",
    "ax.scatter(*optical_center, color=\"yellow\")\n",
    "ax.annotate(\n",
    "    \"Optical center\",\n",
    "    optical_center + np.array([20, 5]),\n",
    "    color=\"white\",\n",
    "    backgroundcolor=(0, 0, 0, 0.5),\n",
    "    zorder=0.1,\n",
    ")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Computing Focal Lengths\n",
    "\n",
    "Consider two vanishing points corresponding to orthogonal directions, and the constraint from above:\n",
    "\n",
    "$$\n",
    "(K^{-1}v_0)^\\top(K^{-1}v_1) = 0, \\text{for each } i \\neq j\n",
    "$$\n",
    "\n",
    "Derive an expression for computing the focal length when the optical center is known, then implement **`focal_length_from_two_vanishing_points()`**.\n",
    "\n",
    "When we assume square pixels and no skew, recall that the intrinsic matrix $K$ is:\n",
    "\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "    f & 0 & c_x \\\\\n",
    "    0 & f & c_y \\\\\n",
    "    0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "*Hint:* Optional, but this problem maybe be simpler if you factorize $K$ as:\n",
    "\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "    1 & 0 & c_x \\\\\n",
    "    0 & 1 & c_y \\\\\n",
    "    0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    f & 0 & 0 \\\\\n",
    "    0 & f & 0 \\\\\n",
    "    0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "When working with homogeneous coordinates, note that the lefthand matrix is a simple translation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cameras import focal_length_from_two_vanishing_points\n",
    "\n",
    "# If your implementation is correct, these should all be ~the same\n",
    "f = focal_length_from_two_vanishing_points(v[0], v[1], optical_center)\n",
    "print(f\"Focal length from v0, v1: {f}\")\n",
    "f = focal_length_from_two_vanishing_points(v[1], v[2], optical_center)\n",
    "print(f\"Focal length from v1, v2: {f}\")\n",
    "f = focal_length_from_two_vanishing_points(v[0], v[2], optical_center)\n",
    "print(f\"Focal length from v0, v2: {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Comparison to EXIF data\n",
    "\n",
    "To validate our focal length computation, one smoke test we can run is compare it to parameters supplied by the camera manufacturer.\n",
    "\n",
    "In JPEG images, these parameters and other metadata are sometimes stored using [EXIF](https://en.wikipedia.org/wiki/Exif) tags that are written when the photo is taken. Run the cell below to read & print some of this using the Python Imaging Library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL.ExifTags import TAGS\n",
    "\n",
    "# Grab EXIF data\n",
    "exif = {TAGS[key]: value for key, value in img._getexif().items()}\n",
    "\n",
    "# Print subset of keys\n",
    "print(f\"EXIF data for {img.filename}\\n=====\")\n",
    "for key in (\n",
    "    \"DateTimeOriginal\",\n",
    "    \"FocalLength\",\n",
    "    \"GPSInfo\",\n",
    "    \"Make\",\n",
    "    \"Model\",\n",
    "):\n",
    "    print(key.ljust(25), exif[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we see that the focal length of our camera system is **`4.3mm`**.\n",
    "\n",
    "Focal lengths are typically in millimeters, but all of the coordinates we've worked with thus far have been in pixel-space. Thus, we first need to convert our focal length from pixels to millimeters.\n",
    "\n",
    "Try to visualize this conversion, then implement **`physical_focal_length_from_calibration()`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cameras import physical_focal_length_from_calibration\n",
    "\n",
    "# Length across sensor diagonal for SM-G970U (Galaxy S10e)\n",
    "# > https://en.wikipedia.org/wiki/Samsung_CMOS\n",
    "sensor_diagonal_mm = 7.06\n",
    "\n",
    "# Length across image diagonal\n",
    "image_diagonal_pixels = np.sqrt(img.width ** 2 + img.height ** 2)\n",
    "\n",
    "f_mm = physical_focal_length_from_calibration(\n",
    "    f, sensor_diagonal_mm, image_diagonal_pixels,\n",
    ")\n",
    "print(f\"Computed focal length:\".ljust(30), f_mm)\n",
    "\n",
    "error = np.abs(f_mm - 4.3) / 4.3\n",
    "print(\"Calibration vs spec error:\".ljust(30), f\"{error * 100:.2f}%\")\n",
    "assert 0.06 < error < 0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Analysis (5 points)\n",
    "\n",
    "If everything went smoothly, your computed focal length should only deviate from the manufacturer spec by ~6.8%.\n",
    "\n",
    "Aside from manufacturing tolerances, name two or more other possible causes for this error, then discuss the limitations of this calibration method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You answer here:** Write your answer in this markdown cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Extra Credit\n",
    "\n",
    "You can choose to attempt both, either, or neither! Each will count for up to 0.5% of your grade (1% total).\n",
    "\n",
    "**a) Projection**\n",
    "\n",
    "Generate a set of geometric shapes: cylinders, cubes, spheres, etc. Then, use your calibrated intrinsics to render them into the scene (i.e. overlaying them onto the image from Q3) with correct perspective. \n",
    "\n",
    "These can be simple wireframe representations (eg `plt.plot()`); no need for fancy graphics.\n",
    "\n",
    "\n",
    "**b) Extrinsinc Calibration** (*Hard, possibly requires a lot of Google*)\n",
    "\n",
    "Given that our box is 340mm (L) x 310mm (W) x 320mm (H), compute a 3D transformation (position, orientation) between the center of the box and the camera. In your submission, describe your approach and verify it by using both your calibrated extrinsics and intrinsics to overlay the image with a wireframe version of the box."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
