{
 "nbformat_minor": 2, 
 "nbformat": 4, 
 "cells": [
  {
   "source": [
    "# Homework 6\n", 
    "*This notebook includes both coding and written questions. Please hand in this notebook file with all the outputs as a pdf on gradescope for \"HW6 pdf\". Upload the three files of code (`compression.py`, `k_nearest_neighbor.py` and `features.py`) on gradescope for \"HW6 code\".*\n", 
    "\n", 
    "This assignment covers:\n", 
    "- image compression using SVD\n", 
    "- kNN methods for image recognition.\n", 
    "- PCA and LDA to improve kNN"
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 1, 
   "cell_type": "code", 
   "source": [
    "# Setup\n", 
    "from time import time\n", 
    "from collections import defaultdict\n", 
    "\n", 
    "import numpy as np\n", 
    "import matplotlib.pyplot as plt\n", 
    "from matplotlib import rc\n", 
    "from skimage import io\n", 
    "\n", 
    "%matplotlib inline\n", 
    "plt.rcParams['figure.figsize'] = (15.0, 12.0) # set default size of plots\n", 
    "plt.rcParams['image.interpolation'] = 'nearest'\n", 
    "plt.rcParams['image.cmap'] = 'gray'\n", 
    "\n", 
    "# for auto-reloading external modules\n", 
    "%load_ext autoreload\n", 
    "%autoreload 2"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "source": [
    "## Part 1 - Image Compression (15 points)\n", 
    "\n", 
    "Image compression is used to reduce the cost of storage and transmission of images (or videos).\n", 
    "One lossy compression method is to apply Singular Value Decomposition (SVD) to an image, and only keep the top n singular values."
   ], 
   "cell_type": "markdown", 
   "metadata": {
    "collapsed": true
   }
  }, 
  {
   "execution_count": 2, 
   "cell_type": "code", 
   "source": [
    "image = io.imread('pitbull.jpg', as_grey=True)\n", 
    "plt.imshow(image)\n", 
    "plt.axis('off')\n", 
    "plt.show()"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "source": [
    "Let's implement image compression using SVD.  \n", 
    "We first compute the SVD of the image, and as seen in class we keep the `n` largest singular values and singular vectors to reconstruct the image.\n", 
    "\n", 
    "Implement function `compress_image` in `compression.py`."
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 3, 
   "cell_type": "code", 
   "source": [
    "from compression import compress_image\n", 
    "\n", 
    "compressed_image, compressed_size = compress_image(image, 100)\n", 
    "compression_ratio = compressed_size / image.size\n", 
    "print('Original image shape:', image.shape)\n", 
    "print('Compressed size: %d' % compressed_size)\n", 
    "print('Compression ratio: %.3f' % compression_ratio)\n", 
    "\n", 
    "assert compressed_size == 298500"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "execution_count": 4, 
   "cell_type": "code", 
   "source": [
    "# Number of singular values to keep\n", 
    "n_values = [10, 50, 100]\n", 
    "\n", 
    "for n in n_values:\n", 
    "    # Compress the image using `n` singular values\n", 
    "    compressed_image, compressed_size = compress_image(image, n)\n", 
    "    \n", 
    "    compression_ratio = compressed_size / image.size\n", 
    "\n", 
    "    print(\"Data size (original): %d\" % (image.size))\n", 
    "    print(\"Data size (compressed): %d\" % compressed_size)\n", 
    "    print(\"Compression ratio: %f\" % (compression_ratio))\n", 
    "\n", 
    "\n", 
    "\n", 
    "    plt.imshow(compressed_image, cmap='gray')\n", 
    "    title = \"n = %s\" % n\n", 
    "    plt.title(title)\n", 
    "    plt.axis('off')\n", 
    "    plt.show()"
   ], 
   "outputs": [], 
   "metadata": {
    "scrolled": false
   }
  }, 
  {
   "source": [
    "## Face Dataset\n", 
    "\n", 
    "We will use a dataset of faces of celebrities. Download the dataset using the following command:\n", 
    "\n", 
    "    sh get_dataset.sh\n", 
    "\n", 
    "The face dataset for CS131 assignment.\n", 
    "The directory containing the dataset has the following structure:\n", 
    "\n", 
    "    faces/\n", 
    "        train/\n", 
    "            angelina jolie/\n", 
    "            anne hathaway/\n", 
    "            ...\n", 
    "        test/\n", 
    "            angelina jolie/\n", 
    "            anne hathaway/\n", 
    "            ...\n", 
    "\n", 
    "Each class has 50 training images and 10 testing images."
   ], 
   "cell_type": "markdown", 
   "metadata": {
    "collapsed": true
   }
  }, 
  {
   "execution_count": 5, 
   "cell_type": "code", 
   "source": [
    "from utils import load_dataset\n", 
    "\n", 
    "X_train, y_train, classes_train = load_dataset('faces', train=True, as_grey=True)\n", 
    "X_test, y_test, classes_test = load_dataset('faces', train=False, as_grey=True)\n", 
    "\n", 
    "assert classes_train == classes_test\n", 
    "classes = classes_train\n", 
    "\n", 
    "print('Class names:', classes)\n", 
    "print('Training data shape:', X_train.shape)\n", 
    "print('Training labels shape: ', y_train.shape)\n", 
    "print('Test data shape:', X_test.shape)\n", 
    "print('Test labels shape: ', y_test.shape)"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "execution_count": 6, 
   "cell_type": "code", 
   "source": [
    "# Visualize some examples from the dataset.\n", 
    "# We show a few examples of training images from each class.\n", 
    "num_classes = len(classes)\n", 
    "samples_per_class = 10\n", 
    "for y, cls in enumerate(classes):\n", 
    "    idxs = np.flatnonzero(y_train == y)\n", 
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n", 
    "    for i, idx in enumerate(idxs):\n", 
    "        plt_idx = i * num_classes + y + 1\n", 
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n", 
    "        plt.imshow(X_train[idx])\n", 
    "        plt.axis('off')\n", 
    "        if i == 0:\n", 
    "            plt.title(y)\n", 
    "plt.show()"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "execution_count": 7, 
   "cell_type": "code", 
   "source": [
    "# Flatten the image data into rows\n", 
    "# we now have one 4096 dimensional featue vector for each example\n", 
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n", 
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n", 
    "print(\"Training data shape:\", X_train.shape)\n", 
    "print(\"Test data shape:\", X_test.shape)"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "source": [
    "---\n", 
    "## Part 2 - k-Nearest Neighbor (30 points)\n", 
    "\n", 
    "We're now going to try to classify the test images using the k-nearest neighbors algorithm on the **raw features of the images** (i.e. the pixel values themselves). We will see later how we can use kNN on better features.\n", 
    "\n", 
    "Here are the steps that we will follow:\n", 
    "\n", 
    "1. We compute the L2 distances between every element of X_test and every element of X_train in `compute_distances`.\n", 
    "2. We split the dataset into 5 folds for cross-validation in `split_folds`.\n", 
    "3. For each fold, and for different values of `k`, we predict the labels and measure accuracy.\n", 
    "4. Using the best `k` found through cross-validation, we measure accuracy on the test set."
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 8, 
   "cell_type": "code", 
   "source": [
    "from k_nearest_neighbor import compute_distances\n", 
    "\n", 
    "# Step 1: compute the distances between all features from X_train and from X_test\n", 
    "dists = compute_distances(X_test, X_train)\n", 
    "assert dists.shape == (160, 800)\n", 
    "print(\"dists shape:\", dists.shape)"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "execution_count": 9, 
   "cell_type": "code", 
   "source": [
    "from k_nearest_neighbor import predict_labels\n", 
    "\n", 
    "# We use k = 1 (which corresponds to only taking the nearest neighbor to decide)\n", 
    "y_test_pred = predict_labels(dists, y_train, k=1)\n", 
    "\n", 
    "# Compute and print the fraction of correctly predicted examples\n", 
    "num_test = y_test.shape[0]\n", 
    "num_correct = np.sum(y_test_pred == y_test)\n", 
    "accuracy = float(num_correct) / num_test\n", 
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "source": [
    "### Cross-Validation\n", 
    "\n", 
    "We don't know the best value for our parameter `k`.  \n", 
    "There is no theory on how to choose an optimal `k`, and the way to choose it is through cross-validation.\n", 
    "\n", 
    "We **cannot** compute any metric on the test set to choose the best `k`, because we want our final test accuracy to reflect a real use case. This real use case would be a setting where we have new examples come and we classify them on the go. There is no way to check the accuracy beforehand on that set of test examples to determine `k`.\n", 
    "\n", 
    "Cross-validation will make use split the data into different fold (5 here).  \n", 
    "For each fold, if we have a total of 5 folds we will have:\n", 
    "- 80% of the data as training data\n", 
    "- 20% of the data as validation data\n", 
    "\n", 
    "We will compute the accuracy on the validation accuracy for each fold, and use the mean of these 5 accuracies to determine the best parameter `k`."
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 10, 
   "cell_type": "code", 
   "source": [
    "from k_nearest_neighbor import split_folds\n", 
    "\n", 
    "# Step 2: split the data into 5 folds to perform cross-validation.\n", 
    "num_folds = 5\n", 
    "\n", 
    "X_trains, y_trains, X_vals, y_vals = split_folds(X_train, y_train, num_folds)\n", 
    "\n", 
    "assert X_trains.shape == (5, 640, 4096)\n", 
    "assert y_trains.shape == (5, 640)\n", 
    "assert X_vals.shape == (5, 160, 4096)\n", 
    "assert y_vals.shape == (5, 160)"
   ], 
   "outputs": [], 
   "metadata": {
    "collapsed": true
   }
  }, 
  {
   "execution_count": 11, 
   "cell_type": "code", 
   "source": [
    "# Step 3: Measure the mean accuracy for each value of `k`\n", 
    "\n", 
    "# List of k to choose from\n", 
    "k_choices = list(range(5, 101, 5))\n", 
    "\n", 
    "# Dictionnary mapping k values to accuracies\n", 
    "# For each k value, we will have `num_folds` accuracies to compute\n", 
    "# k_to_accuracies[1] will be for instance [0.22, 0.23, 0.19, 0.25, 0.20] for 5 folds\n", 
    "k_to_accuracies = {}\n", 
    "\n", 
    "for k in k_choices:\n", 
    "    print(\"Running for k=%d\" % k)\n", 
    "    accuracies = []\n", 
    "    for i in range(num_folds):\n", 
    "        # Make predictions\n", 
    "        fold_dists = compute_distances(X_vals[i], X_trains[i])\n", 
    "        y_pred = predict_labels(fold_dists, y_trains[i], k)\n", 
    "\n", 
    "        # Compute and print the fraction of correctly predicted examples\n", 
    "        num_correct = np.sum(y_pred == y_vals[i])\n", 
    "        accuracy = float(num_correct) / len(y_vals[i])\n", 
    "        accuracies.append(accuracy)\n", 
    "        \n", 
    "    k_to_accuracies[k] = accuracies\n"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "execution_count": 12, 
   "cell_type": "code", 
   "source": [
    "# plot the raw observations\n", 
    "for k in k_choices:\n", 
    "    accuracies = k_to_accuracies[k]\n", 
    "    plt.scatter([k] * len(accuracies), accuracies)\n", 
    "\n", 
    "# plot the trend line with error bars that correspond to standard deviation\n", 
    "accuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])\n", 
    "accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])\n", 
    "plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)\n", 
    "plt.title('Cross-validation on k')\n", 
    "plt.xlabel('k')\n", 
    "plt.ylabel('Cross-validation accuracy')\n", 
    "plt.show()"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "execution_count": 13, 
   "cell_type": "code", 
   "source": [
    "# Based on the cross-validation results above, choose the best value for k,   \n", 
    "# retrain the classifier using all the training data, and test it on the test\n", 
    "# data. You should be able to get above 26% accuracy on the test data.\n", 
    "\n", 
    "best_k = None\n", 
    "# YOUR CODE HERE\n", 
    "# Choose the best k based on the cross validation above\n", 
    "pass\n", 
    "# END YOUR CODE\n", 
    "\n", 
    "y_test_pred = predict_labels(dists, y_train, k=best_k)\n", 
    "\n", 
    "# Compute and display the accuracy\n", 
    "num_correct = np.sum(y_test_pred == y_test)\n", 
    "accuracy = float(num_correct) / num_test\n", 
    "print('For k = %d, got %d / %d correct => accuracy: %f' % (best_k, num_correct, num_test, accuracy))"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "source": [
    "---\n", 
    "## Part 3: PCA (30 points)\n", 
    "\n", 
    "Principal Component Analysis (PCA) is a simple yet popular and useful linear transformation technique that is used in numerous applications, such as stock market predictions, the analysis of gene expression data, and many more. In this tutorial, we will see that PCA is not just a \"black box\", and we are going to unravel its internals in 3 basic steps.\n", 
    "\n", 
    "### Introduction\n", 
    "The sheer size of data in the modern age is not only a challenge for computer hardware but also a main bottleneck for the performance of many machine learning algorithms. The main goal of a PCA analysis is to identify patterns in data; PCA aims to detect the correlation between variables. If a strong correlation between variables exists, the attempt to reduce the dimensionality only makes sense. In a nutshell, this is what PCA is all about: Finding the directions of maximum variance in high-dimensional data and project it onto a smaller dimensional subspace while retaining most of the information.\n", 
    "\n", 
    "### A Summary of the PCA Approach\n", 
    "- Standardize the data.\n", 
    "- Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.\n", 
    "- Sort eigenvalues in descending order and choose the $k$ eigenvectors that correspond to the $k$ largest eigenvalues where $k$ is the number of dimensions of the new feature subspace ($k \\leq d$).\n", 
    "- Construct the projection matrix $\\mathbf{W}$ from the selected $k$ eigenvectors.\n", 
    "- Transform the original dataset $\\mathbf{X}$ via $\\mathbf{W}$ to obtain a $k$-dimensional feature subspace Y."
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 14, 
   "cell_type": "code", 
   "source": [
    "from features import PCA\n", 
    "\n", 
    "pca = PCA()"
   ], 
   "outputs": [], 
   "metadata": {
    "collapsed": true
   }
  }, 
  {
   "source": [
    "### 3.1 - Eigendecomposition\n", 
    "The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \"core\" of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.\n", 
    "\n", 
    "Implement **`_eigen_decomp`** in `pca.py`."
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 15, 
   "cell_type": "code", 
   "source": [
    "# Perform eigenvalue decomposition on the covariance matrix of training data.\n", 
    "e_vecs, e_vals = pca._eigen_decomp(X_train - X_train.mean(axis=0))\n", 
    "\n", 
    "print(e_vals.shape)\n", 
    "print(e_vecs.shape)"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "source": [
    "### 3.2 - Singular Value Decomposition\n", 
    "Doing an eigendecomposition of the covariance matrix is very expensive, especially when the number of features (`D = 4096` here) gets very high.\n", 
    "\n", 
    "To obtain the same eigenvalues and eigenvectors in a more efficient way, we can use Singular Value Decomposition (SVD). If we perform SVD on matrix $X$, we obtain $U$, $S$ and $V$ such that:\n", 
    "$$\n", 
    "X = U S V^T\n", 
    "$$\n", 
    "\n", 
    "- the columns of $U$ are the eigenvectors of $X X^T$\n", 
    "- the columns of $V^T$ are the eigenvectors of $X^T X$\n", 
    "- the values of $S$ are the square roots of the eigenvalues of $X^T X$ (or $X X^T$)\n", 
    "\n", 
    "\n", 
    "Therefore, we can find out the top `k` eigenvectors of the covariance matrix $X^T X$ using SVD.\n", 
    "\n", 
    "Implement **`_svd`** in `pca.py`."
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 16, 
   "cell_type": "code", 
   "source": [
    "# Perform SVD on directly on the training data.\n", 
    "u, s = pca._svd(X_train - X_train.mean(axis=0))\n", 
    "\n", 
    "print(s.shape)\n", 
    "print(u.shape)"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "execution_count": 17, 
   "cell_type": "code", 
   "source": [
    "# Test whether the square of singular values and eigenvalues are the same.\n", 
    "# We also observe that `e_vecs` and `u` are the same (only the sign of each column can differ).\n", 
    "N = X_train.shape[0]\n", 
    "assert np.allclose((s ** 2) / (N - 1), e_vals[:len(s)])\n", 
    "\n", 
    "for i in range(len(s) - 1):\n", 
    "    assert np.allclose(e_vecs[:, i], u[:, i]) or np.allclose(e_vecs[:, i], -u[:, i])\n", 
    "    # (the last eigenvector for i = len(s) - 1 is very noisy because the eigenvalue is almost 0,\n", 
    "    #  so imprecisions in the computation build up)"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "source": [
    "### 3.3 - Dimensionality Reduction\n", 
    "\n", 
    "The top $k$ principal components explain most of the variance of the underlying data.\n", 
    "\n", 
    "By projecting our initial data (the images) onto the subspace spanned by the top k principal components,\n", 
    "we can reduce the dimension of our inputs while keeping most of the information.\n", 
    "\n", 
    "In the example below, we can see that **using the first two components in PCA is not enough** to allow us to see pattern in the data. All the classes seem placed at random in the 2D plane."
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 18, 
   "cell_type": "code", 
   "source": [
    "# Dimensionality reduction by projecting the data onto\n", 
    "# lower dimensional subspace spanned by k principal components\n", 
    "\n", 
    "# To visualize, we will project in 2 dimensions\n", 
    "n_components = 2\n", 
    "pca.fit(X_train)\n", 
    "X_proj = pca.transform(X_train, n_components)\n", 
    "\n", 
    "# Plot the top two principal components\n", 
    "for y in np.unique(y_train):\n", 
    "    plt.scatter(X_proj[y_train==y,0], X_proj[y_train==y,1], label=classes[y])\n", 
    "    \n", 
    "plt.xlabel('1st component')\n", 
    "plt.ylabel('2nd component')\n", 
    "plt.legend()\n", 
    "plt.show()"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "source": [
    "### 3.4 - Visualizing Eigenfaces\n", 
    "\n", 
    "The columns of the PCA projection matrix `pca.W_pca` represent the eigenvectors of $X^T X$.\n", 
    "\n", 
    "We can visualize the biggest singular values as well as the corresponding vectors to get a sense of what the PCA algorithm is extracting.\n", 
    "\n", 
    "If we visualize the top 10 eigenfaces, we can see tht the algorithm focuses on the different shades of the faces. For instance, in face n\u00b02 the light seems to come from the left."
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 19, 
   "cell_type": "code", 
   "source": [
    "for i in range(10):\n", 
    "    plt.subplot(1, 10, i+1)\n", 
    "    plt.imshow(pca.W_pca[:, i].reshape(64, 64))\n", 
    "    plt.title(\"%.2f\" % s[i])\n", 
    "plt.show()"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "execution_count": 20, 
   "cell_type": "code", 
   "source": [
    "# Reconstruct data with principal components\n", 
    "n_components = 100  # Experiment with different number of components.\n", 
    "X_proj = pca.transform(X_train, n_components)\n", 
    "X_rec = pca.reconstruct(X_proj)\n", 
    "\n", 
    "print(X_rec.shape)\n", 
    "print(classes)\n", 
    "\n", 
    "# Visualize reconstructed faces\n", 
    "samples_per_class = 10\n", 
    "for y, cls in enumerate(classes):\n", 
    "    idxs = np.flatnonzero(y_train == y)\n", 
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n", 
    "    for i, idx in enumerate(idxs):\n", 
    "        plt_idx = i * num_classes + y + 1\n", 
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n", 
    "        plt.imshow((X_rec[idx]).reshape((64, 64)))\n", 
    "        plt.axis('off')\n", 
    "        if i == 0:\n", 
    "            plt.title(y)\n", 
    "plt.show()"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "source": [
    "### Written Question 1 (5 points) \n", 
    "*Question*: Consider a dataset of $N$ face images, each with shape $(h, w)$. Then, we need $O(Nhw)$ of memory to store the data. Suppose we perform dimensionality reduction on the dataset with $p$ principal components, and use the components as bases to represent images. Calculate how much memory we need to store the images and the matrix used to get back to the original space.\n", 
    "\n", 
    "Said in another way, how much memory does storing the compressed images **and** the uncompresser cost.\n", 
    "\n", 
    "*Answer:* TODO"
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "source": [
    "### 3.5 - Reconstruction error and captured variance\n", 
    "\n", 
    "We can plot the reconstruction error with respect to the dimension of the projected space.\n", 
    "\n", 
    "The reconstruction gets better with more components.\n", 
    "\n", 
    "We can see in the plot that the inflexion point is around dimension 200 or 300. This means that using this number of components is a good compromise between good reconstruction and low dimension."
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 21, 
   "cell_type": "code", 
   "source": [
    "# Plot reconstruction errors for different k\n", 
    "N = X_train.shape[0]\n", 
    "d = X_train.shape[1]\n", 
    "\n", 
    "ns = range(1, d, 100)\n", 
    "errors = []\n", 
    "\n", 
    "for n in ns:\n", 
    "    X_proj = pca.transform(X_train, n)\n", 
    "    X_rec = pca.reconstruct(X_proj)\n", 
    "\n", 
    "    # Compute reconstruction error\n", 
    "    error = np.mean((X_rec - X_train) ** 2)\n", 
    "    errors.append(error)\n", 
    "\n", 
    "plt.plot(ns, errors)\n", 
    "plt.xlabel('Number of Components')\n", 
    "plt.ylabel('Reconstruction Error')\n", 
    "plt.show()"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "source": [
    "We can do the same process to see how much variance is captured by the projection.\n", 
    "\n", 
    "Again, we see that the inflexion point is around 200 or 300 dimensions."
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 22, 
   "cell_type": "code", 
   "source": [
    "# Plot captured variance\n", 
    "ns = range(1, d, 100)\n", 
    "var_cap = []\n", 
    "\n", 
    "for n in ns:\n", 
    "    var_cap.append(np.sum(s[:n] ** 2)/np.sum(s ** 2))\n", 
    "    \n", 
    "plt.plot(ns, var_cap)\n", 
    "plt.xlabel('Number of Components')\n", 
    "plt.ylabel('Variance Captured')\n", 
    "plt.show()"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "source": [
    "### 3.6 - kNN with PCA\n", 
    "\n", 
    "Performing kNN on raw features (the pixels of the image) does not yield very good results.  \n", 
    "Computing the distance between images in the image space is not a very good metric for actual proximity of images.  \n", 
    "For instance, an image of person A with a dark background will be close to an image of B with a dark background, although these people are not the same.\n", 
    "\n", 
    "Using a technique like PCA can help discover the real interesting features and perform kNN on them could give better accuracy.\n", 
    "\n", 
    "However, we observe here that PCA doesn't really help to disentangle the features and obtain useful distance metrics between the different classes. We basically obtain the same performance as with raw features."
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 23, 
   "cell_type": "code", 
   "source": [
    "num_test = X_test.shape[0]\n", 
    "\n", 
    "# We computed the best k and n for you\n", 
    "best_k = 20\n", 
    "best_n = 500\n", 
    "\n", 
    "\n", 
    "# PCA\n", 
    "pca = PCA()\n", 
    "pca.fit(X_train)\n", 
    "X_proj = pca.transform(X_train, best_n)\n", 
    "X_test_proj = pca.transform(X_test, best_n)\n", 
    "\n", 
    "# kNN\n", 
    "dists = compute_distances(X_test_proj, X_proj)\n", 
    "y_test_pred = predict_labels(dists, y_train, k=best_k)\n", 
    "\n", 
    "# Compute and display the accuracy\n", 
    "num_correct = np.sum(y_test_pred == y_test)\n", 
    "accuracy = float(num_correct) / num_test\n", 
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "source": [
    "---\n", 
    "## Part 4 - Fisherface: Linear Discriminant Analysis (25 points)\n", 
    "\n", 
    "LDA is a linear transformation method like PCA, but with a different goal.  \n", 
    "The main difference is that LDA takes information from the labels of the examples to maximize the separation of the different classes in the transformed space.\n", 
    "\n", 
    "Therefore, LDA is not totally **unsupervised** since it requires labels. PCA is **fully unsupervised**.\n", 
    "\n", 
    "In summary:\n", 
    "- PCA perserves maximum variance in the projected space.\n", 
    "- LDA preserves discrimination between classes in the project space. We want to maximize scatter between classes and minimize scatter intra class."
   ], 
   "cell_type": "markdown", 
   "metadata": {
    "collapsed": true
   }
  }, 
  {
   "execution_count": 24, 
   "cell_type": "code", 
   "source": [
    "from features import LDA\n", 
    "\n", 
    "lda = LDA()"
   ], 
   "outputs": [], 
   "metadata": {
    "collapsed": true
   }
  }, 
  {
   "source": [
    "### 4.1 - Dimensionality Reduction via PCA\n", 
    "To apply LDA, we need $D < N$. Since in our dataset, $N = 800$ and $D = 4096$, we first need to reduce the number of dimensions of the images using PCA.  \n", 
    "More information at: http://www.scholarpedia.org/article/Fisherfaces"
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 25, 
   "cell_type": "code", 
   "source": [
    "N = X_train.shape[0]\n", 
    "c = num_classes\n", 
    "\n", 
    "pca = PCA()\n", 
    "pca.fit(X_train)\n", 
    "X_train_pca = pca.transform(X_train, N-c)\n", 
    "X_test_pca = pca.transform(X_test, N-c)"
   ], 
   "outputs": [], 
   "metadata": {
    "collapsed": true
   }
  }, 
  {
   "source": [
    "### 4.2 - Scatter matrices\n", 
    "\n", 
    "We first need to compute the within-class scatter matrix:\n", 
    "$$\n", 
    "S_W = \\sum_{i=1}^c S_i\n", 
    "$$\n", 
    "where $S_i = \\sum_{x_k \\in Y_i} (x_k - \\mu_i)(x_k - \\mu_i)^T$ is the scatter of class $i$.\n", 
    "\n", 
    "We then need to compute the between-class scatter matrix:\n", 
    "$$\n", 
    "S_B = \\sum_{i=1}^c N_i (\\mu_i - \\mu)(\\mu_i - \\mu)^T\n", 
    "$$\n", 
    "where $N_i$ is the number of examples in class $i$."
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 26, 
   "cell_type": "code", 
   "source": [
    "# Compute within-class scatter matrix\n", 
    "S_W = lda._within_class_scatter(X_train_pca, y_train)\n", 
    "print(S_W.shape)"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "execution_count": 27, 
   "cell_type": "code", 
   "source": [
    "# Compute between-class scatter matrix\n", 
    "S_B = lda._between_class_scatter(X_train_pca, y_train)\n", 
    "print(S_B.shape)"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "source": [
    "### 4.3 - Solving generalized Eigenvalue problem\n", 
    "\n", 
    "Implement methods `fit` and `transform` of the `LDA` class."
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 28, 
   "cell_type": "code", 
   "source": [
    "lda.fit(X_train_pca, y_train)"
   ], 
   "outputs": [], 
   "metadata": {
    "collapsed": true
   }
  }, 
  {
   "execution_count": 29, 
   "cell_type": "code", 
   "source": [
    "# Dimensionality reduction by projecting the data onto\n", 
    "# lower dimensional subspace spanned by k principal components\n", 
    "n_components = 2\n", 
    "X_proj = lda.transform(X_train_pca, n_components)\n", 
    "X_test_proj = lda.transform(X_test_pca, n_components)\n", 
    "\n", 
    "# Plot the top two principal components on the training set\n", 
    "for y in np.unique(y_train):\n", 
    "    plt.scatter(X_proj[y_train==y, 0], X_proj[y_train==y, 1], label=classes[y])\n", 
    "    \n", 
    "plt.xlabel('1st component')\n", 
    "plt.ylabel('2nd component')\n", 
    "plt.legend()\n", 
    "plt.title(\"Training set\")\n", 
    "plt.show()\n", 
    "\n", 
    "# Plot the top two principal components on the test set\n", 
    "for y in np.unique(y_test):\n", 
    "    plt.scatter(X_test_proj[y_test==y, 0], X_test_proj[y_test==y,1], label=classes[y])\n", 
    "    \n", 
    "plt.xlabel('1st component')\n", 
    "plt.ylabel('2nd component')\n", 
    "plt.legend()\n", 
    "plt.title(\"Test set\")\n", 
    "plt.show()"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "source": [
    "### 4.4 - kNN with LDA\n", 
    "\n", 
    "Thanks to having the information from the labels, LDA gives a discriminant space where the classes are far apart from each other.  \n", 
    "This should help kNN a lot, as the job should just be to find the obvious 10 clusters.\n", 
    "\n", 
    "However, as we've seen in the previous plot (section 4.3), the training data gets clustered pretty well, but the test data isn't as nicely clustered as the training data (overfitting?).\n", 
    "\n", 
    "\n", 
    "Perform cross validation following the code below (you can change the values of `k_choices` and `n_choices` to search). Using the best result from cross validation, obtain the test accuracy."
   ], 
   "cell_type": "markdown", 
   "metadata": {}
  }, 
  {
   "execution_count": 30, 
   "cell_type": "code", 
   "source": [
    "num_folds = 5\n", 
    "\n", 
    "X_trains, y_trains, X_vals, y_vals = split_folds(X_train, y_train, num_folds)\n", 
    "\n", 
    "k_choices = [1, 5, 10, 20]\n", 
    "n_choices = [5, 10, 20, 50, 100, 200, 500]\n", 
    "pass\n", 
    "\n", 
    "\n", 
    "# n_k_to_accuracies[(n, k)] should be a list of length num_folds giving the different\n", 
    "# accuracy values that we found when using that value of n and k.\n", 
    "n_k_to_accuracies = defaultdict(list)\n", 
    "\n", 
    "for i in range(num_folds):\n", 
    "    # Fit PCA\n", 
    "    pca = PCA()\n", 
    "    pca.fit(X_trains[i])\n", 
    "    \n", 
    "    N = len(X_trains[i])\n", 
    "    X_train_pca = pca.transform(X_trains[i], N-c)\n", 
    "    X_val_pca = pca.transform(X_vals[i], N-c)\n", 
    "    \n", 
    "    # Fit LDA\n", 
    "    lda = LDA()\n", 
    "    lda.fit(X_train_pca, y_trains[i])\n", 
    "    \n", 
    "    for n in n_choices:\n", 
    "        X_train_proj = lda.transform(X_train_pca, n)\n", 
    "        X_val_proj = lda.transform(X_val_pca, n)\n", 
    "\n", 
    "        dists = compute_distances(X_val_proj, X_train_proj)\n", 
    "            \n", 
    "        for k in k_choices:\n", 
    "            y_pred = predict_labels(dists, y_trains[i], k=k)\n", 
    "\n", 
    "            # Compute and print the fraction of correctly predicted examples\n", 
    "            num_correct = np.sum(y_pred == y_vals[i])\n", 
    "            accuracy = float(num_correct) / len(y_vals[i])\n", 
    "            n_k_to_accuracies[(n, k)].append(accuracy)\n", 
    "\n", 
    "\n", 
    "for n in n_choices:\n", 
    "    print()\n", 
    "    for k in k_choices:\n", 
    "        accuracies = n_k_to_accuracies[(n, k)]\n", 
    "        print(\"For n=%d, k=%d: average accuracy is %f\" % (n, k, np.mean(accuracies)))"
   ], 
   "outputs": [], 
   "metadata": {}
  }, 
  {
   "execution_count": 31, 
   "cell_type": "code", 
   "source": [
    "# Based on the cross-validation results above, choose the best value for k,   \n", 
    "# retrain the classifier using all the training data, and test it on the test\n", 
    "# data. You should be able to get above 40% accuracy on the test data.\n", 
    "\n", 
    "best_k = None\n", 
    "best_n = None\n", 
    "# YOUR CODE HERE\n", 
    "# Choose the best k based on the cross validation above\n", 
    "pass\n", 
    "# END YOUR CODE\n", 
    "\n", 
    "N = len(X_train)\n", 
    "\n", 
    "# Fit PCA\n", 
    "pca = PCA()\n", 
    "pca.fit(X_train)\n", 
    "X_train_pca = pca.transform(X_train, N-c)\n", 
    "X_test_pca = pca.transform(X_test, N-c)\n", 
    "\n", 
    "# Fit LDA\n", 
    "lda = LDA()\n", 
    "lda.fit(X_train_pca, y_train)\n", 
    "\n", 
    "# Project using LDA\n", 
    "X_train_proj = lda.transform(X_train_pca, best_n)\n", 
    "X_test_proj = lda.transform(X_test_pca, best_n)\n", 
    "\n", 
    "dists = compute_distances(X_test_proj, X_train_proj)\n", 
    "y_test_pred = predict_labels(dists, y_train, k=best_k)\n", 
    "\n", 
    "# Compute and display the accuracy\n", 
    "num_correct = np.sum(y_test_pred == y_test)\n", 
    "accuracy = float(num_correct) / num_test\n", 
    "print(\"For k=%d and n=%d\" % (best_k, best_n))\n", 
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
   ], 
   "outputs": [], 
   "metadata": {}
  }
 ], 
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3", 
   "name": "python3", 
   "language": "python"
  }, 
  "language_info": {
   "mimetype": "text/x-python", 
   "nbconvert_exporter": "python", 
   "name": "python", 
   "file_extension": ".py", 
   "version": "3.6.3", 
   "pygments_lexer": "ipython3", 
   "codemirror_mode": {
    "version": 3, 
    "name": "ipython"
   }
  }
 }
}