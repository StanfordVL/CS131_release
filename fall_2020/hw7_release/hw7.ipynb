{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "This assignment covers the Luas-Kanade tracking method. Please hand in `motion.py` and this notebook file to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Setup\n",
    "import numpy as np\n",
    "from skimage import filters\n",
    "from skimage.feature import corner_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Displaying Video\n",
    "We have done some cool stuff with static images in past assignemnts. Now, let's turn our attention to videos! For this assignment, the videos are provided as time series of images. We also provide utility functions to load the image frames and visualize them as a short video clip.\n",
    "\n",
    "*Note: You may need to install video codec like [FFmpeg](http://ffmpeg.zeranoe.com/builds/). If you have conda (or Anaconda), you can generally install it with `conda install -c conda-forge ffmpeg`. For Linux/Mac, you will also be able to install ffmpeg using `apt-get` or `brew`. For Windows, you can find the installation instructions [here](https://www.wikihow.com/Install-FFmpeg-on-Windows).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils import animated_frames, load_frames\n",
    "frames = load_frames('images')\n",
    "ani = animated_frames(frames)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lucas-Kanade Method for Optical Flow\n",
    "\n",
    "### 1.1 Deriving optical flow equation\n",
    "Optical flow methods are used to estimate motion of objects between two consecutive image frames. For example, in the video above, the can of tea seems to be moving to the left. For our system to be able to understand that the can is moving to the left, it would be useful to find a way to add vectors to the can (known as **flow vectors**) which point to the left, thus describing its motion.\n",
    "\n",
    "Given two consecutive frames, how can we find the flow vectors for the first frame which describe how objects move between frames? To start, we make a reasonable assumption called the **brightness constancy** assumption: the pixel intensity of a moving point stays the same between two consecutive frames with small time differences. In other words, picking any pixel of the moving can, its brightness stays approximately the same between frames &mdash; its movement should not affect its brightness after all.\n",
    "\n",
    "Consider pixel intensity (a.k.a. brightness) $I(x, y, t)$ of a point $(x, y)$ in the first frame $t$. Suppose that the point has moved to $(x+\\Delta{x}, y+\\Delta{y})$ after $\\Delta{t}$. According to the brightness constancy assumption, we can relate intensities of the point in the two frames using the following equation:\n",
    "\n",
    "$$\n",
    "I(x,y,t)=I(x+\\Delta{x},y+\\Delta{y},t+\\Delta{t})\n",
    "$$\n",
    "\n",
    "Coming back to the example of the moving can, this equation simply states that the point that we picked will have the same intensity even after it moves in space $(\\Delta{x}$ and $\\Delta{y})$ and between frames $(\\Delta{t})$. From this simple assumption, we can derive what is known as the **optical flow equation**. For a given point for any frame, the optical flow equation is given by:\n",
    "\n",
    "$$\n",
    "I_x({\\mathbf{p}})v_{x} +\n",
    "I_y({\\mathbf{p}})v_{y} +\n",
    "I_t({\\mathbf{p}})\n",
    "= 0\n",
    "$$\n",
    "\n",
    "Here, $I_x$, $I_y$ and $I_t$ are partial derivatives of pixel intensity $I$. Meanwhile,\n",
    "$v_{x}$ and $v_{y}$ are **flow vectors** in the $x-$ and $y-$direction, respectively. These are the vectors we care about! If we can solve for these two values, we will be able to describe the motion of any object between frames.\n",
    "\n",
    "You might be wondering how we went from the brightness constantcy assumption to the optical flow equation. Try to derive it yourself!\n",
    "\n",
    "- **a.** Using a first-order Taylor approximation, derive the optical flow equation from the brightness constancy equation.\n",
    "\n",
    "\n",
    "- **b.** Can the optical flow equation be solved given two consecutive frames without further assumption? Of $I_x, I_y, I_t, v_x,$ and $v_y$, which values can be computed directly given two consecutive frames? Which values cannot be computed without additional information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:** Write your answer in this markdown cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Overview of Lucas-Kanade method\n",
    "\n",
    "One issue with the optical flow equation is that there are two unknowns that we want to solve for ($v_x$ and $v_y$). This problem is known as the **aperture problem**. In other words, just looking an \"aperture\" at one pixel at a time, it is impossible to discern the true direction of motion of the object in question.\n",
    "\n",
    "The Lucas\u2013Kanade method solves this problem by adding another assumption: **spatial coherence**. That is, that the motion of the image contents between two frames is approximately constant within a neighborhood of the point $p$ under consideration.\n",
    "\n",
    "Consider a neighborhood of $p$, $N(p)=\\{p_1,...,p_n\\}$ (e.g. 3x3 window around $p$). Adding the spatial coherence assumption to the optical flow equation, we see that the following should be satisfied:\n",
    "\n",
    "For every $p_i \\in N(p)$,\n",
    "$$\n",
    "I_{x}(p_i)v_x + I_{y}(p_i)v_y = -I_{t}(p_i)\n",
    "$$\n",
    "\n",
    "These equations can be written in matrix form $Av=b$, where\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "    I_{x}(p_1) & I_{y}(p_1)\\\\\n",
    "    I_{x}(p_2) & I_{y}(p_2)\\\\\n",
    "    \\vdots & \\vdots\\\\\n",
    "    I_{x}(p_n) & I_{y}(p_n)\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "v =\n",
    "\\begin{bmatrix}\n",
    "    v_{x}\\\\\n",
    "    v_{y}\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "b =\n",
    "\\begin{bmatrix}\n",
    "    -I_{t}(p_1)\\\\\n",
    "    -I_{t}(p_2)\\\\\n",
    "    \\vdots\\\\\n",
    "    -I_{t}(p_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can now solve for the flow vectors (now represented as $v$) by solving the following least-squares problem: $A^{T}Av=A^{T}b$.\n",
    "\n",
    "- **a.** What is the condition for this equation to be solvable?\n",
    "- **b.** Reason about why Harris corners might be good features to track using Lucas-Kanade method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:** Write your answer in this markdown cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Implementation of Lucas-Kanade method\n",
    "\n",
    "In this section, we are going to implement basic Lucas-Kanade method for feature tracking. In order to do so, we first need to find keypoints to track. Harris corner detector is commonly used to initialize the keypoints to track with Lucas-Kanade method. For this assignment, we are going to use [`skimage` implementation](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_corner.html) of Harris corner detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "from skimage.feature import corner_harris, corner_peaks\n",
    "\n",
    "frames = load_frames('images')\n",
    "\n",
    "# Detect keypoints to track\n",
    "keypoints = corner_peaks(corner_harris(frames[0]),\n",
    "                         exclude_border=5,\n",
    "                         threshold_rel=0.01)\n",
    "\n",
    "# Plot kepoints\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.imshow(frames[0])\n",
    "plt.scatter(keypoints[:,1], keypoints[:,0],\n",
    "            facecolors='none', edgecolors='r')\n",
    "plt.axis('off')\n",
    "plt.title('Detected keypoints in the first frame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function **`lucas_kanade`** in `motion.py` and run the code cell below. You will be able to see small arrows pointing towards the directions where keypoints are moving.\n",
    "\n",
    "The expected output is:\n",
    "![ref_imgs/optical_flow_vectors.png](ref_imgs/optical_flow_vectors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from motion import lucas_kanade\n",
    "\n",
    "# Lucas-Kanade method for optical flow\n",
    "flow_vectors = lucas_kanade(frames[0], frames[1], keypoints, window_size=5)\n",
    "\n",
    "# Plot flow vectors\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.imshow(frames[0])\n",
    "plt.axis('off')\n",
    "plt.title('Optical flow vectors')\n",
    "\n",
    "for y, x, vy, vx in np.hstack((keypoints, flow_vectors)):\n",
    "    plt.arrow(x, y, vx, vy, head_width=5, head_length=5, color='b')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the position of the keypoints in the next frame by adding the flow vectors to the keypoints. \n",
    "\n",
    "The expected output is:\n",
    "![ref_imgs/second_frame_tracked_keypoints.png](ref_imgs/second_frame_tracked_keypoints.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tracked kepoints\n",
    "new_keypoints = keypoints + flow_vectors\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.imshow(frames[1])\n",
    "plt.scatter(new_keypoints[:,1], new_keypoints[:,0],\n",
    "            facecolors='none', edgecolors='r')\n",
    "plt.axis('off')\n",
    "plt.title('Tracked keypoints in the second frame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Feature Tracking in multiple frames\n",
    "Now we can use Lucas-Kanade method to track keypoints across multiple frames. The idea is simple: compute flow vectors at keypoints in $i$-th frame, and add the flow vectors to the points to keep track of the points in $i+1$-th frame. We have provided the function `track_features` for you. First, run the code cell below. You will notice that some of the points just drift away and are not tracked very well.\n",
    "\n",
    "Instead of keeping these 'bad' tracks, we would want to somehow declare some points are 'lost' and just discard them. One simple way to is to compare the patches around tracked points in two subsequent frames. If the patch around a point is NOT similar to the patch around the corresponding point in the next frame, then we declare the point to be lost. Here, we are going to use mean squared error between two normalized patches as the criterion for lost tracks.\n",
    "\n",
    "Implement **`compute_error`** in `motion.py`, and re-run the code cell below. You will see many of the points disappearing in later frames.\n",
    "\n",
    "Below is what the final frame should look like:\n",
    "![ref_imgs/compute_error_final_frame.png](ref_imgs/compute_error_final_frame.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils import animated_scatter\n",
    "from motion import track_features\n",
    "\n",
    "# Detect keypoints to track in the first frame\n",
    "keypoints = corner_peaks(corner_harris(frames[0]),\n",
    "                         exclude_border=5,\n",
    "                         threshold_rel=0.01)\n",
    "\n",
    "trajs = track_features(frames, keypoints,\n",
    "                       error_thresh=1.5,\n",
    "                       optflow_fn=lucas_kanade,\n",
    "                       window_size=5)\n",
    "ani = animated_scatter(frames,trajs)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pyramidal Lucas-Kanade Feature Tracker\n",
    "In this section, we are going to implement a simpler version of the method described in [\"Pyramidal Implementation of the Lucas Kanade Feature Tracker\"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.185.585&rep=rep1&type=pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Iterative Lucas-Kanade method\n",
    "One limitation of the naive Lucas-Kanade method is that it cannot track large motions between frames. You might have noticed that the resulting flow vectors (blue arrows) in the previous section are too small that the tracked keypoints are slightly off from where they should be. In order to address this problem, we can iteratively refine the estimated optical flow vectors. Below is the step-by-step description of the algorithm:\n",
    "\n",
    "Let $p=\\begin{bmatrix}p_x & p_y \\end{bmatrix}^T$ be a point on frame $I$. The goal is to find flow vector $v=\\begin{bmatrix}v_x & v_y \\end{bmatrix}^T$ such that $p+v$ is the corresponding point of $p$ on the next frame $J$.\n",
    "\n",
    "- Initialize flow vector:\n",
    "$$\n",
    "v=\n",
    "\\begin{bmatrix}\n",
    "    0\\\\0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Compute spatial gradient matrix:\n",
    "$$\n",
    "G=\\sum_{x=p_x-w}^{p_x+w}\\sum_{y=p_y-w}^{p_y+w}\n",
    "\\begin{bmatrix}\n",
    "    I_{x}^2(x,y) & I_{x}(x,y)I_{y}(x,y)\\\\\n",
    "    I_{x}(x,y)I_{y}(x,y) & I_{y}^2(x,y)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- **for $k=1$ to $K$**\n",
    "    - Compute temporal difference: $\\delta I_k(x, y) = I(x,y)-J(x+g_x+v_x, y+g_y+v_y)$\n",
    "    - Compute image mismatch vector:\n",
    "$$\n",
    "b_k=\\sum_{x=p_x-w}^{p_x+w}\\sum_{y=p_y-w}^{p_y+w}\n",
    "\\begin{bmatrix}\n",
    "    \\delta I_k(x, y)I_x(x,y)\\\\\n",
    "    \\delta I_k(x, y)I_y(x,y)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "    - Compute optical flow: $v^k=G^{-1}b_k$\n",
    "    - Update flow vector for next iteration: $v := v + v^k$\n",
    "\n",
    "\n",
    "- Return $v$\n",
    "\n",
    "Implement `iterative_lucas_kanade` method in `motion.py` and run the code cell below. You should be able to see slightly longer arrows in the visualization.\n",
    "\n",
    "The expected output is:\n",
    "![ref_imgs/iterative_optical_flow_vectors.png](ref_imgs/iterative_optical_flow_vectors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from motion import iterative_lucas_kanade\n",
    "\n",
    "# Run iterative Lucas-Kanade method\n",
    "flow_vectors = iterative_lucas_kanade(frames[0], frames[1], keypoints)\n",
    "\n",
    "# Plot flow vectors\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.imshow(frames[0])\n",
    "plt.axis('off')\n",
    "plt.title('Optical flow vectors (iterative LK)')\n",
    "\n",
    "for y, x, vy, vx in np.hstack((keypoints, flow_vectors)):\n",
    "    plt.arrow(x, y, vx, vy, head_width=5, head_length=5, color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will plot the tracked keypoints. \n",
    "\n",
    "The expected output is:\n",
    "![ref_imgs/iterative_second_frame_tracked_keypoints.png](ref_imgs/iterative_second_frame_tracked_keypoints.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tracked kepoints\n",
    "new_keypoints = keypoints + flow_vectors\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.imshow(frames[1])\n",
    "plt.scatter(new_keypoints[:,1], new_keypoints[:,0],\n",
    "            facecolors='none', edgecolors='r')\n",
    "plt.axis('off')\n",
    "plt.title('Tracked keypoints in the second frame (iterative LK)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to track the keypoints across frames. Below is what the final frame should look like:\n",
    "![ref_imgs/iterative_compute_error_final_frame.png](ref_imgs/iterative_compute_error_final_frame.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect keypoints to track in the first frame\n",
    "keypoints = corner_peaks(corner_harris(frames[0]),\n",
    "                         exclude_border=5,\n",
    "                         threshold_rel=0.01)\n",
    "\n",
    "# Track keypoints using iterative Lucas-Kanade method\n",
    "trajs = track_features(frames, keypoints,\n",
    "                       error_thresh=1.5,\n",
    "                       optflow_fn=iterative_lucas_kanade,\n",
    "                       window_size=5)\n",
    "ani = animated_scatter(frames,trajs)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Coarse-to-Fine Optical Flow\n",
    "The iterative method still could not track larger motions. If we downscaled the images, larger displacements would become easier to track. On the otherhand, smaller motions would become more difficult to track as we lose details in the images. To address this problem, we can represent images in multi-scale, and compute flow vectors from coarse to fine scale.\n",
    "\n",
    "Run the following code cell to visualize image pyramid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import pyramid_gaussian\n",
    "\n",
    "image = frames[0]\n",
    "\n",
    "# pyramid_gaussian returns tuple of max_layer + 1 images in multiple scales\n",
    "pyramid = tuple(pyramid_gaussian(image, max_layer=3, downscale=2))\n",
    "\n",
    "rows, cols = image.shape\n",
    "composite_image = np.zeros((rows, cols + cols // 2 + 1), dtype=np.double)\n",
    "composite_image[:rows, :cols] = pyramid[0]\n",
    "\n",
    "i_row = 0\n",
    "for p in pyramid[1:]:\n",
    "    n_rows, n_cols = p.shape\n",
    "    composite_image[i_row:i_row + n_rows, cols:cols + n_cols] = p\n",
    "    i_row += n_rows\n",
    "\n",
    "# Display image pyramid\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.imshow(composite_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the description of pyramidal Lucas-Kanade algorithm:\n",
    "\n",
    "Let $p$ be a point on image $I$ and $s$ be the scale of pyramid representation.\n",
    "- Build pyramid representations of $I$ and $J$: $\\{I^L\\}_{L=0,...,L_m}$ and $\\{J^L\\}_{L=0,...,L_m}$\n",
    "\n",
    "\n",
    "- Initialize pyramidal guess $g^{L_m}=\n",
    "\\begin{bmatrix}g_{x}^{L_m} & g_{y}^{L_m}\\end{bmatrix}^T=\\begin{bmatrix}0 & 0\\end{bmatrix}^T$\n",
    "\n",
    "\n",
    "- **for $L=L_m$ to $0$ with step of -1**\n",
    "\n",
    "    - Compute location of $p$ on $I^L$: $p^L=p/s^L$\n",
    "    \n",
    "    - Let $d^L$ be the optical flow vector at level $L$:\n",
    "$$\n",
    "d^L := IterativeLucasKanade(I^L, J^L, p^L, g^L)\n",
    "$$\n",
    "    - Guess for next level $L-1$: $g^{L-1}=s(g^L+d^L)$\n",
    "    \n",
    "    \n",
    "- Return $d=g^0+d^0$\n",
    "\n",
    "Implement `pyramid_lucas_kanade`.\n",
    "\n",
    "The expected output is:\n",
    "![ref_imgs/pyramid_optical_flow_vectors.png](ref_imgs/pyramid_optical_flow_vectors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from motion import pyramid_lucas_kanade\n",
    "\n",
    "# Lucas-Kanade method for optical flow\n",
    "flow_vectors = pyramid_lucas_kanade(frames[0], frames[1], keypoints)\n",
    "\n",
    "# Plot flow vectors\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.imshow(frames[0])\n",
    "plt.axis('off')\n",
    "plt.title('Optical flow vectors (pyramid LK)')\n",
    "\n",
    "for y, x, vy, vx in np.hstack((keypoints, flow_vectors)):\n",
    "    plt.arrow(x, y, vx, vy, head_width=3, head_length=3, color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will plot the tracked keypoints. \n",
    "\n",
    "The expected output is:\n",
    "![ref_imgs/pyramid_second_frame_tracked_keypoints.png](ref_imgs/pyramid_second_frame_tracked_keypoints.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tracked kepoints\n",
    "new_keypoints = keypoints + flow_vectors\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.imshow(frames[1])\n",
    "plt.scatter(new_keypoints[:,1], new_keypoints[:,0],\n",
    "            facecolors='none', edgecolors='r')\n",
    "plt.axis('off')\n",
    "plt.title('Tracked keypoints in the second frame (pyramid LK)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to track the keypoints across frames. Below is what the final frame should look like:\n",
    "![ref_imgs/pyramid_compute_error_final_frame.png](ref_imgs/pyramid_compute_error_final_frame.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import animated_scatter\n",
    "from motion import track_features\n",
    "keypoints = corner_peaks(corner_harris(frames[0]),\n",
    "                         exclude_border=5,\n",
    "                         threshold_rel=0.01)\n",
    "\n",
    "trajs = track_features(frames, keypoints,\n",
    "                       error_thresh=1.5,\n",
    "                       optflow_fn=pyramid_lucas_kanade,\n",
    "                       window_size=5)\n",
    "ani = animated_scatter(frames,trajs)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Object Tracking\n",
    "Let us build a simple object tracker using the Lucas-Kanade method we have implemented in previous sections. In order to test the object tracker, we provide you a short face-tracking sequence. Each frame in the sequence is annotated with the ground-truth location (as bounding box) of face.\n",
    "\n",
    "An object tracker is given an object bounding box in the first frame, and it has to track the object by predicting bounding boxes in subsequent frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import animated_bbox, load_bboxes\n",
    "\n",
    "# Load frames and ground truth bounding boxes\n",
    "frames = load_frames('Man/img')\n",
    "gt_bboxes = load_bboxes('Man/groundtruth_rect.txt')\n",
    "\n",
    "ani = animated_bbox(frames, gt_bboxes)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to track the object, we first find keypoints to track inside the bounding box. Then, we track those points in each of the following frames and output a tight bounding box around the tracked points. In order to prevent all the keypoints being lost, we detect new keypoints within the bounding box every 20 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find features to track within the bounding box\n",
    "x, y, w, h = gt_bboxes[0]\n",
    "roi = frames[0][y:y+h, x:x+w]\n",
    "keypoints = corner_peaks(corner_harris(roi),\n",
    "                         exclude_border=3,\n",
    "                         threshold_rel=0.001)\n",
    "\n",
    "# Shift keypoints by bbox offset\n",
    "keypoints[:,1] += x\n",
    "keypoints[:,0] += y\n",
    "\n",
    "# Plot kepoints\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.imshow(frames[0])\n",
    "plt.scatter(keypoints[:,1], keypoints[:,0],\n",
    "            facecolors='none', edgecolors='r')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from motion import compute_error\n",
    "\n",
    "# Initailze keypoints abd bounding box\n",
    "kp_I = keypoints\n",
    "x, y, w, h = gt_bboxes[0]\n",
    "bboxes = [(x, y, w, h)]\n",
    "\n",
    "for i in range(len(frames)-1):\n",
    "    I = frames[i] # current frame\n",
    "    J = frames[i+1] # next frame\n",
    "    \n",
    "    # estimate keypoints in frame J\n",
    "    flow_vectors = pyramid_lucas_kanade(I, J, kp_I)\n",
    "    kp_J = kp_I + flow_vectors\n",
    "    \n",
    "    # Leave out lost points\n",
    "    new_keypoints = []\n",
    "    for yi, xi, yj, xj in np.hstack((kp_I, kp_J)):\n",
    "        if yj > J.shape[0]-2 or yj < 1 or xj > J.shape[1]-2 or xj < 1:\n",
    "            print('out of bound')\n",
    "            continue\n",
    "        else:\n",
    "            patch_I = I[int(yi)-1:int(yi)+2, int(xi)-1:int(xi)+2]\n",
    "            patch_J = J[int(yj)-1:int(yj)+2, int(xj)-1:int(xj)+2]\n",
    "            error = compute_error(patch_I, patch_J)\n",
    "            if error > 3.0:\n",
    "                continue\n",
    "            else:\n",
    "                new_keypoints.append([yj, xj])\n",
    "    \n",
    "    # Update keypoints\n",
    "    kp_I = np.array(new_keypoints)\n",
    "    \n",
    "    # Find bounding box around the keypoints\n",
    "    if len(kp_I) > 0:\n",
    "        x = int(kp_I[:,1].min())\n",
    "        y = int(kp_I[:,0].min())\n",
    "        w = int(kp_I[:,1].max()) - x\n",
    "        h = int(kp_I[:,0].max()) - y\n",
    "    else:\n",
    "        (x, y, w, h) = (0, 0, 0, 0)\n",
    "    bboxes.append((x,y,w,h))\n",
    "\n",
    "    # Refresh keypoints every 20 frames\n",
    "    if (i+1) % 20 == 0 and (w * h > 0):\n",
    "        roi = J[y:y+h, x:x+w]\n",
    "        new_keypoints = corner_peaks(corner_harris(roi),\n",
    "                                 exclude_border=5,\n",
    "                                 threshold_rel=0.01)\n",
    "        new_keypoints[:,1] += x\n",
    "        new_keypoints[:,0] += y\n",
    "        kp_I = np.vstack((kp_I, new_keypoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = animated_bbox(frames, bboxes)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Evaluating Object Tracker: intersection over union (IoU)\n",
    "Intersection over union (IoU) is a common metric for evaluating performance of an object tracker. The IoU of two bounding boxes is defined as:\n",
    "$$\n",
    "\\text{IoU} = \\frac{\\text{area of intersection}}{\\text{area of union}}\n",
    "$$\n",
    "Implement `IoU` in `motion.py` to evaluate our object tracker. With default parameters, you will get IoU score of ~0.37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from motion import IoU\n",
    "\n",
    "average_iou = 0.0\n",
    "for gt_bbox, bbox in zip(gt_bboxes, bboxes):\n",
    "    average_iou += IoU(gt_bbox, bbox)\n",
    "    \n",
    "average_iou /= len(gt_bboxes)\n",
    "print(average_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit: Optimizing Object Tracker\n",
    "Optimize the object tracker in the code cell below. You may modify the code and define new parameters. We will grant extra credit for IoU score > 0.45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from motion import compute_error\n",
    "\n",
    "### Define your parameters here\n",
    "### Describe the paramters in comments\n",
    "error_thresh = 3.0\n",
    "n_frames = 20\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# You may modify the code below to use the params you\n",
    "# define above.\n",
    "#####################################################\n",
    "\n",
    "# Find features to track within the bounding box\n",
    "x, y, w, h = gt_bboxes[0]\n",
    "roi = frames[0][y:y+h, x:x+w]\n",
    "keypoints = corner_peaks(corner_harris(roi),\n",
    "                         exclude_border=3,\n",
    "                         threshold_rel=0.001)\n",
    "\n",
    "# Shift keypoints by bbox offset\n",
    "keypoints[:,1] += x\n",
    "keypoints[:,0] += y\n",
    "\n",
    "# Initailze keypoints and bounding box\n",
    "kp_I = keypoints\n",
    "x, y, w, h = gt_bboxes[0]\n",
    "bboxes = [(x, y, w, h)]\n",
    "\n",
    "# Start tracking\n",
    "for i in range(len(frames)-1):\n",
    "    I = frames[i] # Current frame\n",
    "    J = frames[i+1] # Next frame\n",
    "    flow_vectors = pyramid_lucas_kanade(I, J, kp_I) # Compute flow vectors\n",
    "    kp_J = kp_I + flow_vectors # Estimate keypoints in frame J\n",
    "    \n",
    "    new_keypoints = []\n",
    "    for yi, xi, yj, xj in np.hstack((kp_I, kp_J)):\n",
    "        # keypoint falls outside the image\n",
    "        if yj > J.shape[0]-2 or yj < 1 or xj > J.shape[1]-2 or xj < 1:\n",
    "            print('out of bound')\n",
    "            continue\n",
    "        else:\n",
    "            # Compute error to find lost points\n",
    "            patch_I = I[int(yi)-1:int(yi)+2, int(xi)-1:int(xi)+2]\n",
    "            patch_J = J[int(yj)-1:int(yj)+2, int(xj)-1:int(xj)+2]\n",
    "            error = compute_error(patch_I, patch_J)\n",
    "            if error > error_thresh:\n",
    "                continue\n",
    "            else:\n",
    "                new_keypoints.append([yj, xj])\n",
    "    \n",
    "    # Update keypoints\n",
    "    kp_I = np.array(new_keypoints)\n",
    "    \n",
    "    # Find bounding box around the keypoints\n",
    "    if len(kp_I) > 0:\n",
    "        x = int(kp_I[:,1].min())\n",
    "        y = int(kp_I[:,0].min())\n",
    "        w = int(kp_I[:,1].max()) - x\n",
    "        h = int(kp_I[:,0].max()) - y\n",
    "    else:\n",
    "        (x, y, w, h) = (0, 0, 0, 0)\n",
    "    bboxes.append((x,y,w,h))\n",
    "\n",
    "    # Detect new keypoints every n_frames\n",
    "    if (i+1) % n_frames == 0 and (w * h > 0):\n",
    "        roi = J[y:y+h, x:x+w]\n",
    "        new_keypoints = corner_peaks(corner_harris(roi),\n",
    "                                 exclude_border=5,\n",
    "                                 threshold_rel=0.01)\n",
    "        new_keypoints[:,1] += x\n",
    "        new_keypoints[:,0] += y\n",
    "        kp_I = np.vstack((kp_I, new_keypoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from motion import IoU\n",
    "\n",
    "average_iou = 0.0\n",
    "for gt_bbox, bbox in zip(gt_bboxes, bboxes):\n",
    "    average_iou += IoU(gt_bbox, bbox)\n",
    "    \n",
    "average_iou /= len(gt_bboxes)\n",
    "print(average_iou)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}