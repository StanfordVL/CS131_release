{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "In this homework, we will implement a simplified version of object detection process. Note that the tests on the notebook are not comprehensive, autograder will contain more tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import random \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from skimage import io\n",
    "from skimage.feature import hog\n",
    "from skimage import data, color, exposure\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "import glob, os\n",
    "import fnmatch\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from detection import *\n",
    "from visualization import *\n",
    "from utils import *\n",
    "\n",
    "# This code is to make matplotlib figures appear inline in the\n",
    "# notebook rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 1: Hog Representation (5 points)\n",
    "\n",
    "In this section, we will compute the average hog representation of human faces.<br>\n",
    "There are 31 aligned face images provided in the `\\face` folder. They are all aligned and have the same size. We will get an average face from these images and compute a hog feature representation for the averaged face. <br>\n",
    "Use the hog function provided by skimage library, and implement a hog representation of objects.\n",
    "Implement **`hog_feature`** function in `detection.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_paths = fnmatch.filter(os.listdir('./face'), '*.jpg')\n",
    "list.sort(image_paths)\n",
    "n = len(image_paths)\n",
    "face_shape, avg_face = load_faces(image_paths, n)\n",
    "\n",
    "(face_feature, hog_image) = hog_feature(avg_face)\n",
    "\n",
    "print(\"Sum of face feature = \", np.sum(face_feature))\n",
    "assert np.abs(np.sum(face_feature) - 499.970465079) < 1e-2\n",
    "\n",
    "plot_part1(avg_face, hog_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Sliding Window (20 points)\n",
    "Implement **`sliding_window`** function to have windows slide across an image with a specific window size. The window slides through the image and checks if an object is detected with a high similarity score with the template at every location. We compute these scores as the dot product of the HoG features of the template and the HoG features of each window as the windows slide through the image. These scores will generate a response map and you will be able to find the location of the window with the highest hog score. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'image_0001.jpg'\n",
    "image = io.imread(image_path, as_gray=True)\n",
    "image = rescale(image, 0.8)\n",
    "\n",
    "(winH, winW) = face_shape\n",
    "(score, r, c, response_map_resized, response_map) = \\\n",
    "    sliding_window(image, face_feature, step_size=30, window_size=face_shape, return_unresized_response=True)\n",
    "\n",
    "print(\"Maximum HoG face feature score over sliding window = \", score)\n",
    "crop = image[r:r+winH, c:c+winW]\n",
    "plot_part2(image, r, c, response_map_resized, response_map, winW, winH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sliding window successfully found the human face in the above example. However, in the cell below, we are only changing the scale of the image, and you can see that sliding window does not work once the scale of the image is changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'image_0001.jpg'\n",
    "image = io.imread(image_path, as_gray=True)\n",
    "image = rescale(image, 1.2)\n",
    "\n",
    "(winH, winW) = face_shape\n",
    "(score, r, c, response_map_resized, response_map) = \\\n",
    "    sliding_window(image, face_feature, step_size=30, window_size=face_shape, return_unresized_response=True)\n",
    "\n",
    "print(\"Maximum HoG face feature score over sliding window = \", score)\n",
    "crop = image[r:r+winH, c:c+winW]\n",
    "plot_part2(image, r, c, response_map_resized, response_map, winW, winH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Image Pyramids (20 points)\n",
    "In order to make sliding window work for different scales of images, you need to implement image pyramids where you resize the image to different scales and run the sliding window method on each resized image. This way you scale the objects and can detect both small and large objects. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1 Image Pyramid (5 points)\n",
    "\n",
    "Implement **`pyramid`** function in `detection.py`, this will create pyramid of images at different scales. Run the following code, and you will see the shape of the original image gets smaller until it reaches a minimum size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'image_0001.jpg'\n",
    "\n",
    "image = io.imread(image_path, as_gray=True)\n",
    "image = rescale(image, 1.2)\n",
    "\n",
    "images = pyramid(image, scale = 0.9)\n",
    "\n",
    "plot_part3_1(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.2 Pyramid Score (15 points)\n",
    "\n",
    "After getting the image pyramid, we will run sliding window on all the images to find a place that gets the highest score. Implement **`pyramid_score`** function in `detection.py`. It will return the highest score and its related information in the image pyramids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'image_0001.jpg'\n",
    "\n",
    "image = io.imread(image_path, as_gray=True)\n",
    "image = rescale(image, 1.2)\n",
    "\n",
    "(winH, winW) = face_shape\n",
    "max_score, maxr, maxc, max_scale, max_response_map = pyramid_score \\\n",
    "        (image, face_feature, face_shape, step_size = 30, scale=0.8)\n",
    "\n",
    "print(\"Maximum HoG face feature score over pyramid and sliding window = \", max_score)\n",
    "plot_part3_2(image, max_scale, winW, winH, maxc, maxr, max_response_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above example, we can see that image pyramid has fixed the problem of scaling. Then in the example below, we will try another image and implement a deformable parts model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'image_0338.jpg'\n",
    "image = io.imread(image_path, as_gray=True)\n",
    "image = rescale(image, 1.0)\n",
    "\n",
    "(winH, winW) = face_shape\n",
    "\n",
    "max_score, maxr, maxc, max_scale, max_response_map = pyramid_score \\\n",
    "    (image, face_feature, face_shape, step_size = 30, scale=0.8)\n",
    "\n",
    "print(\"Maximum HoG face feature score over pyramid and sliding window = \", max_score)\n",
    "plot_part3_2(image, max_scale, winW, winH, maxc, maxr, max_response_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Deformable Parts Detection (15 Points)\n",
    "In order to solve the problem above, you will implement deformable parts model in this section, and apply it on human faces. <br>\n",
    "The first step is to get a detector for each part of the face, including left eye, right eye, nose and mouth. <br>\n",
    "For example for the left eye, we have provided the groundtruth location of left eyes for each image in the `\\face` directory. This is stored in the `lefteyes` array with shape `(n,2)`, each row is the `(r,c)` location of the center of left eye. You will then find the average hog representation of the left eyes in the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run through the following code to get a detector for left eyes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = fnmatch.filter(os.listdir('./face'), '*.jpg')\n",
    "\n",
    "parts = read_facial_labels(image_paths)\n",
    "lefteyes, righteyes, noses, mouths = parts\n",
    "\n",
    "# Typical shape for left eye\n",
    "lefteye_h = 10\n",
    "lefteye_w = 20\n",
    "\n",
    "lefteye_shape = (lefteye_h, lefteye_w)\n",
    "\n",
    "avg_lefteye = get_detector(lefteye_h, lefteye_w, lefteyes, image_paths)\n",
    "(lefteye_feature, lefteye_hog) = hog_feature(avg_lefteye, pixel_per_cell=2)\n",
    "\n",
    "plot_part4(avg_lefteye, lefteye_hog, 'left eye')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run through the following code to get a detector for right eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "righteye_h = 10\n",
    "righteye_w = 20\n",
    "\n",
    "righteye_shape = (righteye_h, righteye_w)\n",
    "\n",
    "avg_righteye = get_detector(righteye_h, righteye_w, righteyes, image_paths)\n",
    "\n",
    "(righteye_feature, righteye_hog) = hog_feature(avg_righteye, pixel_per_cell=2)\n",
    "\n",
    "plot_part4(avg_righteye, righteye_hog, 'right eye')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run through the following code to get a detector for nose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nose_h = 30\n",
    "nose_w = 26\n",
    "\n",
    "nose_shape = (nose_h, nose_w)\n",
    "\n",
    "avg_nose = get_detector(nose_h, nose_w, noses, image_paths)\n",
    "\n",
    "(nose_feature, nose_hog) = hog_feature(avg_nose, pixel_per_cell=2)\n",
    "\n",
    "plot_part4(avg_nose, nose_hog, 'nose')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run through the following code to get a detector for mouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouth_h = 20\n",
    "mouth_w = 36\n",
    "\n",
    "mouth_shape = (mouth_h, mouth_w)\n",
    "\n",
    "avg_mouth = get_detector(mouth_h, mouth_w, mouths, image_paths)\n",
    "\n",
    "(mouth_feature, mouth_hog) = hog_feature(avg_mouth, pixel_per_cell=2)\n",
    "\n",
    "detectors_list = [lefteye_feature, righteye_feature, nose_feature, mouth_feature]\n",
    "\n",
    "plot_part4(avg_mouth, mouth_hog, 'mouth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.1 Compute displacement (10 points)\n",
    "\n",
    "Implement **`compute_displacement`** to get an average shift vector mu and standard deviation sigma for each part of the face. The vector mu is the distance from the main center, i.e the center of the face, to the center of the part. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for compute_displacement\n",
    "test_array = np.array([[0,1],[1,2],[2,3],[3,4]])\n",
    "test_shape = (6,6)\n",
    "mu, std = compute_displacement(test_array, test_shape)\n",
    "assert(np.all(mu == [1.5,0.5]))\n",
    "assert(np.sum(std-[ 1.11803399,  1.11803399])<1e-5)\n",
    "print(\"Your implementation is correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lefteye_mu, lefteye_std = compute_displacement(lefteyes, face_shape)\n",
    "righteye_mu, righteye_std = compute_displacement(righteyes, face_shape)\n",
    "nose_mu, nose_std = compute_displacement(noses, face_shape)\n",
    "mouth_mu, mouth_std = compute_displacement(mouths, face_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the shift vectors, we can run our detector on a test image. We will first run the following code to detect each part of left eye, right eye, nose and mouth in  the image. You will see a response map for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'image_0338.jpg'\n",
    "image = io.imread(image_path, as_gray=True)\n",
    "image = rescale(image, 1.0)\n",
    "\n",
    "(face_H, face_W) = face_shape\n",
    "max_score, face_r, face_c, face_scale, face_response_map = pyramid_score\\\n",
    "    (image, face_feature, face_shape,step_size = 30, scale=0.8)\n",
    "\n",
    "print(\"Maximum HoG face feature score over pyramid and sliding window = \", max_score)\n",
    "plot_part5_1(face_response_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score, lefteye_r, lefteye_c, lefteye_scale, lefteye_response_map = \\\n",
    "    pyramid_score(image, lefteye_feature,lefteye_shape, step_size = 20,scale=0.9, pixel_per_cell = 2)\n",
    "\n",
    "lefteye_response_map = resize(lefteye_response_map, face_response_map.shape)\n",
    "\n",
    "print(\"Maximum HoG face feature score over pyramid and sliding window = \", max_score)\n",
    "plot_part5_1(lefteye_response_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score, righteye_r, righteye_c, righteye_scale, righteye_response_map = \\\n",
    "    pyramid_score (image, righteye_feature, righteye_shape, step_size = 20,scale=0.9, pixel_per_cell=2)\n",
    "\n",
    "righteye_response_map = resize(righteye_response_map, face_response_map.shape)\n",
    "\n",
    "print(\"Maximum HoG face feature score over pyramid and sliding window = \", max_score)\n",
    "plot_part5_1(righteye_response_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score, nose_r, nose_c, nose_scale, nose_response_map = \\\n",
    "    pyramid_score (image, nose_feature, nose_shape, step_size = 20,scale=0.9, pixel_per_cell = 2)\n",
    "\n",
    "nose_response_map = resize(nose_response_map, face_response_map.shape)\n",
    "\n",
    "print(\"Maximum HoG face feature score over pyramid and sliding window = \", max_score)\n",
    "plot_part5_1(nose_response_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score, mouth_r, mouth_c, mouth_scale, mouth_response_map =\\\n",
    "    pyramid_score (image, mouth_feature, mouth_shape, step_size = 20,scale=0.9, pixel_per_cell = 2)\n",
    "\n",
    "mouth_response_map = resize(mouth_response_map, face_response_map.shape)\n",
    "\n",
    "print(\"Maximum HoG face feature score over pyramid and sliding window = \", max_score)\n",
    "plot_part5_1(mouth_response_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.2 Shift heatmap (5 points)\n",
    "\n",
    "After getting the response maps for each part of the face, we will shift these maps so that they all have the same center as the face. We have calculated the shift vector mu in `compute_displacement`, so we are shifting based on vector mu. Implement `shift_heatmap` function in `detection.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_heatmap_shifted = shift_heatmap(face_response_map, [0,0])\n",
    "print(\"Heatmap face max and min = \", face_heatmap_shifted.max(), face_heatmap_shifted.min())\n",
    "print(\"Heatmap face max location = \", np.unravel_index(face_heatmap_shifted.argmax(), face_heatmap_shifted.shape))\n",
    "plot_part5_2_face(face_heatmap_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lefteye_heatmap_shifted = shift_heatmap(lefteye_response_map, lefteye_mu)\n",
    "righteye_heatmap_shifted = shift_heatmap(righteye_response_map, righteye_mu)\n",
    "nose_heatmap_shifted = shift_heatmap(nose_response_map, nose_mu)\n",
    "mouth_heatmap_shifted = shift_heatmap(mouth_response_map, mouth_mu)\n",
    "\n",
    "print(\"Heatmap left eye max and min = \", \n",
    "      lefteye_heatmap_shifted.max(), lefteye_heatmap_shifted.min())\n",
    "print(\"Heatmap left eye max location = \", \n",
    "      np.unravel_index(lefteye_heatmap_shifted.argmax(), lefteye_heatmap_shifted.shape))\n",
    "\n",
    "print(\"Heatmap right eye max and min = \", \n",
    "      righteye_heatmap_shifted.max(), righteye_heatmap_shifted.min())\n",
    "print(\"Heatmap right eye max location = \", \n",
    "      np.unravel_index(righteye_heatmap_shifted.argmax(), righteye_heatmap_shifted.shape))\n",
    "\n",
    "print(\"Heatmap nose max and min = \", \n",
    "      nose_heatmap_shifted.max(), nose_heatmap_shifted.min())\n",
    "print(\"Heatmap nose max location = \", \n",
    "      np.unravel_index(nose_heatmap_shifted.argmax(), nose_heatmap_shifted.shape))\n",
    "\n",
    "print(\"Heatmap mouth max and min = \", \n",
    "      mouth_heatmap_shifted.max(), mouth_heatmap_shifted.min())\n",
    "print(\"Heatmap mouth max location = \", \n",
    "      np.unravel_index(mouth_heatmap_shifted.argmax(), mouth_heatmap_shifted.shape))\n",
    "\n",
    "plot_part5_2_parts(lefteye_heatmap_shifted, righteye_heatmap_shifted,\n",
    "                 nose_heatmap_shifted, mouth_heatmap_shifted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Gaussian Filter (15 points)\n",
    "\n",
    "## Part 5.1 Gaussian Filter (10 points)\n",
    "In this part, apply gaussian filter convolution to each heatmap. Blur by kernel of standard deviation sigma, and then add the heatmaps of the parts with the heatmap of the face. On the combined heatmap, find the maximum value and its location. You can use function provided by skimage to implement **`gaussian_heatmap`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_face = face_heatmap_shifted\n",
    "\n",
    "heatmaps = [lefteye_heatmap_shifted, \n",
    "           righteye_heatmap_shifted,\n",
    "           nose_heatmap_shifted,\n",
    "           mouth_heatmap_shifted]\n",
    "sigmas = [lefteye_std, righteye_std, nose_std, mouth_std]\n",
    "\n",
    "heatmap, i , j = gaussian_heatmap(heatmap_face, heatmaps, sigmas)\n",
    "print(\"Heatmap shape = \", heatmap.shape)\n",
    "print(\"Image shape = \", image.shape)\n",
    "print(\"Gaussian heatmap max and min = \", heatmap.max(), heatmap.min())\n",
    "print(\"Gaussian heatmap max location = \", np.unravel_index(heatmap.argmax(), heatmap.shape))\n",
    "\n",
    "print(\"Resizing heatmap to image shape ...\")\n",
    "plot_part6_1(winH, winW, heatmap, image, i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Result Analysis (5 points)\n",
    "\n",
    "Does your DPM work on detecting human faces? Can you think of a case where DPM may work better than the detector we had in part 3 (sliding window + image pyramid)? You can also have examples that are not faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:** Write your answer in this markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "You have tried detecting one face from the image, and the next step is to extend it to detecting multiple occurences of the object. For example in the following image, how do you detect more than one face from your response map? Implement the function **`detect_multiple`**, and write code to visualize your detected faces in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'image_0002.jpg'\n",
    "image = io.imread(image_path, as_gray=True)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'image_0002.jpg'\n",
    "image = io.imread(image_path, as_gray=True)\n",
    "heatmap = get_heatmap(image, face_feature, face_shape, detectors_list, parts)\n",
    "\n",
    "plt.imshow(heatmap, cmap='viridis', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_faces = detect_multiple(image, heatmap)\n",
    "\n",
    "# Visualize your detected faces\n",
    "\n",
    "### YOUR CODE HERE\n",
    "pass\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: K-Nearest Neighbors Classification (25 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Dataset\n",
    "\n",
    "We will use a dataset of faces of celebrities. Download the dataset using the following command:\n",
    "\n",
    "    sh get_dataset.sh\n",
    "\n",
    "The face dataset for CS131 assignment.\n",
    "The directory containing the dataset has the following structure:\n",
    "\n",
    "    faces/\n",
    "        train/\n",
    "            angelina jolie/\n",
    "            anne hathaway/\n",
    "            ...\n",
    "        test/\n",
    "            angelina jolie/\n",
    "            anne hathaway/\n",
    "            ...\n",
    "\n",
    "Each class has 50 training images and 10 testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_knn import load_dataset\n",
    "\n",
    "X_train, y_train, classes_train = load_dataset('faces', train=True, as_gray=True)\n",
    "X_test, y_test, classes_test = load_dataset('faces', train=False, as_gray=True)\n",
    "\n",
    "assert classes_train == classes_test\n",
    "classes = classes_train\n",
    "\n",
    "print('Class names:', classes)\n",
    "print('Training data shape:', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape:', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 10\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx])\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the image data into rows\n",
    "# we now have one 4096 dimensional featue vector for each example\n",
    "X_train_flat = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test_flat = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "print(\"New training data shape:\", X_train_flat.shape)\n",
    "print(\"New test data shape:\", X_test_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6.1: Cross Validation on Raw Pixel Features (15 Points)\n",
    "\n",
    "We're now going to try to classify the test images using the k-nearest neighbors algorithm on the **raw features of the images** (i.e. the pixel values themselves). We will see later how we can use kNN on better features.\n",
    "\n",
    "The gist of the k-nearest neighbors algorithm is to predict a test image's class based on which classes the k nearest train images belong to.  For example, using k = 3, if we found that for test image X, the three nearest train images were 2 pictures of Angelina Jolie, and one picture of Audrey Hepburn, we would predict that the test image X is a picture of Angelina Jolie.\n",
    "\n",
    "Here are the steps that we will follow:\n",
    "\n",
    "1. We compute the L2 distances between every element of X_test and every element of X_train in `compute_distances`.\n",
    "2. We split the dataset into 5 folds for cross-validation in `split_folds`.\n",
    "3. For each fold, and for different values of `k`, we predict the labels and measure accuracy.\n",
    "4. Using the best `k` found through cross-validation, we measure accuracy on the test set.\n",
    "\n",
    "Resources for understanding cross-validation:\n",
    "https://towardsdatascience.com/why-and-how-to-cross-validate-a-model-d6424b45261f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from k_nearest_neighbor import compute_distances\n",
    "\n",
    "# Step 1: compute the distances between all features from X_train and from X_test\n",
    "dists = compute_distances(X_test_flat, X_train_flat)\n",
    "assert dists.shape == (160, 800)\n",
    "print(\"dists shape:\", dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from k_nearest_neighbor import predict_labels\n",
    "\n",
    "# We use k = 1 (which corresponds to only taking the nearest neighbor to decide)\n",
    "y_test_pred = predict_labels(dists, y_train, k=1)\n",
    "\n",
    "# Compute and print the fraction of correctly predicted examples\n",
    "num_test = y_test.shape[0]\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / num_test\n",
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "We don't know the best value for our parameter `k`.  \n",
    "There is no theory on how to choose an optimal `k`, and the way to choose it is through cross-validation.\n",
    "\n",
    "We **cannot** compute any metric on the test set to choose the best `k`, because we want our final test accuracy to reflect a real use case. This real use case would be a setting where we have new examples come and we classify them on the go. There is no way to check the accuracy beforehand on that set of test examples to determine `k`.\n",
    "\n",
    "Cross-validation will make use split the data into different fold (5 here).  \n",
    "For each fold, if we have a total of 5 folds we will have:\n",
    "- 80% of the data as training data\n",
    "- 20% of the data as validation data\n",
    "\n",
    "We will compute the accuracy on the validation accuracy for each fold, and use the mean of these 5 accuracies to determine the best parameter `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from k_nearest_neighbor import split_folds\n",
    "\n",
    "# Step 2: split the data into 5 folds to perform cross-validation.\n",
    "num_folds = 5\n",
    "\n",
    "X_trains, y_trains, X_vals, y_vals = split_folds(X_train_flat, y_train, num_folds)\n",
    "\n",
    "assert X_trains.shape == (5, 640, 4096)\n",
    "assert y_trains.shape == (5, 640)\n",
    "assert X_vals.shape == (5, 160, 4096)\n",
    "assert y_vals.shape == (5, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Measure the mean accuracy for each value of `k`\n",
    "\n",
    "# List of k to choose from\n",
    "k_choices = list(range(5, 101, 5))\n",
    "\n",
    "# Dictionnary mapping k values to accuracies\n",
    "# For each k value, we will have `num_folds` accuracies to compute\n",
    "# k_to_accuracies[1] will be for instance [0.22, 0.23, 0.19, 0.25, 0.20] for 5 folds\n",
    "k_to_accuracies = {}\n",
    "\n",
    "for k in k_choices:\n",
    "    print(\"Running for k=%d\" % k)\n",
    "    accuracies = []\n",
    "    for i in range(num_folds):\n",
    "        # Make predictions\n",
    "        fold_dists = compute_distances(X_vals[i], X_trains[i])\n",
    "        y_pred = predict_labels(fold_dists, y_trains[i], k)\n",
    "\n",
    "        # Compute and print the fraction of correctly predicted examples\n",
    "        num_correct = np.sum(y_pred == y_vals[i])\n",
    "        accuracy = float(num_correct) / len(y_vals[i])\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    k_to_accuracies[k] = accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the raw observations\n",
    "plt.figure(figsize=(12,8))\n",
    "for k in k_choices:\n",
    "    accuracies = k_to_accuracies[k]\n",
    "    plt.scatter([k] * len(accuracies), accuracies)\n",
    "\n",
    "# plot the trend line with error bars that correspond to standard deviation\n",
    "accuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])\n",
    "accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])\n",
    "plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)\n",
    "plt.title('Cross-validation on k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Cross-validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the cross-validation results above, choose the best value for k,   \n",
    "# retrain the classifier using all the training data, and test it on the test\n",
    "# data. You should be able to get above 26% accuracy on the test data.\n",
    "\n",
    "best_k = None\n",
    "### YOUR CODE HERE\n",
    "# Choose the best k based on the cross validation above\n",
    "pass\n",
    "### END YOUR CODE\n",
    "\n",
    "y_test_pred = predict_labels(dists, y_train, k=best_k)\n",
    "\n",
    "# Compute and display the accuracy\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / num_test\n",
    "print('For k = %d, got %d / %d correct => accuracy: %f' % (best_k, num_correct, num_test, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6.2: Cross Validation on HOG Features (10 Points)\n",
    "\n",
    "We're now going to try to classify the test images using the k-nearest neighbors algorithm on HOG features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HOG datasets\n",
    "X_train_hog = [hog_feature(x)[0] for x in X_train]\n",
    "X_test_hog = [hog_feature(x)[0] for x in X_test]\n",
    "print(\"Loaded {} HoG features.\".format(len(X_train_hog)))\n",
    "print(\"Loaded {} HoG features.\".format(len(X_test_hog)))\n",
    "\n",
    "X_train_hog = np.stack(X_train_hog)\n",
    "X_test_hog = np.stack(X_test_hog)\n",
    "print(\"HOG Training data shape:\", X_train_hog.shape)\n",
    "print(\"HOG Test data shape:\", X_test_hog.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Cross Validation datasets\n",
    "num_folds = 5\n",
    "X_hog_trains, y_trains, X_hog_vals, y_vals = split_folds(X_train_hog, y_train, num_folds)\n",
    "\n",
    "    \n",
    "# List of k to choose from\n",
    "k_choices = list(range(5, 101, 5))\n",
    "k_to_accuracies = {}\n",
    "\n",
    "for k in k_choices:\n",
    "    print(\"Running for k=%d\" % k)\n",
    "    accuracies = []\n",
    "    for i in range(num_folds):\n",
    "        # Make predictions\n",
    "        fold_dists = compute_distances(X_hog_vals[i], X_hog_trains[i])\n",
    "        y_pred = predict_labels(fold_dists, y_trains[i], k)\n",
    "\n",
    "        # Compute and print the fraction of correctly predicted examples\n",
    "        num_correct = np.sum(y_pred == y_vals[i])\n",
    "        accuracy = float(num_correct) / len(y_vals[i])\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    k_to_accuracies[k] = accuracies\n",
    "    \n",
    "    \n",
    "# plot the raw observations\n",
    "plt.figure(figsize=(12,8))\n",
    "for k in k_choices:\n",
    "    accuracies = k_to_accuracies[k]\n",
    "    plt.scatter([k] * len(accuracies), accuracies)\n",
    "\n",
    "# plot the trend line with error bars that correspond to standard deviation\n",
    "accuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])\n",
    "accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])\n",
    "plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)\n",
    "plt.title('Cross-validation on k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Cross-validation accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the cross-validation results above, choose the best value for k,   \n",
    "# retrain the classifier using all the training data, and test it on the test\n",
    "# data. You should be able to get above 50% accuracy on the test data.\n",
    "\n",
    "best_k = None\n",
    "### YOUR CODE HERE\n",
    "# Choose the best k based on the cross validation above\n",
    "pass\n",
    "### END YOUR CODE\n",
    "\n",
    "dists = compute_distances(X_test_hog, X_train_hog)\n",
    "y_test_pred = predict_labels(dists, y_train, k=best_k)\n",
    "\n",
    "# Compute and display the accuracy\n",
    "num_test = X_test_hog.shape[0]\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = float(num_correct) / num_test\n",
    "print('For k = %d, got %d / %d correct => accuracy: %f' % (best_k, num_correct, num_test, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Why did HOG features do so much better than raw pixels? You'll notice that even the luckiest high outlier of cross validation on raw pixels is outperformed by the unluckiest low outlier in HOG. Remember that the goal of this classification task is to learn to classify the identity of a profile picture using the selected feature type. How do you think we could improve to do even better?\n",
    "\n",
    "**Your Answer:** Write your answer in this markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Why did we tell you to choose the best k from cross validation, and then evaluate accuracy for that k on the test set, instead of directly evaluating a range of k values on the test set and picking the one with the best accuracy?\n",
    "\n",
    "**Your Answer:** Write your answer in this markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: How did you decide which value of k was 'best'? In a real-world scenario, if you were deploying this K-Nearest Neighbors HOG feature classifier, how would you consider the roles of the mean, variance, maximum, and/or minimum of each value of k that you observed in cross validation when choosing the 'best' k?\n",
    "\n",
    "**Your Answer:** Write your answer in this markdown cell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}