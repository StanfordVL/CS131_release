{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829eeee5-b288-4240-b668-187f7be574e8",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "In this homework, we will go through the Hough transform, line detection with RANSAC, Harris corner detector, simple descriptpr and keypoint matching, and SIFT points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ba302d-1513-49cd-8cf1-8cc5ee2ae825",
   "metadata": {},
   "source": [
    "## Notes on Running This Notebook\n",
    "\n",
    "Make sure to run each Part from it's begining to ensure that you compute all of the dependencies of your current question and don't crossover variables with the same name from other questions. For example, don't run parts 4 and 5 and then return to run only the last cell of part 3; your panorama won't be using the right transformed images!  So long as you run each Part from it's beginning, you can run the Parts in any order.\n",
    "\n",
    "When assembling your PDF, we recommend running all cells in order from the top of the notebook to prevent any of these discontinuity errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f67bb-bc9e-457b-8df7-e7ea3200c097",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872849b4-307c-484f-80b8-e963442b26d6",
   "metadata": {},
   "source": [
    "### **Step 1**\n",
    "\n",
    "First, run the cells below to clone the `CS131_release` [repo](https://github.com/StanfordVL/CS131_release) and `cd` into the correct directory in order to access some necessary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d7744-dbf3-4b5b-9800-665e5763dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"CS131_release\"):\n",
    "    # Clone the repository if it doesn't already exist\n",
    "    !git clone https://github.com/StanfordVL/CS131_release.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f1694-19ca-4030-8f11-e8be3928e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd CS131_release/winter_2025/hw2_release/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7900f-0471-45e0-b38a-4f3673e6854d",
   "metadata": {},
   "source": [
    "### **Step 2**\n",
    "Next, run the cells below to install the necessary libraries and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9508a7-624a-46a7-81e1-3512f48cedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary dependencies\n",
    "# (restart your runtime session if prompted to, and then re-run this cell)\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354be440-9b19-4d02-9834-45262e22a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the print function from newer versions of python\n",
    "from __future__ import print_function\n",
    "\n",
    "# Numpy is the main package for scientific computing with Python.\n",
    "# This will be one of our most used libraries in this class\n",
    "import numpy as np\n",
    "\n",
    "# The Time library helps us time code runtimes\n",
    "import time\n",
    "\n",
    "# PIL (Pillow) is a useful library for opening, manipulating, and saving images\n",
    "from PIL import Image\n",
    "\n",
    "# skimage (Scikit-Image) is a library for image processing\n",
    "from skimage import io\n",
    "from skimage import filters\n",
    "from skimage.feature import corner_peaks\n",
    "from skimage.io import imread\n",
    "\n",
    "from scipy.ndimage import convolve\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import cv2 as cv\n",
    "\n",
    "# Matplotlib is a useful plotting library for python\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "# This code is to make matplotlib figures appear inline in the\n",
    "# notebook rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165d3ab-baac-4958-973f-69d2b21df494",
   "metadata": {},
   "source": [
    "## Part 1: Lane Detection (15 points)\n",
    "\n",
    "In this section we will implement a simple lane detection application using Canny edge detector and Hough transform.\n",
    "Here are some example images of how your final lane detector will look like.\n",
    "<img src=\"lane1.png\" width=\"400\">\n",
    "<img src=\"lane2.png\" width=\"400\">\n",
    "\n",
    "The algorithm can broken down into the following steps:\n",
    "1. Detect edges using the Canny edge detector.\n",
    "2. Extract the edges in the region of interest (a triangle covering the bottom corners and the center of the image).\n",
    "3. Run Hough transform to detect lanes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8be60-469a-4d68-8433-4f921f2391c6",
   "metadata": {},
   "source": [
    "### 1.1 Edge detection\n",
    "Lanes on the roads are usually thin and long lines with bright colors. Our edge detection algorithm by itself should be able to find the lanes pretty well. Run the code cell below to load the example image and detect edges from the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8959b938-752e-4b10-9c1a-492af6d11b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(size, sigma):\n",
    "    \"\"\" Implementation of Gaussian Kernel.\n",
    "\n",
    "    This function follows the gaussian kernel formula,\n",
    "    and creates a kernel matrix.\n",
    "\n",
    "    Hints:\n",
    "    - Use np.pi and np.exp to compute pi and exp.\n",
    "\n",
    "    Args:\n",
    "        size: int of the size of output matrix.\n",
    "        sigma: float of sigma to calculate kernel.\n",
    "\n",
    "    Returns:\n",
    "        kernel: numpy array of shape (size, size).\n",
    "    \"\"\"\n",
    "\n",
    "    kernel = np.zeros((size, size))\n",
    "\n",
    "    k = size // 2\n",
    "\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            kernel[i,j] = np.exp(-((i-k)**2 + (j-k)**2)/(2*sigma**2)) / (2*np.pi*sigma**2)\n",
    "\n",
    "    return kernel\n",
    "\n",
    "def conv(image, kernel):\n",
    "    \"\"\" An implementation of convolution filter.\n",
    "\n",
    "    This function uses element-wise multiplication and np.sum()\n",
    "    to efficiently compute weighted sum of neighborhood at each\n",
    "    pixel.\n",
    "\n",
    "    Args:\n",
    "        image: numpy array of shape (Hi, Wi).\n",
    "        kernel: numpy array of shape (Hk, Wk).\n",
    "\n",
    "    Returns:\n",
    "        out: numpy array of shape (Hi, Wi).\n",
    "    \"\"\"\n",
    "    Hi, Wi = image.shape\n",
    "    Hk, Wk = kernel.shape\n",
    "    out = np.zeros((Hi, Wi))\n",
    "\n",
    "    # For this assignment, we will use edge values to pad the images.\n",
    "    # Zero padding will make derivatives at the image boundary very big,\n",
    "    # whereas we want to ignore the edges at the boundary.\n",
    "    pad_width0 = Hk // 2\n",
    "    pad_width1 = Wk // 2\n",
    "    pad_width = ((pad_width0,pad_width0),(pad_width1,pad_width1))\n",
    "    padded = np.pad(image, pad_width, mode='edge')\n",
    "\n",
    "    kernel = np.flip(kernel, axis=0)\n",
    "    kernel = np.flip(kernel, axis=1)\n",
    "\n",
    "    for i in range(Hi):\n",
    "        for j in range (Wi):\n",
    "            out[i, j] = np.sum(kernel * padded[i:i+Hk,j:j+Wk])\n",
    "\n",
    "    return out\n",
    "\n",
    "def partial_x(img):\n",
    "    \"\"\" Computes partial x-derivative of input img.\n",
    "\n",
    "    Hints:\n",
    "        - You may use the conv function in defined in this file.\n",
    "\n",
    "    Args:\n",
    "        img: numpy array of shape (H, W).\n",
    "    Returns:\n",
    "        out: x-derivative image.\n",
    "    \"\"\"\n",
    "\n",
    "    out = None\n",
    "    dx = np.array([[1, 0, -1]])/2\n",
    "    out = conv(img, dx)\n",
    "    return out\n",
    "\n",
    "def partial_y(img):\n",
    "    \"\"\" Computes partial y-derivative of input img.\n",
    "\n",
    "    Hints:\n",
    "        - You may use the conv function in defined in this file.\n",
    "\n",
    "    Args:\n",
    "        img: numpy array of shape (H, W).\n",
    "    Returns:\n",
    "        out: y-derivative image.\n",
    "    \"\"\"\n",
    "\n",
    "    out = None\n",
    "    dy = np.array([[1, 0, -1]]).T/2\n",
    "    out = conv(img, dy)\n",
    "    return out\n",
    "\n",
    "def gradient(img):\n",
    "    \"\"\" Returns gradient magnitude and direction of input img.\n",
    "\n",
    "    Args:\n",
    "        img: Grayscale image. Numpy array of shape (H, W).\n",
    "\n",
    "    Returns:\n",
    "        G: Magnitude of gradient at each pixel in img.\n",
    "            Numpy array of shape (H, W).\n",
    "        theta: Direction(in degrees, 0 <= theta < 360) of gradient\n",
    "            at each pixel in img. Numpy array of shape (H, W).\n",
    "\n",
    "    Hints:\n",
    "        - Use np.sqrt and np.arctan2 to calculate square root and arctan\n",
    "    \"\"\"\n",
    "    G = np.zeros(img.shape)\n",
    "    theta = np.zeros(img.shape)\n",
    "\n",
    "    Gx = partial_x(img)\n",
    "    Gy = partial_y(img)\n",
    "\n",
    "    G = np.sqrt(Gx*Gx+Gy*Gy)\n",
    "    theta = np.arctan2(Gy, Gx) / (2*np.pi) * 360\n",
    "    theta = theta % 360\n",
    "\n",
    "    return G, theta\n",
    "\n",
    "def non_maximum_suppression(G, theta):\n",
    "    \"\"\" Performs non-maximum suppression.\n",
    "\n",
    "    This function performs non-maximum suppression along the direction\n",
    "    of gradient (theta) on the gradient magnitude image (G).\n",
    "\n",
    "    Args:\n",
    "        G: gradient magnitude image with shape of (H, W).\n",
    "        theta: direction of gradients with shape of (H, W).\n",
    "\n",
    "    Returns:\n",
    "        out: non-maxima suppressed image.\n",
    "    \"\"\"\n",
    "    H, W = G.shape\n",
    "    out = np.zeros((H, W))\n",
    "\n",
    "    # Round the gradient direction to the nearest 45 degrees\n",
    "    theta = np.floor((theta + 22.5) / 45) * 45\n",
    "    theta = (theta % 360.0).astype(np.int32)\n",
    "\n",
    "    #print(G)\n",
    "\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            if theta[i,j] == 0 or theta[i,j] == 180:\n",
    "                before = G[i, max(j-1,0)]\n",
    "                after = G[i, min(j+1,W-1)]\n",
    "\n",
    "            elif theta[i,j] == 45 or theta[i,j] == (45+180):\n",
    "                if i-1 < 0 or j-1 < 0:\n",
    "                    before = 0\n",
    "                else:\n",
    "                    before = G[i-1, j-1]\n",
    "                if i+1 > H-1 or j+1 > W-1:\n",
    "                    after = 0\n",
    "                else:\n",
    "                    after = G[i+1, j+1]\n",
    "\n",
    "            elif theta[i,j] == 90 or theta[i,j] == (90+180):\n",
    "                before = G[max(i-1,0), j]\n",
    "                after = G[min(i+1,H-1), j]\n",
    "\n",
    "            else:\n",
    "                if i-1 < 0 or j+1 > W-1:\n",
    "                    before = 0\n",
    "                else:\n",
    "                    before = G[i-1, j+1]\n",
    "\n",
    "                if i+1 > H-1 or j-1 < 0:\n",
    "                    after = 0\n",
    "                else:\n",
    "                    after = G[i+1, j-1]\n",
    "\n",
    "            out[i,j] = G[i,j] if max(G[i,j], before, after) == G[i,j] else 0\n",
    "\n",
    "    return out\n",
    "\n",
    "def double_thresholding(img, high, low):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img: numpy array of shape (H, W) representing NMS edge response.\n",
    "        high: high threshold(float) for strong edges.\n",
    "        low: low threshold(float) for weak edges.\n",
    "\n",
    "    Returns:\n",
    "        strong_edges: Boolean array representing strong edges.\n",
    "            Strong edeges are the pixels with the values greater than\n",
    "            the higher threshold.\n",
    "        weak_edges: Boolean array representing weak edges.\n",
    "            Weak edges are the pixels with the values smaller or equal to the\n",
    "            higher threshold and greater than the lower threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    strong_edges = np.zeros(img.shape, dtype=np.bool)\n",
    "    weak_edges = np.zeros(img.shape, dtype=np.bool)\n",
    "\n",
    "    strong_edges = img > high\n",
    "    weak_edges = (img > low) * (img <= high)\n",
    "\n",
    "    return strong_edges, weak_edges\n",
    "\n",
    "def link_edges(strong_edges, weak_edges):\n",
    "    \"\"\" Find weak edges connected to strong edges and link them.\n",
    "\n",
    "    Iterate over each pixel in strong_edges and perform breadth first\n",
    "    search across the connected pixels in weak_edges to link them.\n",
    "    Here we consider a pixel (a, b) is connected to a pixel (c, d)\n",
    "    if (a, b) is one of the eight neighboring pixels of (c, d).\n",
    "\n",
    "    Args:\n",
    "        strong_edges: binary image of shape (H, W).\n",
    "        weak_edges: binary image of shape (H, W).\n",
    "\n",
    "    Returns:\n",
    "        edges: numpy boolean array of shape(H, W).\n",
    "    \"\"\"\n",
    "\n",
    "    H, W = strong_edges.shape\n",
    "    indices = np.stack(np.nonzero(strong_edges)).T\n",
    "    edges = np.zeros((H, W), dtype=np.bool)\n",
    "\n",
    "    # Make new instances of arguments to leave the original\n",
    "    # references intact\n",
    "    weak_edges = np.copy(weak_edges)\n",
    "    edges = np.copy(strong_edges)\n",
    "\n",
    "    for r, c in indices:\n",
    "        Q = []\n",
    "        Q.append((r, c))\n",
    "\n",
    "        while Q:\n",
    "            y, x = Q.pop(0)\n",
    "            for i, j in get_neighbors(y, x, H, W):\n",
    "                if weak_edges[i, j] == True:\n",
    "                    weak_edges[i, j] = False\n",
    "                    edges[i, j] = True\n",
    "                    Q.append((i, j))\n",
    "    return edges\n",
    "\n",
    "def get_neighbors(y, x, H, W):\n",
    "    \"\"\" Return indices of valid neighbors of (y, x).\n",
    "\n",
    "    Return indices of all the valid neighbors of (y, x) in an array of\n",
    "    shape (H, W). An index (i, j) of a valid neighbor should satisfy\n",
    "    the following:\n",
    "        1. i >= 0 and i < H\n",
    "        2. j >= 0 and j < W\n",
    "        3. (i, j) != (y, x)\n",
    "\n",
    "    Args:\n",
    "        y, x: location of the pixel.\n",
    "        H, W: size of the image.\n",
    "    Returns:\n",
    "        neighbors: list of indices of neighboring pixels [(i, j)].\n",
    "    \"\"\"\n",
    "    neighbors = []\n",
    "\n",
    "    for i in (y-1, y, y+1):\n",
    "        for j in (x-1, x, x+1):\n",
    "            if i >= 0 and i < H and j >= 0 and j < W:\n",
    "                if (i == y and j == x):\n",
    "                    continue\n",
    "                neighbors.append((i, j))\n",
    "\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552454da-bdd9-45d0-9442-8f3782c339e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canny(img, kernel_size=5, sigma=1.4, high=20, low=15):\n",
    "    \"\"\" Implement canny edge detector by calling functions above.\n",
    "\n",
    "    Args:\n",
    "        img: binary image of shape (H, W).\n",
    "        kernel_size: int of size for kernel matrix.\n",
    "        sigma: float for calculating kernel.\n",
    "        high: high threshold for strong edges.\n",
    "        low: low threashold for weak edges.\n",
    "    Returns:\n",
    "        edge: numpy array of shape(H, W).\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f4bb5d-d4e7-4f1d-962f-2aa3ad0c31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = io.imread('road.jpg', as_gray=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31a825-be55-4893-a91f-c2d04c13c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Canny edge detector\n",
    "edges = canny(img, kernel_size=5, sigma=1.4, high=0.03, low=0.02)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Input Image')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(edges)\n",
    "plt.axis('off')\n",
    "plt.title('Edges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f55b7-a260-45de-8799-e8160df4256d",
   "metadata": {},
   "source": [
    "### 1.2 Extracting region of interest (ROI)\n",
    "We can see that the Canny edge detector could find the edges of the lanes. However, we can also see that there are edges of other objects that we are not interested in. Given the position and orientation of the camera, we know that the lanes will be located in the lower half of the image. The code below defines a binary mask for the ROI and extract the edges within the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6042a402-5491-41e9-8b0c-7bc81c1bd42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = img.shape\n",
    "\n",
    "# Generate mask for ROI (Region of Interest)\n",
    "mask = np.zeros((H, W))\n",
    "for i in range(H):\n",
    "    for j in range(W):\n",
    "        if i > (H / W) * j and i > -(H / W) * j + H:\n",
    "            mask[i, j] = 1\n",
    "\n",
    "# Extract edges in ROI\n",
    "roi = edges * mask\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(mask)\n",
    "plt.title('Mask')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(roi)\n",
    "plt.title('Edges in ROI')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba41f9-14a2-4d1b-9c60-33f3414566d2",
   "metadata": {},
   "source": [
    "### 1.3 Fitting lines using Hough transform (15 points)\n",
    "The output from the edge detector is still a collection of connected points. However, it would be more natural to represent a lane as a line parameterized as $y = ax + b$, with a slope $a$ and y-intercept $b$. We will use Hough transform to find parameterized lines that represent the detected edges.\n",
    "\n",
    "In general, a straight line $y = ax + b$ can be represented as a point $(a, b)$ in the parameter space. This is the parameterization we often use when introducing the Hough transform.  However, this cannot represent vertical lines as the slope parameter will be unbounded. Alternatively, we parameterize a line using $\\theta\\in{[-\\pi, \\pi]}$ and $\\rho\\in{\\mathbb{R}}$ as follows:\n",
    "\n",
    "$$\n",
    "\\rho = x\\cdot{cos\\theta} + y\\cdot{sin\\theta}\n",
    "$$\n",
    "\n",
    "Using this parameterization, we can map every point in $xy$-space to a sine-like line in $\\theta\\rho$-space (or Hough space). We then accumulate the parameterized points in the Hough space and choose points (in Hough space) with highest accumulated values. A point in Hough space then can be transformed back into a line in $xy$-space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90e4b3-a7a5-46fa-b2ca-c728a28c0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hough_transform(img):\n",
    "    \"\"\" Transform points in the input image into Hough space.\n",
    "\n",
    "    Use the parameterization:\n",
    "        rho = x * cos(theta) + y * sin(theta)\n",
    "    to transform a point (x,y) to a sine-like function in Hough space.\n",
    "\n",
    "    Args:\n",
    "        img: binary image of shape (H, W).\n",
    "        \n",
    "    Returns:\n",
    "        accumulator: numpy array of shape (m, n).\n",
    "        rhos: numpy array of shape (m, ).\n",
    "        thetas: numpy array of shape (n, ).\n",
    "    \"\"\"\n",
    "    # Set rho and theta ranges\n",
    "    W, H = img.shape\n",
    "    diag_len = int(np.ceil(np.sqrt(W * W + H * H)))\n",
    "    rhos = np.linspace(-diag_len, diag_len, diag_len * 2 + 1)\n",
    "    thetas = np.deg2rad(np.arange(-90.0, 90.0))\n",
    "\n",
    "    # Cache some reusable values\n",
    "    cos_t = np.cos(thetas)\n",
    "    sin_t = np.sin(thetas)\n",
    "    num_thetas = len(thetas)\n",
    "\n",
    "    # Initialize accumulator in the Hough space\n",
    "    accumulator = np.zeros((2 * diag_len + 1, num_thetas), dtype=np.uint64)\n",
    "    ys, xs = np.nonzero(img)\n",
    "\n",
    "    # Transform each point (x, y) in image\n",
    "    # Find rho corresponding to values in thetas\n",
    "    # and increment the accumulator in the corresponding coordiate.\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return accumulator, rhos, thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4cc50-cee9-4ad5-8c9c-aae9fd2ce83b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform Hough transform on the ROI\n",
    "acc, rhos, thetas = hough_transform(roi)\n",
    "\n",
    "# Coordinates for right lane\n",
    "xs_right = []\n",
    "ys_right = []\n",
    "\n",
    "# Coordinates for left lane\n",
    "xs_left = []\n",
    "ys_left = []\n",
    "\n",
    "for i in range(20):\n",
    "    idx = np.argmax(acc)\n",
    "    r_idx = idx // acc.shape[1]\n",
    "    t_idx = idx % acc.shape[1]\n",
    "    acc[r_idx, t_idx] = 0 # Zero out the max value in accumulator\n",
    "\n",
    "    rho = rhos[r_idx]\n",
    "    theta = thetas[t_idx]\n",
    "    \n",
    "    # Transform a point in Hough space to a line in xy-space.\n",
    "    a = - (np.cos(theta)/np.sin(theta)) # slope of the line\n",
    "    b = (rho/np.sin(theta)) # y-intersect of the line\n",
    "\n",
    "    # Break if both right and left lanes are detected\n",
    "    if xs_right and xs_left:\n",
    "        break\n",
    "    \n",
    "    if a < 0: # Left lane\n",
    "        if xs_left:\n",
    "            continue\n",
    "        xs = xs_left\n",
    "        ys = ys_left\n",
    "    else: # Right Lane\n",
    "        if xs_right:\n",
    "            continue\n",
    "        xs = xs_right\n",
    "        ys = ys_right\n",
    "\n",
    "    for x in range(img.shape[1]):\n",
    "        y = a * x + b\n",
    "        if y > img.shape[0] * 0.6 and y < img.shape[0]:\n",
    "            xs.append(x)\n",
    "            ys.append(int(round(y)))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.plot(xs_left, ys_left, linewidth=5.0)\n",
    "plt.plot(xs_right, ys_right, linewidth=5.0)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95887814-7f2b-463f-ace5-0c0abfd7fac5",
   "metadata": {},
   "source": [
    "## Part 2: Panorama Stitching \n",
    "Panorama stitching is an early success of computer vision. Matthew Brown and David G. Lowe published a famous [panoramic image stitching paper](https://link.springer.com/article/10.1007/s11263-006-0002-3) in 2007. Since then, automatic panorama stitching technology has been widely adopted in many applications such as Google Street View, panorama photos on smartphones,\n",
    "and stitching software such as Photosynth and AutoStitch.\n",
    "\n",
    "In this assignment, we will detect and match keypoints from multiple images to build a single panoramic image. This will involve several tasks:\n",
    "1. Use Harris corner detector to find keypoints.\n",
    "2. Build a descriptor to describe each point in an image. <br>\n",
    "   Compare two sets of descriptors coming from two different images and find matching keypoints.\n",
    "3. Given a list of matching keypoints, use least-squares method to find the affine transformation matrix that maps points in one image to another.\n",
    "4. Use RANSAC to give a more robust estimate of affine transformation matrix. <br>\n",
    "   Given the transformation matrix, use it to transform the second image and overlay it on the first image, forming a panorama.\n",
    "5. Implement a different descriptor (HOG descriptor) and get another stitching result.\n",
    "\n",
    "There's a lot of material to get hands-on with so we recommend starting early!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0209938c-20c3-4629-a9cd-01eb5c87ea47",
   "metadata": {},
   "source": [
    "## 2.1 Harris Corner Detector (20 points)\n",
    "\n",
    "\n",
    "In this section, you are going to implement Harris corner detector for keypoint localization. Review the lecture slides on Harris corner detector to understand how it works. The Harris detection algorithm can be divide into the following steps:\n",
    "1. Compute $x$ and $y$ derivatives ($I_x, I_y$) of an image\n",
    "2. Compute products of derivatives ($I_x^2, I_y^2, I_{xy}$) at each pixel\n",
    "3. Compute matrix $M$ at each pixel, where\n",
    "$$\n",
    "M = \\sum_{x,y} w(x,y)\n",
    "    \\begin{bmatrix}\n",
    "        I_{x}^2 & I_{x}I_{y} \\\\\n",
    "        I_{x}I_{y} & I_{y}^2\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "4. Compute corner response $R=Det(M)-k(Trace(M)^2)$ at each pixel\n",
    "5. Output corner response map $R(x,y)$\n",
    "\n",
    "Step 1 is already done for you in the function **`harris_corners`**. We used the [Sobel Operator](https://en.wikipedia.org/wiki/Sobel_operator), which computes smoothed gradients at each pixel in the x and y direction. See skimage documentation for [sobel_v](https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.sobel_v) and [sobel_h](https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.sobel_h) for more information on the sobel kernels and operators. \n",
    "\n",
    "For step 3, we've created a uniform window function w for you in the starter code. You can assume that the window size will be odd.\n",
    "\n",
    "Complete the function implementation of **`harris_corners`** and run the code below.\n",
    "\n",
    "### Hint: There are two ways to solve this problem\n",
    "\n",
    "**Vectorized**: If you want to be really efficient, you can use the function `scipy.ndimage.convolve`, and compute the response map R at every pixel all at once. If you're clever with your convolutions and determinant and trace calculations, you can compute the windowed gradients in $M$ ($\\sum_{x,y} w(x,y) \\cdot I_x^2$, and $\\sum_{x,y} w(x,y) \\cdot I_y^2$, and $\\sum_{x,y} w(x,y) \\cdot I_{xy}$), and then compute the response map without any for loops! \n",
    "\n",
    "**Iterative**: The more intuitive solution is to iterate through each pixel of the image, compute $M$ based on the surrounding neighborhood of pixel gradients in $I_x$, $I_y$, and $I_{xy}$, and then compute the response map pixel $R(x,y)$. You may find your implementations of conv_nested and conv_fast from HW 1 to be useful references! \n",
    "\n",
    "Note that you'll want to explictly specify zero-padding to match the Harris response map definition, but we'll accept the default behavior of `scipy.ndimage.convolve` as well. If you use zero-padding, both the vectorized and for-loop implementations will get you to the same answer! \n",
    "\n",
    "The 'Alternate Accepted Harris Corner Solution' image presents the result of `scipy.ndimage.convolve`'s default reflection padding, while the 'Harris Corner Solution' image presents the zero-padding solution. Similarly, 'Alternate Accepted Detected Corners Solution' image presents the result of `scipy.ndimage.convolve`'s default reflection padding, while the 'Detected Corners Solution' image presents the zero-padding solution. **Both are accepted solutions!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75d99c-91db-42f4-bbfc-ed3ec3ee8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harris_corners(img, window_size=3, k=0.04):\n",
    "    \"\"\"\n",
    "    Compute Harris corner response map. Follow the math equation\n",
    "    R=Det(M)-k(Trace(M)^2).\n",
    "\n",
    "    Hint:\n",
    "        You may use the function scipy.ndimage.filters.convolve,\n",
    "        which is already imported above. If you use convolve(), remember to\n",
    "        specify zero-padding to match our equations, for example:\n",
    "\n",
    "            out_image = convolve(in_image, kernel, mode='constant', cval=0)\n",
    "\n",
    "        You can also use for nested loops compute M and the subsequent Harris\n",
    "        corner response for each output pixel, intead of using convolve().\n",
    "        Your implementation of conv_fast or conv_nested in HW1 may be a\n",
    "        useful reference!\n",
    "\n",
    "    Args:\n",
    "        img: Grayscale image of shape (H, W)\n",
    "        window_size: size of the window function\n",
    "        k: sensitivity parameter\n",
    "\n",
    "    Returns:\n",
    "        response: Harris response image of shape (H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    H, W = img.shape\n",
    "    window = np.ones((window_size, window_size))\n",
    "\n",
    "    response = np.zeros((H, W))\n",
    "\n",
    "    # 1. Compute x and y derivatives (I_x, I_y) of an image\n",
    "    dx = filters.sobel_v(img)\n",
    "    dy = filters.sobel_h(img)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2236d2-2d97-4253-bc9d-2645a55cae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harris_corners_alternate_solutions(img, window_size=3, k=0.04):\n",
    "    \"\"\"\n",
    "    Compute Harris corner response map. Follow the math equation\n",
    "    R=Det(M)-k(Trace(M)^2).\n",
    "\n",
    "    Hint:\n",
    "        You may use the function scipy.ndimage.filters.convolve,\n",
    "        which is already imported above. If you use convolve(), remember to\n",
    "        specify zero-padding to match our equations, for example:\n",
    "\n",
    "            out_image = convolve(in_image, kernel, mode='constant', cval=0)\n",
    "\n",
    "        You can also use for nested loops compute M and the subsequent Harris\n",
    "        corner response for each output pixel, intead of using convolve().\n",
    "        Your implementation of conv_fast or conv_nested in HW1 may be a\n",
    "        useful reference!\n",
    "\n",
    "    Args:\n",
    "        img: Grayscale image of shape (H, W)\n",
    "        window_size: size of the window function\n",
    "        k: sensitivity parameter\n",
    "\n",
    "    Returns:\n",
    "        response: Harris response image of shape (H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    H, W = img.shape\n",
    "    window = np.ones((window_size, window_size))\n",
    "\n",
    "    response_forloop = np.zeros((H, W))\n",
    "    response_convolve_default = np.zeros((H, W))\n",
    "\n",
    "    # 1. Compute x and y derivatives (I_x, I_y) of an image\n",
    "    dx = filters.sobel_v(img)\n",
    "    dy = filters.sobel_h(img)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return response_convolve_default, response_forloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551e7af8-6099-4bb5-bf22-bcbdf0cd757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imread('tictactoe.png', as_gray=True)\n",
    "\n",
    "# Compute Harris corner response\n",
    "response = harris_corners(img)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "response_convolve_default, response_forloop = #\n",
    "\n",
    "### END YOUR CODE\n",
    "\n",
    "# Display corner response\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(response)\n",
    "plt.axis('off')\n",
    "plt.title('Harris Corner Response')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(imread('tictactoe_solution_harris.png', as_gray=True))\n",
    "plt.axis('off')\n",
    "plt.title('Harris Corner Solution')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(imread('tictactoe_alternate_solution_harris.png', as_gray=True))\n",
    "plt.axis('off')\n",
    "plt.title('Alternate Accepted Harris Corner Solution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e885af16-a29e-4a07-9a23-b1ab12a14a6e",
   "metadata": {},
   "source": [
    "Once you implement the Harris detector correctly, you will be able to see small bright blobs around the corners of the tic tac toe grids and shapes in the output corner response image. The function `corner_peaks` from `skimage.feature` performs non-maximum suppression to take local maxima of the response map and localize keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278540b6-9b23-4279-9980-3fda40c66276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform non-maximum suppression in response map\n",
    "# and output corner coordinates\n",
    "corners = corner_peaks(response, threshold_rel=0.01)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "corners_convolve_default = #\n",
    "corners_forloop = #\n",
    "# END CODE \n",
    "\n",
    "# Display detected corners\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img)\n",
    "plt.scatter(corners[:,1], corners[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Corners')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(imread('tictactoe_solution_detected_corners.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Detected Corners Solution')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(imread('tictactoe_alternate_solution_detected_corners.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Alternate Accepted Detected Corners Solution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cec7c1f-8451-4188-ba07-497d66d54675",
   "metadata": {},
   "source": [
    "## Part 3 Describing and Matching Keypoints (20 points)\n",
    "\n",
    "We are now able to localize keypoints in two images by running the Harris corner detector independently on them. Next question is, how do we determine which pair of keypoints come from corresponding locations in those two images? In order to *match* the detected keypoints, we must come up with a way to *describe* the keypoints based on their local appearance. Generally, each region around detected keypoint locations is converted into  a fixed-size vectors called *descriptors*.\n",
    "\n",
    "### Part 3.1 Creating Descriptors (10 points)\n",
    "\n",
    "In this section, you are going to implement the **`simple_descriptor`** function, where each keypoint is described by the normalized intensity of a small patch around it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111238c9-d9ec-4103-9710-0f79acb08ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_descriptor(patch):\n",
    "    \"\"\"\n",
    "    Describe the patch by normalizing the image values into a standard\n",
    "    normal distribution (having mean of 0 and standard deviation of 1)\n",
    "    and then flattening into a 1D array.\n",
    "\n",
    "    The normalization will make the descriptor more robust to change\n",
    "    in lighting condition.\n",
    "\n",
    "    Hint:\n",
    "        In this case of normalization, if a denominator is zero, divide by 1 instead.\n",
    "\n",
    "    Args:\n",
    "        patch: grayscale image patch of shape (H, W)\n",
    "\n",
    "    Returns:\n",
    "        feature: 1D array of shape (H * W)\n",
    "    \"\"\"\n",
    "    feature = []\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a7a8b-a71d-4829-bfe4-6a8af9abeb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = imread('oldunion1.png', as_gray=True)\n",
    "img2 = imread('oldunion2.png', as_gray=True)\n",
    "\n",
    "# Detect keypoints in two images\n",
    "keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "\n",
    "print(\"Keypoints 1 shape = \", keypoints1.shape)\n",
    "print(\"Keypoints 2 shape = \", keypoints2.shape)\n",
    "\n",
    "# Display detected keypoints\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.scatter(keypoints1[:500,1], keypoints1[:500,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 1')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2)\n",
    "plt.scatter(keypoints2[:500,1], keypoints2[:500,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 2')\n",
    "plt.show()\n",
    "\n",
    "# Display detected keypoints solution\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(imread('oldunion_solution_kp1.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 1 Solution')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(imread('oldunion_solution_kp2.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 2 Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec9321-4c13-4957-8d73-d6c7ca39d54e",
   "metadata": {},
   "source": [
    "### Part 3.2 Matching Descriptors (10 points)\n",
    "Next, implement the **`match_descriptors`** function to find good matches in two sets of descriptors. First, calculate Euclidean distance between all pairs of descriptors from image 1 and image 2. Then use this to determine if there is a good match: for each descriptor in image 1, if the distance to the closest descriptor in image 2 is significantly (by a given factor) smaller than the distance to the second-closest, we call it a match. The output of the function is an array where each row holds the indices of one pair of matching descriptors.\n",
    "\n",
    "*Checking your answer*: you should see an identical matching of keypoints as the solution, but the precise colors of each line will change with every run of keypoint matching so colors do not need to match.\n",
    "\n",
    "*Optional ungraded food for thought*: Think about why this method of keypoint matching is not commutative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b0633-2855-4ee6-91f8-5495649f9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_keypoints(image, keypoints, desc_func, patch_size=16):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image: grayscale image of shape (H, W)\n",
    "        keypoints: 2D array containing a keypoint (y, x) in each row\n",
    "        desc_func: function that takes in an image patch and outputs\n",
    "            a 1D feature vector describing the patch\n",
    "        patch_size: size of a square patch at each keypoint\n",
    "\n",
    "    Returns:\n",
    "        desc: array of features describing the keypoints\n",
    "    \"\"\"\n",
    "\n",
    "    image.astype(np.float32)\n",
    "    desc = []\n",
    "\n",
    "    for i, kp in enumerate(keypoints):\n",
    "        y, x = kp\n",
    "        patch = image[y-(patch_size//2):y+((patch_size+1)//2),\n",
    "                      x-(patch_size//2):x+((patch_size+1)//2)]\n",
    "        desc.append(desc_func(patch))\n",
    "    return np.array(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b250d7b9-4b85-4d25-9111-5c7020ac50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_descriptors(desc1, desc2, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Match the feature descriptors by finding distances between them. A match is formed\n",
    "    when the distance to the closest vector is much smaller than the distance to the\n",
    "    second-closest, that is, the ratio of the distances should be STRICTLY SMALLER\n",
    "    than the threshold (NOT equal to). Return the matches as pairs of vector indices.\n",
    "\n",
    "    Hint:\n",
    "        The Numpy functions np.sort, np.argmin, np.asarray might be useful\n",
    "\n",
    "        The Scipy function cdist calculates Euclidean distance between all\n",
    "        pairs of inputs\n",
    "    Args:\n",
    "        desc1: an array of shape (M, P) holding descriptors of size P about M keypoints\n",
    "        desc2: an array of shape (N, P) holding descriptors of size P about N keypoints\n",
    "\n",
    "    Returns:\n",
    "        matches: an array of shape (Q, 2) where each row holds the indices of one pair\n",
    "        of matching descriptors\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "\n",
    "    M = desc1.shape[0]\n",
    "    dists = cdist(desc1, desc2)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0854b31f-9a61-4f92-b693-be7d5ae57764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matches(ax, image1, image2, keypoints1, keypoints2, matches,\n",
    "                 keypoints_color='k', matches_color=None, only_matches=False):\n",
    "    \"\"\"Plot matched features.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        Matches and image are drawn in this ax.\n",
    "    image1 : (N, M [, 3]) array\n",
    "        First grayscale or color image.\n",
    "    image2 : (N, M [, 3]) array\n",
    "        Second grayscale or color image.\n",
    "    keypoints1 : (K1, 2) array\n",
    "        First keypoint coordinates as ``(row, col)``.\n",
    "    keypoints2 : (K2, 2) array\n",
    "        Second keypoint coordinates as ``(row, col)``.\n",
    "    matches : (Q, 2) array\n",
    "        Indices of corresponding matches in first and second set of\n",
    "        descriptors, where ``matches[:, 0]`` denote the indices in the first\n",
    "        and ``matches[:, 1]`` the indices in the second set of descriptors.\n",
    "    keypoints_color : matplotlib color, optional\n",
    "        Color for keypoint locations.\n",
    "    matches_color : matplotlib color, optional\n",
    "        Color for lines which connect keypoint matches. By default the\n",
    "        color is chosen randomly.\n",
    "    only_matches : bool, optional\n",
    "        Whether to only plot matches and not plot the keypoint locations.\n",
    "    \"\"\"\n",
    "\n",
    "    image1.astype(np.float32)\n",
    "    image2.astype(np.float32)\n",
    "\n",
    "    new_shape1 = list(image1.shape)\n",
    "    new_shape2 = list(image2.shape)\n",
    "\n",
    "    if image1.shape[0] < image2.shape[0]:\n",
    "        new_shape1[0] = image2.shape[0]\n",
    "    elif image1.shape[0] > image2.shape[0]:\n",
    "        new_shape2[0] = image1.shape[0]\n",
    "\n",
    "    if image1.shape[1] < image2.shape[1]:\n",
    "        new_shape1[1] = image2.shape[1]\n",
    "    elif image1.shape[1] > image2.shape[1]:\n",
    "        new_shape2[1] = image1.shape[1]\n",
    "\n",
    "    if new_shape1 != image1.shape:\n",
    "        new_image1 = np.zeros(new_shape1, dtype=image1.dtype)\n",
    "        new_image1[:image1.shape[0], :image1.shape[1]] = image1\n",
    "        image1 = new_image1\n",
    "\n",
    "    if new_shape2 != image2.shape:\n",
    "        new_image2 = np.zeros(new_shape2, dtype=image2.dtype)\n",
    "        new_image2[:image2.shape[0], :image2.shape[1]] = image2\n",
    "        image2 = new_image2\n",
    "\n",
    "    image = np.concatenate([image1, image2], axis=1)\n",
    "\n",
    "    offset = image1.shape\n",
    "\n",
    "    if not only_matches:\n",
    "        ax.scatter(keypoints1[:, 1], keypoints1[:, 0],\n",
    "                   facecolors='none', edgecolors=keypoints_color)\n",
    "        ax.scatter(keypoints2[:, 1] + offset[1], keypoints2[:, 0],\n",
    "                   facecolors='none', edgecolors=keypoints_color)\n",
    "\n",
    "    ax.imshow(image, interpolation='nearest', cmap='gray')\n",
    "    ax.axis((0, 2 * offset[1], offset[0], 0))\n",
    "\n",
    "    for i in range(matches.shape[0]):\n",
    "        idx1 = matches[i, 0]\n",
    "        idx2 = matches[i, 1]\n",
    "\n",
    "        if matches_color is None:\n",
    "            color = np.random.rand(3)\n",
    "        else:\n",
    "            color = matches_color\n",
    "\n",
    "        ax.plot((keypoints1[idx1, 1], keypoints2[idx2, 1] + offset[1]),\n",
    "                (keypoints1[idx1, 0], keypoints2[idx2, 0]),\n",
    "'-', color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7008d5-1449-4cfd-986c-a5bf7862e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed to compare output against solution\n",
    "np.random.seed(131)\n",
    "\n",
    "patch_size = 5\n",
    "    \n",
    "# Extract features from the corners\n",
    "desc1 = describe_keypoints(img1, keypoints1,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "desc2 = describe_keypoints(img2, keypoints2,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "\n",
    "print(\"Desc1 shape = \", desc1.shape)\n",
    "print(\"Desc2 shape = \", desc2.shape)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "matches = match_descriptors(desc1[:10000,:], desc2[:10000,:], 0.7)\n",
    "\n",
    "# Plot matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "ax.axis('off')\n",
    "plt.title('Matched Simple Descriptor')\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, matches[:30,:])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('oldunion_solution_simple_descriptor.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Matched Simple Descriptor Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5f76e-ac87-433c-b962-24b4572fc6a9",
   "metadata": {},
   "source": [
    "## Part 4 Describing and Matching Keypoints with SIFT (10 points)\n",
    "\n",
    "In 2004, D.Lowe, University of British Columbia, came up with a new algorithm, Scale Invariant Feature Transform (SIFT) in his paper, Distinctive Image Features from Scale-Invariant Keypoints, which extract keypoints and compute its descriptors. We are now going to determine the keypoints of an image using Open CV functions. \n",
    "\n",
    "### Part 4.1 Creating Descriptors (10 points)\n",
    "\n",
    "In this section, you are going to implement an Open CV sift function to find the keypoints and descriptors. Then you are going to map the keypoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26567b3c-d549-44d3-a2b3-b9b5f994fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the image\n",
    "img1 = cv.imread('oldunion1.png')\n",
    "img2 = cv.imread('oldunion2.png')\n",
    "\n",
    " # Converting image to grayscale\n",
    "img1 = cv.cvtColor(img1,cv.COLOR_BGR2GRAY)\n",
    "img2 = cv.cvtColor(img2,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "### YOUR CODE HERE \n",
    "# Applying SIFT detector\n",
    "\n",
    "# Find the keypoints and descriptors with SIFT\n",
    "\n",
    "### END YOUR CODE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7705c91-91f2-4cab-9417-f553f5cf2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marking the keypoint on the image using circles\n",
    "img1=cv.drawKeypoints(img1, kp1, img1, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "img2=cv.drawKeypoints(img2, kp2, img2, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Display detected keypoints\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints in Image 1')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2)\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints in Image 2')\n",
    "plt.show()\n",
    "\n",
    "# Display detected keypoints solution\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(imread('solution_detected_kp1.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints in Image 1 Solution')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(imread('solution_detected_kp2.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints in Image 2 Solution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bf87d1-f2bc-4941-82b2-33c32f3be5e1",
   "metadata": {},
   "source": [
    "### Part 4.2 Matching Descriptors with SIFT (10 points)\n",
    "Next, match the descriptors with the BFMatcher() function. \n",
    "\n",
    "*Checking your answer*: the lines are hard to see, but your solution should be similar to our solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41b4f9-34f7-46aa-934b-b70951dae357",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "# BFMatcher with default params\n",
    "\n",
    "# Match descriptors.\n",
    "\n",
    "### END YOUR CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15576fc7-b043-4149-b287-42f82aade45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img3 = cv.drawMatches(img1,kp1,img2,kp2,matches[:100],None,flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    " \n",
    "plt.imshow(img3)\n",
    "\n",
    "# Plot matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "ax.axis('off')\n",
    "plt.title('Matched Simple Descriptor')\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, matches[:30,:])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('oldunion_solution_simple_descriptor.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Matched Simple Descriptor Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8405a1-d62d-48df-a3f0-2c444c090d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
