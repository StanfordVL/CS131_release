{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "520c64d2",
   "metadata": {
    "id": "520c64d2"
   },
   "source": [
    "# Homework 4\n",
    "In this homework, we will delve deeper into panorama stitching. We'll also explore epipolar geometry and stereo matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rlpMmL7BlRWi",
   "metadata": {
    "id": "rlpMmL7BlRWi"
   },
   "source": [
    "## Submission Instructions\n",
    "1. Make sure that the first line in any code cell with a function is the `def` line. <br>\n",
    "The HW conversion script will fail to detect the function unless the function definition is on the first line of the code cell. <br>\n",
    "Also double-check that each function is alone in its cell (there should be no function calls or imports anywhere in that cell).\n",
    "\n",
    "2. Run the [conversion script](https://colab.research.google.com/drive/1DABLKz0Q9udhyZh1V6Ynv82PAsIws7-i?usp=sharing) to get your .py file. As a sanity check, the autograder expects the following functions in your .py file:\n",
    "    * **`harris_corners`**\n",
    "    * **`simple_descriptor`**\n",
    "    * **`match_descriptors`**\n",
    "    * **`fit_affine_matrix`**\n",
    "    * **`ransac`**\n",
    "    * **`linear_blend`**\n",
    "    * **`stitch_multiple_images`**\n",
    "\n",
    "  **The code you write in Part 2 will not be autograded, we'll grade it as part of the PDF.**\n",
    "\n",
    "3. Make sure the Python file you upload to Gradescope is named **`hw4.py`**. This is required for the autograder to run correctly.\n",
    "\n",
    "4. We added a section to the conversion script that generates an **HTML file that you can Ctrl+P and save as a PDF**. <br> This will ensure images don't get cut off in your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6458d647",
   "metadata": {
    "id": "6458d647"
   },
   "source": [
    "_**Notes on Running This Notebook:**_\n",
    "\n",
    "Make sure to run each part from its begining to ensure that you compute all of the dependencies of your current question and don't crossover variables with the same name from other questions. So long as you run each part from its beginning, you can run the parts in any order. When assembling your PDF, we recommend running all cells in order from the top of the notebook to prevent any of these discontinuity errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GWFsmOnsoA1J",
   "metadata": {
    "id": "GWFsmOnsoA1J"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3223b",
   "metadata": {
    "id": "e4a3223b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"CS131_release\"):\n",
    "    # Clone the repository if it doesn't already exist\n",
    "    !git clone https://github.com/StanfordVL/CS131_release.git\n",
    "\n",
    "%cd CS131_release/winter_2025/hw4_release/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b81ccb",
   "metadata": {
    "id": "f6b81ccb"
   },
   "outputs": [],
   "source": [
    "# Install the necessary dependencies\n",
    "# (restart your runtime session if prompted to, and then re-run this cell)\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CZeVp0GlYHzF",
   "metadata": {
    "id": "CZeVp0GlYHzF"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from skimage import filters\n",
    "from skimage.feature import corner_peaks\n",
    "from skimage.io import imread\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.ndimage import convolve\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "\n",
    "from utils import pad, unpad, get_output_space, warp_image, plot_matches, describe_keypoints\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 9.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aKO0T9OFKS1I",
   "metadata": {
    "id": "aKO0T9OFKS1I"
   },
   "source": [
    "# Part 1: Panorama Stitching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e91e1c",
   "metadata": {
    "id": "77e91e1c"
   },
   "source": [
    "Panorama stitching is an early success of computer vision. Matthew Brown and David G. Lowe published a famous [panoramic image stitching paper](http://matthewalunbrown.com/papers/ijcv2007.pdf) in 2007. Since then, automatic panorama stitching technology has been widely adopted in many applications such as Google Street View, panorama photos on smartphones,\n",
    "and stitching software such as Photosynth and AutoStitch.\n",
    "\n",
    "**We will detect and match keypoints from multiple images to build a single panoramic image. This will involve several tasks:**\n",
    "1. Compare two sets of descriptors coming from two different images and find matching keypoints *(we did this in Homework 2)*.\n",
    "2. Given matching keypoints, use least-squares to find the affine transformation matrix that maps points in one image to another.\n",
    "3. Use RANSAC to give a more robust estimate of the affine transformation matrix. <br>\n",
    "Given the transformation matrix, use it to transform the second image and overlay it on the first image, forming a panorama.\n",
    "4. Blend panorama images together to remove blurry regions of overlapping images.\n",
    "5. Stich multiple panorama images together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X2MJpP4xfQXw",
   "metadata": {
    "id": "X2MJpP4xfQXw"
   },
   "source": [
    "## 1. Describing and Matching Keypoints (0 points)\n",
    "In Homework 2, recall that we used Harris corner detection to detect keypoints in two images. Given these two sets of keypoints, we then determined which pairs of keypoints come from the same 3D points projected onto the two images. We did so by first converting the region around each keypoint into a descriptor. Then, we found good matches in the two sets of descriptors based on Euclidean distance.\n",
    "\n",
    "Add your code from Homework 2 for the three functions below. You can also feel free to paste in the reference solutions posted on Ed!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P1qLHP2Tfsf5",
   "metadata": {
    "id": "P1qLHP2Tfsf5"
   },
   "outputs": [],
   "source": [
    "def harris_corners(img, window_size=3, k=0.04):\n",
    "    \"\"\"\n",
    "    Compute Harris corner response map. Follow the math equation\n",
    "    R=Det(M)-k(Trace(M)^2).\n",
    "\n",
    "    Hint:\n",
    "        You may use the function scipy.ndimage.filters.convolve,\n",
    "        which is already imported above. If you use convolve(), remember to\n",
    "        specify zero-padding to match our equations, for example:\n",
    "\n",
    "            out_image = convolve(in_image, kernel, mode='constant', cval=0)\n",
    "\n",
    "        You can also use for nested loops compute M and the subsequent Harris\n",
    "        corner response for each output pixel, intead of using convolve().\n",
    "        Your implementation of conv_fast or conv_nested in HW1 may be a\n",
    "        useful reference!\n",
    "\n",
    "    Args:\n",
    "        img: Grayscale image of shape (H, W)\n",
    "        window_size: size of the window function\n",
    "        k: sensitivity parameter\n",
    "\n",
    "    Returns:\n",
    "        response: Harris response image of shape (H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    H, W = img.shape\n",
    "    window = np.ones((window_size, window_size))\n",
    "\n",
    "    response = np.zeros((H, W))\n",
    "\n",
    "    # 1. Compute x and y derivatives (I_x, I_y) of an image\n",
    "    dx = filters.sobel_v(img)\n",
    "    dy = filters.sobel_h(img)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return response\n",
    "\n",
    "def simple_descriptor(patch):\n",
    "    \"\"\"\n",
    "    Describe the patch by normalizing the image values into a standard\n",
    "    normal distribution (having mean of 0 and standard deviation of 1)\n",
    "    and then flattening into a 1D array.\n",
    "\n",
    "    The normalization will make the descriptor more robust to change\n",
    "    in lighting condition.\n",
    "\n",
    "    Hint:\n",
    "        In this case of normalization, if a denominator is zero, divide by 1 instead.\n",
    "\n",
    "    Args:\n",
    "        patch: grayscale image patch of shape (H, W)\n",
    "\n",
    "    Returns:\n",
    "        feature: 1D array of shape (H * W)\n",
    "    \"\"\"\n",
    "\n",
    "    feature = []\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return feature\n",
    "\n",
    "def match_descriptors(desc1, desc2, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Match the feature descriptors by finding distances between them. A match is formed\n",
    "    when the distance to the closest vector is much smaller than the distance to the\n",
    "    second-closest, that is, the ratio of the distances should be STRICTLY SMALLER\n",
    "    than the threshold (NOT equal to). Return the matches as pairs of vector indices.\n",
    "\n",
    "    Hint:\n",
    "        The Numpy functions np.sort, np.argmin, np.asarray might be useful\n",
    "\n",
    "        The Scipy function cdist calculates Euclidean distance between all\n",
    "        pairs of inputs\n",
    "    Args:\n",
    "        desc1: an array of shape (M, P) holding descriptors of size P about M keypoints\n",
    "        desc2: an array of shape (N, P) holding descriptors of size P about N keypoints\n",
    "\n",
    "    Returns:\n",
    "        matches: an array of shape (Q, 2) where each row holds the indices of one pair\n",
    "        of matching descriptors\n",
    "    \"\"\"\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    M = desc1.shape[0]\n",
    "    dists = cdist(desc1, desc2)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c2899",
   "metadata": {
    "id": "4d6c2899"
   },
   "source": [
    "Run the following cell to detect and match the keypoints in two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aea9b37",
   "metadata": {
    "id": "8aea9b37"
   },
   "outputs": [],
   "source": [
    "img1 = imread('uttower1.jpg', as_gray=True)\n",
    "img2 = imread('uttower2.jpg', as_gray=True)\n",
    "\n",
    "np.random.seed(131)\n",
    "\n",
    "# Detect keypoints in two images\n",
    "keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "\n",
    "# Extract features from the corners\n",
    "desc1 = describe_keypoints(img1, keypoints1,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=5)\n",
    "desc2 = describe_keypoints(img2, keypoints2,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=5)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "matches = match_descriptors(desc1, desc2, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb1bea",
   "metadata": {
    "id": "3deb1bea"
   },
   "source": [
    "## 2. Transformation Estimation (20 points)\n",
    "\n",
    "Now, we will use these matched keypoints to find a **transformation matrix** that maps points in the second image to the corresponding coordinates in the first image. In other words, if the point $p_1 = [y_1,x_1]$ in image 1 matches with $p_2=[y_2, x_2]$ in image 2, we need to find an affine transformation matrix $H$ such that\n",
    "\n",
    "$$\n",
    "\\tilde{p_2}H = \\tilde{p_1},\n",
    "$$\n",
    "\n",
    "where $\\tilde{p_1}$ and $\\tilde{p_2}$ are homogenous coordinates of $p_1$ and $p_2$.\n",
    "\n",
    "Note that it may be impossible to find the transformation $H$ that maps every point in image 2 exactly to the corresponding point in image 1. However, **we can estimate the transformation matrix with the least squares method.** Given $N$ matched keypoint pairs, let $X_1$ and $X_2$ be $N \\times 3$ matrices whose rows are homogenous coordinates of corresponding keypoints in image 1 and image 2 respectively. Then, we can estimate $H$ by solving the least squares problem,\n",
    "\n",
    "$$\n",
    "X_2 H = X_1\n",
    "$$\n",
    "\n",
    "Implement **`fit_affine_matrix`** below.\n",
    "\n",
    "*Hint: read the [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html) for `np.linalg.lstsq`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3CjJ9hsGY8nY",
   "metadata": {
    "id": "3CjJ9hsGY8nY"
   },
   "outputs": [],
   "source": [
    "def fit_affine_matrix(p1, p2, to_pad=True):\n",
    "    \"\"\"\n",
    "    Fit affine matrix such that p2 * H = p1. First, pad the descriptor vectors\n",
    "    with a 1 using pad() to convert to homogeneous coordinates, then return\n",
    "    the least squares fit affine matrix in homogeneous coordinates.\n",
    "\n",
    "    Hint:\n",
    "        You can use np.linalg.lstsq function to solve the problem.\n",
    "\n",
    "        Explicitly specify np.linalg.lstsq's new default parameter rcond=None\n",
    "        to suppress deprecation warnings, and match the autograder.\n",
    "\n",
    "    Args:\n",
    "        p1: an array of shape (M, P) holding descriptors of size P about M keypoints\n",
    "        p2: an array of shape (M, P) holding descriptors of size P about M keypoints\n",
    "\n",
    "    Return:\n",
    "        H: a matrix of shape (P+1, P+1) that transforms p2 to p1 in homogeneous\n",
    "        coordinates\n",
    "    \"\"\"\n",
    "\n",
    "    assert (p1.shape[0] == p2.shape[0]),\\\n",
    "        'Different number of points in p1 and p2'\n",
    "\n",
    "    if to_pad:\n",
    "        p1 = pad(p1)\n",
    "        p2 = pad(p2)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    # Sometimes numerical issues cause least-squares to produce the last\n",
    "    # column which is not exactly [0, 0, 1]\n",
    "    H[:,2] = np.array([0, 0, 1])\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5df1aa",
   "metadata": {
    "id": "6e5df1aa"
   },
   "outputs": [],
   "source": [
    "# Sanity check for fit_affine_matrix\n",
    "\n",
    "# Test inputs\n",
    "a = np.array([[0.5, 0.1], [0.4, 0.2], [0.8, 0.2]])\n",
    "b = np.array([[0.3, -0.2], [-0.4, -0.9], [0.1, 0.1]])\n",
    "\n",
    "H = fit_affine_matrix(b, a)\n",
    "\n",
    "# Target output\n",
    "sol = np.array(\n",
    "    [[1.25, 2.5, 0.0],\n",
    "     [-5.75, -4.5, 0.0],\n",
    "     [0.25, -1.0, 1.0]]\n",
    ")\n",
    "\n",
    "error = np.sum((H - sol) ** 2)\n",
    "\n",
    "if error < 1e-20:\n",
    "    print('Implementation correct!')\n",
    "else:\n",
    "    print('There is something wrong.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00474282",
   "metadata": {
    "id": "00474282"
   },
   "source": [
    "After checking that your **`fit_affine_matrix`** function is running correctly, run the following code to apply it to images.\n",
    "\n",
    "Images will be warped and image 2 will be mapped to image 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c7873",
   "metadata": {
    "id": "f32c7873"
   },
   "outputs": [],
   "source": [
    "p1 = keypoints1[matches[:,0]]\n",
    "p2 = keypoints2[matches[:,1]]\n",
    "\n",
    "# Find affine transformation matrix H that maps p2 to p1\n",
    "H = fit_affine_matrix(p1, p2)\n",
    "\n",
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "print(\"Output shape:\", output_shape)\n",
    "print(\"Offset:\", offset)\n",
    "\n",
    "# Warp images into output space\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 Warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 Warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b60e5",
   "metadata": {
    "id": "c88b60e5"
   },
   "source": [
    "Next, the two warped images are merged to get a panorama.\n",
    "\n",
    "Your panorama may not look good at this point, but we will later use other techniques to get a better result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9763f",
   "metadata": {
    "id": "93c9763f"
   },
   "outputs": [],
   "source": [
    "# Merge the two images\n",
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.title('Fit-Affine Panorama')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_fit_affine_panorama.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Fit-Affine Panorama Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1294483",
   "metadata": {
    "id": "c1294483"
   },
   "source": [
    "## 3. RANSAC (20 points)\n",
    "Rather than directly feeding all our keypoint matches into **``fit_affine_matrix``**, we can use **RANSAC** (\"RANdom SAmple Consensus\") to select only \"inliers\" to use for computing the transformation matrix.\n",
    "\n",
    "Use Euclidean distance as a measure of inliers vs. outliers.\n",
    "\n",
    "\n",
    "The steps of RANSAC are:\n",
    "1. Select random set of matches\n",
    "2. Compute affine transformation matrix\n",
    "      - You can call your **`fit_affine_matrix`** function for this. **Make sure to explicitly pass in `to_pad=False`.**\n",
    "3. Find inliers using the given threshold\n",
    "4. Repeat and keep the largest set of inliers (use >, i.e. break ties by whichever set is seen first)\n",
    "5. Re-compute least-squares estimate on all of the inliers\n",
    "\n",
    "Implement **`ransac`** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Uuz16CSCjdL7",
   "metadata": {
    "id": "Uuz16CSCjdL7"
   },
   "outputs": [],
   "source": [
    "def ransac(keypoints1, keypoints2, matches, n_iters=200, threshold=20):\n",
    "    \"\"\"\n",
    "    Use RANSAC to find a robust affine transformation:\n",
    "\n",
    "        1. Select random set of matches\n",
    "        2. Compute affine transformation matrix\n",
    "        3. Compute inliers via Euclidean distance\n",
    "        4. Keep the largest set of inliers (use >, i.e. break ties by whichever set is seen first)\n",
    "        5. Re-compute least-squares estimate on all of the inliers\n",
    "\n",
    "    Update max_inliers as a boolean array where True represents the keypoint\n",
    "    at this index is an inlier, while False represents that it is not an inlier.\n",
    "\n",
    "    Hint:\n",
    "        You can use fit_affine_matrix to compute the affine transformation matrix.\n",
    "        Make sure to pass in to_pad=False, since we pad the matches for you here.\n",
    "\n",
    "        You can compute elementwise boolean operations between two numpy arrays,\n",
    "        and use boolean arrays to select array elements by index:\n",
    "        https://numpy.org/doc/stable/reference/arrays.indexing.html#boolean-array-indexing\n",
    "\n",
    "    Args:\n",
    "        keypoints1: M1 x 2 matrix, each row is a point\n",
    "        keypoints2: M2 x 2 matrix, each row is a point\n",
    "        matches: N x 2 matrix, each row represents a match\n",
    "            [index of keypoint1, index of keypoint 2]\n",
    "        n_iters: the number of iterations RANSAC will run\n",
    "        threshold: the threshold to find inliers\n",
    "\n",
    "    Returns:\n",
    "        H: a robust estimation of affine transformation from keypoints2 to\n",
    "        keypoints 1\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy matches array, to avoid overwriting it\n",
    "    orig_matches = matches.copy()\n",
    "    matches = matches.copy()\n",
    "\n",
    "    N = matches.shape[0]\n",
    "    n_samples = int(N * 0.2)\n",
    "\n",
    "    matched1 = pad(keypoints1[matches[:,0]])\n",
    "    matched2 = pad(keypoints2[matches[:,1]])\n",
    "\n",
    "    max_inliers = np.zeros(N, dtype=bool)\n",
    "    n_inliers = 0\n",
    "\n",
    "    # RANSAC iteration start\n",
    "\n",
    "    # Note: while there're many ways to do random sampling, we use\n",
    "    # `np.random.shuffle()` followed by slicing out the first `n_samples`\n",
    "    # matches here in order to align with the autograder.\n",
    "    # Sample with this code:\n",
    "    for i in range(n_iters):\n",
    "        # 1. Select random set of matches\n",
    "        np.random.shuffle(matches)\n",
    "        samples = matches[:n_samples]\n",
    "        sample1 = pad(keypoints1[samples[:,0]])\n",
    "        sample2 = pad(keypoints2[samples[:,1]])\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "        # 2. Compute affine transformation matrix, map sample2 to sample1\n",
    "\n",
    "        # 3. Compute inliers via Euclidean distance\n",
    "\n",
    "        # 4. Keep the largest set of inliers\n",
    "\n",
    "    # 5. Re-compute least-squares estimate on all of the inliers\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return H, orig_matches[max_inliers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_sjjRshYjdYx",
   "metadata": {
    "id": "_sjjRshYjdYx"
   },
   "source": [
    "Now, run through the following cells to get a panorama. You'll be able to see the difference from the result we got before without RANSAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525334d1",
   "metadata": {
    "id": "525334d1"
   },
   "outputs": [],
   "source": [
    "# Set seed to compare output against solution image\n",
    "np.random.seed(131)\n",
    "\n",
    "H, robust_matches = ransac(keypoints1, keypoints2, matches, threshold=1)\n",
    "print(\"Robust matches shape = \", robust_matches.shape)\n",
    "print(\"H = \\n\", H)\n",
    "\n",
    "# Visualize robust matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 9))\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, robust_matches)\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Matches')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_ransac.png'))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Matches Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048db598",
   "metadata": {
    "id": "048db598"
   },
   "source": [
    "We can now use the tranformation matrix $H$ computed using the robust matches to warp our images and create a better-looking panorama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fd5ea",
   "metadata": {
    "id": "de7fd5ea"
   },
   "outputs": [],
   "source": [
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "\n",
    "# Warp images into output space\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612dde50",
   "metadata": {
    "id": "612dde50"
   },
   "outputs": [],
   "source": [
    "# Merge the two images\n",
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Panorama')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_ransac_panorama.png'))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Panorama Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254febf",
   "metadata": {
    "id": "d254febf"
   },
   "source": [
    "## 4. Better Image Merging (10 points)\n",
    "You will notice the blurry region and unpleasant lines in the middle of the final panoramic image. Using a very simple technique called linear blending, we can smooth out a lot of these artifacts.\n",
    "\n",
    "Currently, all the pixels in the overlapping region are weighted equally. However, since the pixels at the left and right ends of the overlap are very well complemented by the pixels in the other image, they can be made to contribute less to the final panorama.\n",
    "\n",
    "Linear blending can be done with the following steps:\n",
    "\n",
    "1. Define left and right margins for blending to occur between\n",
    "2. Define a weight matrix for image 1 such that:\n",
    "    - From the left of the output space to the left margin the weight is 1\n",
    "    - From the left margin to the right margin, the weight linearly decrements from 1 to 0\n",
    "3. Define a weight matrix for image 2 such that:\n",
    "    - From the right of the output space to the right margin the weight is 1\n",
    "    - From the left margin to the right margin, the weight linearly increments from 0 to 1\n",
    "4. Apply the weight matrices to their corresponding images\n",
    "5. Combine the images\n",
    "\n",
    "In **`linear_blend`** below, implement the linear blending scheme to make the panorama look more natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KI6xnj-QkXhu",
   "metadata": {
    "id": "KI6xnj-QkXhu"
   },
   "outputs": [],
   "source": [
    "def linear_blend(img1_warped, img2_warped):\n",
    "    \"\"\"\n",
    "    Linearly blend img1_warped and img2_warped by following the steps:\n",
    "\n",
    "    1. Define left and right margins (already done for you)\n",
    "    2. Define a weight matrices for img1_warped and img2_warped\n",
    "        np.linspace and np.tile functions will be useful\n",
    "    3. Apply the weight matrices to their corresponding images\n",
    "    4. Combine the images\n",
    "\n",
    "    Args:\n",
    "        img1_warped: Refernce image warped into output space\n",
    "        img2_warped: Transformed image warped into output space\n",
    "\n",
    "    Returns:\n",
    "        merged: Merged image in output space\n",
    "    \"\"\"\n",
    "\n",
    "    out_H, out_W = img1_warped.shape # Height and width of output space\n",
    "    img1_mask = (img1_warped != 0)  # Mask == 1 inside the image\n",
    "    img2_mask = (img2_warped != 0)  # Mask == 1 inside the image\n",
    "\n",
    "    # Find column of middle row where warped image 1 ends\n",
    "    # This is where to end weight mask for warped image 1\n",
    "    right_margin = out_W - np.argmax(np.fliplr(img1_mask)[out_H//2, :].reshape(1, out_W), 1)[0]\n",
    "\n",
    "    # Find column of middle row where warped image 2 starts\n",
    "    # This is where to start weight mask for warped image 2\n",
    "    left_margin = np.argmax(img2_mask[out_H//2, :].reshape(1, out_W), 1)[0]\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WscycVJUrjhu",
   "metadata": {
    "id": "WscycVJUrjhu"
   },
   "source": [
    "Now let's see how linear blending improves our result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78becc75",
   "metadata": {
    "id": "78becc75"
   },
   "outputs": [],
   "source": [
    "img1 = imread('uttower1.jpg', as_gray=True)\n",
    "img2 = imread('uttower2.jpg', as_gray=True)\n",
    "\n",
    "# Set seed to compare output against solution\n",
    "np.random.seed(131)\n",
    "\n",
    "# Detect keypoints in both images\n",
    "ec1_keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                              threshold_rel=0.05,\n",
    "                              exclude_border=8)\n",
    "ec1_keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                              threshold_rel=0.05,\n",
    "                              exclude_border=8)\n",
    "\n",
    "print(\"EC1 keypoints1 shape = \", ec1_keypoints1.shape)\n",
    "print(\"EC1 keypoints2 shape = \", ec1_keypoints2.shape)\n",
    "\n",
    "# Extract features from the corners\n",
    "ec1_desc1 = describe_keypoints(img1, ec1_keypoints1,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=16)\n",
    "ec1_desc2 = describe_keypoints(img2, ec1_keypoints2,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=16)\n",
    "\n",
    "print(\"EC1 desc1 shape = \", ec1_desc1.shape)\n",
    "print(\"EC1 desc2 shape = \", ec1_desc2.shape)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "ec1_matches = match_descriptors(ec1_desc1, ec1_desc2, 0.7)\n",
    "\n",
    "H, robust_matches = ransac(ec1_keypoints1, ec1_keypoints2, ec1_matches, threshold=1)\n",
    "print(\"Robust matches shape = \", robust_matches.shape)\n",
    "print(\"H = \\n\", H)\n",
    "\n",
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "print(\"Output shape:\", output_shape)\n",
    "print(\"Offset:\", offset)\n",
    "\n",
    "# Warp images into output space\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Merge the warped images using linear blending scheme\n",
    "merged = linear_blend(img1_warped, img2_warped)\n",
    "\n",
    "plt.imshow(merged)\n",
    "plt.axis('off')\n",
    "plt.title('Linear Blend')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_linear_blend.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Linear Blend Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea49cf",
   "metadata": {
    "collapsed": true,
    "id": "00ea49cf"
   },
   "source": [
    "## 5. Stitching Multiple Images (10 points)\n",
    "Implement **`stitch_multiple_images`** below to stitch together an ordered chain of images.\n",
    "\n",
    "Given a sequence of $m$ images ($I_1, I_2,...,I_m$), take every neighboring pair of images and compute the transformation matrix which converts points from the coordinate frame of $I_{i+1}$ to the frame of $I_{i}$.\n",
    "\n",
    "Then, select a reference image $I_{\\text{ref}}$, which is the first or left-most image in the chain. We want our final panorama image to be in the coordinate frame of $I_{\\text{ref}}$.\n",
    "\n",
    "You do **not** need to use linear blending for this problem: it's not included in the solution so the autograder does not expect it.\n",
    "\n",
    "*Hints:*\n",
    "- You may want to review the Linear Algebra recitation slides on how to combine the effects of multiple transformation matrices.\n",
    "- The inverse of transformation matrix has the reverse effect. Use [`numpy.linalg.inv`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html) whenever you want to compute matrix inverse.\n",
    "- Take a look at the code cells in Part 3 (RANSAC) for an example of how to merge warped images and track the overlap using masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZtIcXNbtk_OU",
   "metadata": {
    "id": "ZtIcXNbtk_OU"
   },
   "outputs": [],
   "source": [
    "def stitch_multiple_images(imgs, desc_func=simple_descriptor, patch_size=5):\n",
    "    \"\"\"\n",
    "    Stitch an ordered chain of images together.\n",
    "\n",
    "    Args:\n",
    "        imgs: List of length m containing the ordered chain of m images\n",
    "        desc_func: Function that takes in an image patch and outputs\n",
    "            a 1D feature vector describing the patch\n",
    "        patch_size: Size of square patch at each keypoint\n",
    "\n",
    "    Returns:\n",
    "        panorama: Final panorma image in coordinate frame of reference image\n",
    "    \"\"\"\n",
    "\n",
    "    # Detect keypoints in each image\n",
    "    keypoints = []  # keypoints[i] corresponds to imgs[i]\n",
    "    for img in imgs:\n",
    "        kypnts = corner_peaks(harris_corners(img, window_size=3),\n",
    "                              threshold_rel=0.05,\n",
    "                              exclude_border=8)\n",
    "        keypoints.append(kypnts)\n",
    "\n",
    "    # Describe keypoints\n",
    "    descriptors = []  # descriptors[i] corresponds to keypoints[i]\n",
    "    for i, kypnts in enumerate(keypoints):\n",
    "        desc = describe_keypoints(imgs[i], kypnts,\n",
    "                                  desc_func=desc_func,\n",
    "                                  patch_size=patch_size)\n",
    "        descriptors.append(desc)\n",
    "\n",
    "    # Match keypoints in neighboring images\n",
    "    matches = []  # matches[i] corresponds to matches between\n",
    "                  # descriptors[i] and descriptors[i+1]\n",
    "    for i in range(len(imgs)-1):\n",
    "        mtchs = match_descriptors(descriptors[i], descriptors[i+1], 0.7)\n",
    "        matches.append(mtchs)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return panorama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UJTsM_Yor2xi",
   "metadata": {
    "id": "UJTsM_Yor2xi"
   },
   "source": [
    "We can now visualize the final panorama!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d7eb6b",
   "metadata": {
    "id": "c4d7eb6b"
   },
   "outputs": [],
   "source": [
    "# Set seed to compare output against solution\n",
    "np.random.seed(131)\n",
    "\n",
    "# Load images to be stitched\n",
    "ec2_img1 = imread('yosemite1.jpg', as_gray=True)\n",
    "ec2_img2 = imread('yosemite2.jpg', as_gray=True)\n",
    "ec2_img3 = imread('yosemite3.jpg', as_gray=True)\n",
    "ec2_img4 = imread('yosemite4.jpg', as_gray=True)\n",
    "\n",
    "imgs = [ec2_img1, ec2_img2, ec2_img3, ec2_img4]\n",
    "\n",
    "# Stitch images together\n",
    "panorama = stitch_multiple_images(imgs, desc_func=simple_descriptor, patch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db68981",
   "metadata": {
    "id": "2db68981",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize final panorama image\n",
    "plt.imshow(panorama)\n",
    "plt.axis('off')\n",
    "plt.title('Stiched Images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ebe64",
   "metadata": {
    "id": "111ebe64"
   },
   "outputs": [],
   "source": [
    "plt.imshow(imread('solution_stitched_images.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Stiched Images Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ge8dzGgLKcKs",
   "metadata": {
    "id": "ge8dzGgLKcKs"
   },
   "source": [
    "# Part 2: Epipolar Geometry and Stereo Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uutLHJCMZgTH",
   "metadata": {
    "id": "uutLHJCMZgTH"
   },
   "source": [
    "## 1. Drawing Epipolar Lines (10 points)\n",
    "\n",
    "Recall our discussion of epipolar geometry from Lecture 9.\n",
    "\n",
    "The **epipolar constraint** states that if we observe a single point $x$ in one image, we must find the corresponding $x'$ in the other image along the **epipolar line** corresponding to $x$. We represented the epipolar constraint as an equation $$x'^T \\mathbf{F}x = 0$$ where $\\bf F$ is the **fundamental matrix**. We discussed estimating $\\bf F$ using the normalized 8-point algorithm.\n",
    "\n",
    "In this problem, we'll use the OpenCV library to estimate this fundamental matrix and draw epipolar lines on two images that capture the same scene from different cameras. First, run the following cell to load and display the two images we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YFi7XL7X7zT0",
   "metadata": {
    "id": "YFi7XL7X7zT0"
   },
   "outputs": [],
   "source": [
    "img1 = cv.imread('dvd_left.png', cv.IMREAD_GRAYSCALE)\n",
    "img2 = cv.imread('dvd_right.png', cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(img2)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rniOS1_zTzex",
   "metadata": {
    "id": "rniOS1_zTzex"
   },
   "source": [
    "Similar to Homework 2, we'll use SIFT to extract keypoints and descriptors from the two images.\n",
    "\n",
    " We'll then use a [FLANN-based nearest neighbor search](https://github.com/flann-lib/flann) to filter good matches.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Gwnzvcg5TusW",
   "metadata": {
    "id": "Gwnzvcg5TusW"
   },
   "outputs": [],
   "source": [
    "sift = cv.SIFT_create()\n",
    "\n",
    "# Find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "flann = cv.FlannBasedMatcher(index_params,search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < 0.8*n.distance:\n",
    "        pts2.append(kp2[m.trainIdx].pt)\n",
    "        pts1.append(kp1[m.queryIdx].pt)\n",
    "\n",
    "pts1 = np.int32(pts1)\n",
    "pts2 = np.int32(pts2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l5yEBIE7UYVO",
   "metadata": {
    "id": "l5yEBIE7UYVO"
   },
   "source": [
    "**Now, fill in code in the cell below.**\n",
    "1. Estimate the fundamental matrix using a call to **[`cv.findFundamentalMat`](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#ga59b0d57f46f8677fb5904294a23d404a)** with the relevant parameters.\n",
    "  - For the `method` parameter, you can play around with different values and compare the results you get.\n",
    "  - In our solution code, we used `cv.FM_LMEDS`, which applies the LMedS (Least Median of Squares) algorithm.\n",
    "\n",
    "2. Using the `mask` output as a boolean mask, select only the inlier points (those where `mask.ravel()` is 1) in both images.\n",
    "\n",
    "3. Compute the epipolar lines for both images using [**`cv.computeCorrespondEpilines`**](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#ga19e3401c94c44b47c229be6e51d158b7).\n",
    "  - You'll need to reshape the points array you pass in from to be shape `(N, 1, 2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yBUQDbKkUWNG",
   "metadata": {
    "id": "yBUQDbKkUWNG"
   },
   "outputs": [],
   "source": [
    "# Estimate the fundamental matrix\n",
    "F, mask = pass # YOUR CODE HERE\n",
    "\n",
    "# Select only the inlier points\n",
    "pts1 = pass # YOUR CODE HERE\n",
    "pts2 = pass # YOUR CODE HERE\n",
    "\n",
    "# Compute the epipolar lines in image 1 corresponding to points in image 2\n",
    "lines1 = pass # YOUR CODE HERE\n",
    "\n",
    "# Compute the epipolar lines in image 2 corresponding to points in image 1\n",
    "lines2 = pass # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-ZRHG-i_WGVa",
   "metadata": {
    "id": "-ZRHG-i_WGVa"
   },
   "source": [
    "Finally, we can visualize the epipolar lines on both the left and right images.\n",
    "\n",
    "*Note: It's normal to get different outputs here each time you re-run the above cells. <br>\n",
    "If your output isn't similar to the solution, try re-running this part of the notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Lmj-azzL-xEu",
   "metadata": {
    "id": "Lmj-azzL-xEu"
   },
   "outputs": [],
   "source": [
    "# Helper function to draw the epipolar lines\n",
    "def drawlines(img1, img2, lines, pts1, pts2):\n",
    "  r, c = img1.shape\n",
    "  img1 = cv.cvtColor(img1, cv.COLOR_GRAY2BGR)\n",
    "  img2 = cv.cvtColor(img2, cv.COLOR_GRAY2BGR)\n",
    "\n",
    "  for r, pt1, pt2 in zip(lines, pts1, pts2):\n",
    "      color = tuple(np.random.randint(0,255,3).tolist())\n",
    "      x0, y0 = map(int, [0, -r[2]/r[1] ])\n",
    "      x1, y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])\n",
    "      img1 = cv.line(img1, (x0,y0), (x1,y1), color, 1)\n",
    "      img1 = cv.circle(img1, tuple(pt1), 5, color, -1)\n",
    "      img2 = cv.circle(img2, tuple(pt2), 5, color, -1)\n",
    "  return img1, img2\n",
    "\n",
    "# Draw epipolar lines corresponding to points in right image on left image\n",
    "lines1 = lines1.reshape(-1,3)\n",
    "img5, img6 = drawlines(img1, img2, lines1, pts1, pts2)\n",
    "\n",
    "# Draw epipolar lines corresponding to points in left image on right image\n",
    "lines2 = lines2.reshape(-1,3)\n",
    "img3, img4 = drawlines(img2, img1, lines2, pts2, pts1)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(img5)\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(img3)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XFulRCNPIe21",
   "metadata": {
    "id": "XFulRCNPIe21"
   },
   "outputs": [],
   "source": [
    "plt.imshow(imread('solution_epipolar_lines.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Epipolar Lines Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "znc1oxb-Kg0O",
   "metadata": {
    "id": "znc1oxb-Kg0O"
   },
   "source": [
    "## 2. Computing Disparity Map (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IUsOPG3RqBVF",
   "metadata": {
    "id": "IUsOPG3RqBVF"
   },
   "source": [
    "Recall the basic stereo matching algorithm we discussed in Lecture 9: for each pixel $x$ in the first image, we can find the corresponding epipolar scanline in the second image, pick the best matching pixel $x'$ on that scaline, and then compute the $\\textbf{disparity} = x - x'$.\n",
    "\n",
    "In this section, we'll work with these two images of a motorcycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yGrVPV26qsox",
   "metadata": {
    "id": "yGrVPV26qsox"
   },
   "outputs": [],
   "source": [
    "img1 = cv.imread('motorcycle_left.png', cv.IMREAD_GRAYSCALE)\n",
    "img2 = cv.imread('motorcycle_right.png', cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(img2)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w0_Z2pCOqqJo",
   "metadata": {
    "id": "w0_Z2pCOqqJo"
   },
   "source": [
    "Given these two images, we can compute a disparity map using OpenCV's stereo semi-global block matching **[`StereoSGBM`](https://docs.opencv.org/3.4/d2/d85/classcv_1_1StereoSGBM.html#details).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QgHaYge2ONii",
   "metadata": {
    "id": "QgHaYge2ONii"
   },
   "outputs": [],
   "source": [
    "stereo = cv.StereoSGBM_create(minDisparity=16, numDisparities=200, blockSize=7)\n",
    "disparity = stereo.compute(img1, img2).astype(np.float32) / 16.0\n",
    "\n",
    "plt.imshow(disparity)\n",
    "plt.axis('off')\n",
    "plt.title('Disparity Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7oRjHYp6rOwt",
   "metadata": {
    "id": "7oRjHYp6rOwt"
   },
   "source": [
    "**Question:** Does the disparity map above look reasonable to you? Why do closer objects appear lighter and farther objects appear darker? What additional steps would we need to take to be able to convert this disparity map to a dense depth map?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_Vd8Vj3br6x_",
   "metadata": {
    "id": "_Vd8Vj3br6x_"
   },
   "source": [
    "**Your Answer Here:** Write a short paragraph responding to the questions above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafd46d",
   "metadata": {},
   "source": [
    "## 3. Rerendering from Different Camera Viewpoint (Extra Credit)\n",
    "\n",
    "We can use the disparity map computed above to rerender the motorcycle image from a new camera viewpoint.\n",
    "\n",
    "To do this, we have some basic function templates below.\n",
    "\n",
    "1. In **`disparity_to_depth`**, convert from disparity to depth, using the relation $$\\text{depth} = \\frac{B * f}{\\text{disparity}}$$ Estimates of $B$ (the baseline) and $f$ (the focal length) are given to you.\n",
    "\n",
    "2. In **`backproject_to_3D`**, backproject the 2D image points to 3D using the full camera intrinsics $K$.\n",
    "\n",
    "3. In **`transform_3D_points`**, apply the given rotation and translation matrices to the 3D points.\n",
    "\n",
    "4. In **`project_to_new_image`**, reproject the translated 3D points onto our new image plane (our new viewpoint).\n",
    "\n",
    "You can add any other functions that you want to improve the appearance of the rerendered image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f13546",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRA CREDIT FUNCTIONS (feel free to add any functions to this)\n",
    "\n",
    "def disparity_to_depth(disparity, B, f):\n",
    "    \"\"\"\n",
    "    Convert disparity map to depth map.\n",
    "\n",
    "    Args:\n",
    "        disparity: disparity output from OpenCV's stereo matching algorithm\n",
    "        B: baseline\n",
    "        f: focal length\n",
    "\n",
    "    Returns:\n",
    "        depth: depth map\n",
    "\n",
    "    Hint:\n",
    "        When applying the formula (B * f) / disparity, add a small\n",
    "        value (like 1e-6) to the denominator to avoid division by zero.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return depth\n",
    "\n",
    "def backproject_to_3D(disparity, K, B):\n",
    "    \"\"\"\n",
    "    Convert disparity map to 3D points using camera intrinsics.\n",
    "\n",
    "    Args:\n",
    "        disparity: disparity output from OpenCV's stereo matching algorithm\n",
    "        K: full camera intrinsics matrix\n",
    "        B: baseline\n",
    "\n",
    "    Returns:\n",
    "        points_3D: 3D points in the original camera frame\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return points_3D\n",
    "\n",
    "def transform_3D_points(points_3D, R_new, T_new):\n",
    "    \"\"\"\n",
    "    Apply rigid transformation to 3D points.\n",
    "\n",
    "    Args:\n",
    "        points_3D: 3D points in the original camera frame\n",
    "        R_new: 3x3 rotation matrix\n",
    "        T_new: 3x1 translation vector\n",
    "\n",
    "    Returns:\n",
    "        points_3D_transformed: 3D points in the new camera frame\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return points_3D_transformed\n",
    "\n",
    "def project_to_new_image(points_3D_transformed, K_new, image):\n",
    "    \"\"\"\n",
    "    Project transformed 3D points back to the 2D image plane.\n",
    "\n",
    "    Args:\n",
    "        points_3D_transformed: 3D points in the new camera frame\n",
    "        K_new: full camera intrinsics matrix for the new camera\n",
    "        image: original image\n",
    "\n",
    "    Returns:\n",
    "        output_image: original image in the new camera frame\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d36f549",
   "metadata": {},
   "source": [
    "The following cell will test out the functions above (feel free to modify it depending on your specific implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv.imread('motorcycle_left.png')\n",
    "image_height, image_width, _ = image.shape\n",
    "\n",
    "def estimate_intrinsics(image_width, image_height, fov=960):\n",
    "    \"\"\"Estimate camera intrinsics with an assumed field of view in degrees.\"\"\"\n",
    "    f = image_width / (2 * np.tan(np.radians(fov / 2)))\n",
    "    K = np.array([[f, 0, image_width / 2],\n",
    "                  [0, f, image_height / 2],\n",
    "                  [0, 0, 1]])\n",
    "    return f, K\n",
    "\n",
    "f, K = estimate_intrinsics(image_width, image_height)\n",
    "B = 0.1 # an estimate\n",
    "K_new = K\n",
    "\n",
    "# New camera pose (you can set these to whatever you like)\n",
    "R_new = np.eye(3)  # No rotation\n",
    "T_new = np.array([0.02, 0, 0])  # Move 2cm right\n",
    "\n",
    "# Compute 3D points and transform to new view\n",
    "points_3D = backproject_to_3D(disparity, K, B)\n",
    "points_3D_transformed = transform_3D_points(points_3D, R_new, T_new)\n",
    "output_image = project_to_new_image(points_3D_transformed, K_new, image)\n",
    "\n",
    "cv.imwrite('motorcycle_novel_view.png', output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75132652",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imread('motorcycle_novel_view.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Rerendered Image')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
