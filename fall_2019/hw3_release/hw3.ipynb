{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "This assignment covers the Harris corner detector, RANSAC and the HOG descriptor for panorama stitching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Setup\n",
    "import numpy as np\n",
    "from skimage import filters\n",
    "from skimage.feature import corner_peaks\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Panorama Stitching\n",
    "Panorama stitching is an early success of computer vision. Matthew Brown and David G. Lowe published a famous [panoramic image stitching paper](http://matthewalunbrown.com/papers/ijcv2007.pdf) in 2007. Since then, automatic panorama stitching technology has been widely adopted in many applications such as Google Street View, panorama photos on smartphones,\n",
    "and stitching software such as Photosynth and AutoStitch.\n",
    "\n",
    "In this assignment, we will detect and match keypoints from multiple images to build a single panoramic image. This will involve several tasks:\n",
    "1. Use Harris corner detector to find keypoints.\n",
    "2. Build a descriptor to describe each point in an image. <br>\n",
    "   Compare two sets of descriptors coming from two different images and find matching keypoints.\n",
    "3. Given a list of matching keypoints, use least-squares method to find the affine transformation matrix that maps points in one image to another.\n",
    "4. Use RANSAC to give a more robust estimate of affine transformation matrix. <br>\n",
    "   Given the transformation matrix, use it to transform the second image and overlay it on the first image, forming a panorama.\n",
    "5. Implement a different descriptor (HOG descriptor) and get another stitching result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Harris Corner Detector (20 points)\n",
    "\n",
    "\n",
    "In this section, you are going to implement Harris corner detector for keypoint localization. Review the lecture slides on Harris corner detector to understand how it works. The Harris detection algorithm can be divide into the following steps:\n",
    "1. Compute $x$ and $y$ derivatives ($I_x, I_y$) of an image\n",
    "2. Compute products of derivatives ($I_x^2, I_y^2, I_{xy}$) at each pixel\n",
    "3. Compute matrix $M$ at each pixel, where\n",
    "$$\n",
    "M = \\sum_{x,y} w(x,y)\n",
    "    \\begin{bmatrix}\n",
    "        I_{x}^2 & I_{x}I_{y} \\\\\n",
    "        I_{x}I_{y} & I_{y}^2\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "4. Compute corner response $R=Det(M)-k(Trace(M)^2)$ at each pixel\n",
    "5. Output corner response map $R(x,y)$\n",
    "\n",
    "Step 1 is already done for you in the function **`harris_corners`** in `panorama.py`. Complete the function implementation and run the code below.\n",
    "\n",
    "*-Hint: You may use the function `scipy.ndimage.filters.convolve`, which is already imported in `panoramy.py`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import harris_corners\n",
    "\n",
    "img = imread('sudoku.png', as_grey=True)\n",
    "\n",
    "# Compute Harris corner response\n",
    "response = harris_corners(img)\n",
    "\n",
    "# Display corner response\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(response)\n",
    "plt.axis('off')\n",
    "plt.title('Harris Corner Response')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(imread('solution_harris.png', as_grey=True))\n",
    "plt.axis('off')\n",
    "plt.title('Harris Corner Solution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you implement the Harris detector correctly, you will be able to see small bright blobs around the corners of the sudoku grids and letters in the output corner response image. The function `corner_peaks` from `skimage.feature` performs non-maximum suppression to take local maxima of the response map and localize keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform non-maximum suppression in response map\n",
    "# and output corner coordiantes\n",
    "corners = corner_peaks(response, threshold_rel=0.01)\n",
    "\n",
    "# Display detected corners\n",
    "plt.imshow(img)\n",
    "plt.scatter(corners[:,1], corners[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Corners')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Describing and Matching Keypoints (20 points)\n",
    "\n",
    "We are now able to localize keypoints in two images by running the Harris corner detector independently on them. Next question is, how do we determine which pair of keypoints come from corresponding locations in those two images? In order to *match* the detected keypoints, we must come up with a way to *describe* the keypoints based on their local appearance. Generally, each region around detected keypoint locations is converted into  a fixed-size vectors called *descriptors*.\n",
    "\n",
    "### Part 2.1 Creating Descriptors (10 points)\n",
    "\n",
    "In this section, you are going to implement the **`simple_descriptor`** function, where each keypoint is described by the normalized intensity of a small patch around it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from panorama import harris_corners\n",
    "\n",
    "img1 = imread('uttower1.jpg', as_grey=True)\n",
    "img2 = imread('uttower2.jpg', as_grey=True)\n",
    "\n",
    "# Detect keypoints in two images\n",
    "keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "\n",
    "# Display detected keypoints\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.scatter(keypoints1[:,1], keypoints1[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 1')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2)\n",
    "plt.scatter(keypoints2[:,1], keypoints2[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2 Matching Descriptors (10 points)\n",
    "Next, implement the **`match_descriptors`** function to find good matches in two sets of descriptors. First, calculate Euclidean distance between all pairs of descriptors from image 1 and image 2. Then use this to determine if there is a good match: if the distance to the closest vector is significantly (by a given factor) smaller than the distance to the second-closest, we call it a match. The output of the function is an array where each row holds the indices of one pair of matching descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import simple_descriptor, match_descriptors, describe_keypoints\n",
    "from utils import plot_matches\n",
    "\n",
    "patch_size = 5\n",
    "    \n",
    "# Extract features from the corners\n",
    "desc1 = describe_keypoints(img1, keypoints1,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "desc2 = describe_keypoints(img2, keypoints2,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "matches = match_descriptors(desc1, desc2, 0.7)\n",
    "\n",
    "# Plot matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "ax.axis('off')\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, matches)\n",
    "plt.show()\n",
    "plt.imshow(imread('solution_simple_descriptor.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Matched Simple Descriptor Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Transformation Estimation (20 points)\n",
    "\n",
    "We now have a list of matched keypoints across the two images. We will use this to find a transformation matrix that maps points in the second image to the corresponding coordinates in the first image. In other words, if the point $p_1 = [y_1,x_1]$ in image 1 matches with $p_2=[y_2, x_2]$ in image 2, we need to find an affine transformation matrix $H$ such that\n",
    "\n",
    "$$\n",
    "\\tilde{p_2}H = \\tilde{p_1},\n",
    "$$\n",
    "\n",
    "where $\\tilde{p_1}$ and $\\tilde{p_2}$ are homogenous coordinates of $p_1$ and $p_2$.\n",
    "\n",
    "Note that it may be impossible to find the transformation $H$ that maps every point in image 2 exactly to the corresponding point in image 1. However, we can estimate the transformation matrix with least squares. Given $N$ matched keypoint pairs, let $X_1$ and $X_2$ be $N \\times 3$ matrices whose rows are homogenous coordinates of corresponding keypoints in image 1 and image 2 respectively. Then, we can estimate $H$ by solving the least squares problem,\n",
    "\n",
    "$$\n",
    "X_2 H = X_1\n",
    "$$\n",
    "\n",
    "Implement **`fit_affine_matrix`** in `panorama.py`\n",
    "\n",
    "*-Hint: read the [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html) about np.linalg.lstsq*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import fit_affine_matrix\n",
    "\n",
    "# Sanity check for fit_affine_matrix\n",
    "\n",
    "# Test inputs\n",
    "a = np.array([[0.5, 0.1], [0.4, 0.2], [0.8, 0.2]])\n",
    "b = np.array([[0.3, -0.2], [-0.4, -0.9], [0.1, 0.1]])\n",
    "\n",
    "H = fit_affine_matrix(b, a)\n",
    "\n",
    "# Target output\n",
    "sol = np.array(\n",
    "    [[1.25, 2.5, 0.0],\n",
    "     [-5.75, -4.5, 0.0],\n",
    "     [0.25, -1.0, 1.0]]\n",
    ")\n",
    "\n",
    "error = np.sum((H - sol) ** 2)\n",
    "\n",
    "if error < 1e-20:\n",
    "    print('Implementation correct!')\n",
    "else:\n",
    "    print('There is something wrong.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking that your `fit_affine_matrix` function is running correctly, run the following code to apply it to images.\n",
    "Images will be warped and image 2 will be mapped to image 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_output_space, warp_image\n",
    "\n",
    "# Extract matched keypoints\n",
    "p1 = keypoints1[matches[:,0]]\n",
    "p2 = keypoints2[matches[:,1]]\n",
    "\n",
    "# Find affine transformation matrix H that maps p2 to p1\n",
    "H = fit_affine_matrix(p1, p2)\n",
    "\n",
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "print(\"Output shape:\", output_shape)\n",
    "print(\"Offset:\", offset)\n",
    "\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the two warped images are merged to get a panorama. Your panorama may not look good at this point, but we will later use other techniques to get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 RANSAC (20 points)\n",
    "Rather than directly feeding all our keypoint matches into ``fit_affine_matrix`` function, we can instead use RANSAC (\"RANdom SAmple Consensus\") to select only \"inliers\" to use for computing the transformation matrix.\n",
    "\n",
    "The steps of RANSAC are:\n",
    "    1. Select random set of matches\n",
    "    2. Compute affine transformation matrix\n",
    "    3. Find inliers using the given threshold\n",
    "    4. Repeat and keep the largest set of inliers\n",
    "    5. Re-compute least-squares estimate on all of the inliers\n",
    "\n",
    "Implement **`ransac`** in `panorama.py`, run through the following code to get a panorama. You can see the difference from the result we get without RANSAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import ransac\n",
    "\n",
    "# Set seed to compare output against solution image\n",
    "np.random.seed(131)\n",
    "\n",
    "H, robust_matches = ransac(keypoints1, keypoints2, matches, threshold=1)\n",
    "\n",
    "# Visualize robust matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, robust_matches)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_ransac.png'))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the tranformation matrix $H$ computed using the robust matches to warp our images and create a better-looking panorama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_ransac_panorama.png'))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Panorama Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 Histogram of Oriented Gradients (HOG) (20 points)\n",
    "In the above code, you are using the `simple_descriptor`, and in this section, you are going to implement a simplified version of HOG descriptor. <br>\n",
    "HOG stands for Histogram of\tOriented Gradients. In HOG descriptor, the distribution ( histograms ) of the directions of gradients ( oriented gradients ) are used as features. Gradients ( x and y derivatives ) of an image are useful because the magnitude of a gradient is large around edges and corners ( regions of abrupt intensity changes ) and we know that edges and corners pack in a lot more information about object shape than flat regions.<br>\n",
    "The steps of HOG are: \n",
    "    1. Compute the gradient image in x and y directions\n",
    "        Use the sobel filter provided by skimage.filters\n",
    "    2. Compute gradient histograms\n",
    "        Divide image into cells, and calculate histogram of gradients in each cell\n",
    "    3. Flatten block of histograms into feature vector\n",
    "    4. Normalize flattened block\n",
    "\n",
    "Implement **`hog_descriptor`** in `panorama.py` and run through the following code to get a panorama image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import hog_descriptor\n",
    "\n",
    "img1 = imread('uttower1.jpg', as_grey=True)\n",
    "img2 = imread('uttower2.jpg', as_grey=True)\n",
    "\n",
    "# Detect keypoints in both images\n",
    "keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the corners\n",
    "desc1 = describe_keypoints(img1, keypoints1,\n",
    "                           desc_func=hog_descriptor,\n",
    "                           patch_size=16)\n",
    "desc2 = describe_keypoints(img2, keypoints2,\n",
    "                           desc_func=hog_descriptor,\n",
    "                           patch_size=16)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "matches = match_descriptors(desc1, desc2, 0.7)\n",
    "\n",
    "# Plot matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "ax.axis('off')\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, matches)\n",
    "plt.show()\n",
    "plt.imshow(imread('solution_hog.png'))\n",
    "plt.axis('off')\n",
    "plt.title('HOG descriptor Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've described our keypoints with the HOG descriptor and have found matches between these keypoints, we can use RANSAC to select robust matches for computing the tranasformtion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import ransac\n",
    "\n",
    "# Set seed to compare output against solution image\n",
    "np.random.seed(131)\n",
    "\n",
    "H, robust_matches = ransac(keypoints1, keypoints2, matches, threshold=1)\n",
    "\n",
    "# Plot matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, robust_matches)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_hog_ransac.png'))\n",
    "plt.axis('off')\n",
    "plt.title('HOG descriptor + RANSAC Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the computed transformation matrix $H$ to warp our images and produce our panorama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_hog_panorama.png'))\n",
    "plt.axis('off')\n",
    "plt.title('HOG Descriptor Panorama Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit: Better Image Merging\n",
    "You will notice the blurry region and unpleasant lines in the middle of the final panoramic image. Using a very simple technique called linear blending, we can smooth out a lot of these artifacts from the panorama.\n",
    "\n",
    "Currently, all the pixels in the overlapping region are weighted equally. However, since the pixels at the left and right ends of the overlap are very well complemented by the pixels in the other image, they can be made to contribute less to the final panorama.\n",
    "\n",
    "Linear blending can be done with the following steps:\n",
    "    1. Define left and right margins for blending to occur between\n",
    "    2. Define a weight matrix for image 1 such that:\n",
    "        - From the left of the output space to the left margin the weight is 1\n",
    "        - From the left margin to the right margin, the weight linearly decrements from 1 to 0\n",
    "    3. Define a weight matrix for image 2 such that:\n",
    "        - From the right of the output space to the right margin the weight is 1\n",
    "        - From the left margin to the right margin, the weight linearly increments from 0 to 1\n",
    "    4. Apply the weight matrices to their corresponding images\n",
    "    5. Combine the images \n",
    "\n",
    "In **`linear_blend`** in `panorama.py` implement the linear blending scheme to make the panorama look more natural. This extra credit can be worth up to 1% of your final grade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import linear_blend\n",
    "\n",
    "img1 = imread('uttower1.jpg', as_grey=True)\n",
    "img2 = imread('uttower2.jpg', as_grey=True)\n",
    "\n",
    "# Set seed to compare output against solution\n",
    "np.random.seed(131)\n",
    "\n",
    "# Detect keypoints in both images\n",
    "keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "\n",
    "# Extract features from the corners\n",
    "desc1 = describe_keypoints(img1, keypoints1,\n",
    "                           desc_func=hog_descriptor,\n",
    "                           patch_size=16)\n",
    "desc2 = describe_keypoints(img2, keypoints2,\n",
    "                           desc_func=hog_descriptor,\n",
    "                           patch_size=16)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "matches = match_descriptors(desc1, desc2, 0.7)\n",
    "\n",
    "H, robust_matches = ransac(keypoints1, keypoints2, matches, threshold=1)\n",
    "\n",
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Merge the warped images using linear blending scheme\n",
    "merged = linear_blend(img1_warped, img2_warped)\n",
    "\n",
    "plt.imshow(merged)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extra Credit: Stitching Multiple Images\n",
    "Implement **`stitch_multiple_images`** in `panorama.py` to stitch together an ordered chain of images. This extra credit can be worth up to 1% of your final grade.\n",
    "\n",
    "Given a sequence of $m$ images ($I_1, I_2,...,I_m$), take every neighboring pair of images and compute the transformation matrix which converts points from the coordinate frame of $I_{i+1}$ to the frame of $I_{i}$. Then, select a reference image $I_{ref}$, which is in the middle of the chain. We want our final panorama image to be in the coordinate frame of $I_{ref}$.\n",
    "\n",
    "*-Hint:*\n",
    "- If you are confused, you may want to review the Linear Algebra slides on how to combine the effects of multiple transformation matrices.\n",
    "- The inverse of transformation matrix has the reverse effect. Please use [`numpy.linalg.inv`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html) function whenever you want to compute matrix inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import stitch_multiple_images\n",
    "\n",
    "# Set seed to compare output against solution\n",
    "np.random.seed(131)\n",
    "\n",
    "# Load images to be stitched\n",
    "img1 = imread('yosemite1.jpg', as_grey=True)\n",
    "img2 = imread('yosemite2.jpg', as_grey=True)\n",
    "img3 = imread('yosemite3.jpg', as_grey=True)\n",
    "img4 = imread('yosemite4.jpg', as_grey=True)\n",
    "\n",
    "imgs = [img1, img2, img3, img4]\n",
    "\n",
    "# Stitch images together\n",
    "panorama = stitch_multiple_images(imgs, desc_func=simple_descriptor, patch_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final panorama image\n",
    "plt.imshow(panorama)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
