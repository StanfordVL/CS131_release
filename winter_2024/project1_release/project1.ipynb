{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "520c64d2",
   "metadata": {},
   "source": [
    "# Project 1: Geometry\n",
    "In this project, we will first delve deeper into geometry that underlies how camera images are formed and panorama stitching. In the end, you will also get to explore and have fun on your own with geometry and computer vision! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6458d647",
   "metadata": {},
   "source": [
    "_**Notes on Running This Notebook:**_\n",
    "\n",
    "Make sure to run each part from its begining to ensure that you compute all of the dependencies of your current question and don't crossover variables with the same name from other questions. So long as you run each part from its beginning, you can run the parts in any order.\n",
    "\n",
    "When assembling your PDF, we recommend running all cells in order from the top of the notebook to prevent any of these discontinuity errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e432ef",
   "metadata": {},
   "source": [
    "## <font color='blue'>Part 1: Geometry & Camera (60 points)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8164ea5",
   "metadata": {},
   "source": [
    "## 1.1 Transformations in 3D (27 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2455ed66",
   "metadata": {},
   "source": [
    "In order to make sense of how objects in our world are rendered in a camera, we typically need to understand how they are located relative to the camera. In this question, we'll examine some properties of the transformations that formalize this process by expressing coordinates with respect to multiple frames. \n",
    "\n",
    "We'll be considering a scene with two frames: a world frame ($W$) and a camera frame ($C$).\n",
    "\n",
    "Notice that:\n",
    "- We have 3D points $p$, $q$, $r$, and $s$ that define a square, which is parallel to the world $zy$ plane\n",
    "- $C_z$ and $C_x$ belong to the plane defined by $W_z$ and $W_x$\n",
    "- $C_y$ is parallel to $W_y$\n",
    "\n",
    "<!-- into camera space as we translate a simple shape from world coordinates to camera coordinates. We will take this square in world coordinates and transform it into the camera coordinates.-->\n",
    "\n",
    "<img src=\"part1/images/projection_geometry.png\" alt=\"projection geometry figure\" width=\"640\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def hash_numpy(x):\n",
    "    import hashlib\n",
    "\n",
    "    return hashlib.sha1(x.view(np.uint8)).hexdigest()\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b536da",
   "metadata": {},
   "source": [
    "### 1.1 (a) Reference Frame Definitions (3 points)\n",
    "\n",
    "First, let's take a moment to validate our understanding of 3D reference frames.\n",
    "\n",
    "Consider creating:\n",
    "- A point $w$ at the origin of the world frame ($O_w$)\n",
    "- A point $c$ at the origin of the camera frame ($O_c$)\n",
    "\n",
    "Examine the $x$, $y$, and $z$ axes of each frame, then express these points with respect to the world and camera frames. Fill in **`w_wrt_camera`**, **`w_wrt_world`**, and **`c_wrt_camera`**.\n",
    "\n",
    "You can consider the length $d = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1.0\n",
    "\n",
    "# Abbreviation note:\n",
    "# - \"wrt\" stands for \"with respect to\", which is ~synonymous with \"relative to\"\n",
    "\n",
    "w_wrt_world = np.array([0.0, 0.0, 0.0])  # Done for you\n",
    "w_wrt_camera = None  # Assign me!\n",
    "\n",
    "c_wrt_world = None  # Assign me!\n",
    "c_wrt_camera = None  # Assign me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check your answers!\n",
    "assert (\n",
    "    (3,)\n",
    "    == w_wrt_world.shape\n",
    "    == w_wrt_camera.shape\n",
    "    == c_wrt_world.shape\n",
    "    == c_wrt_camera.shape\n",
    "), \"Wrong shape!\"\n",
    "assert (\n",
    "    hash_numpy(w_wrt_world) == \"d3399b7262fb56cb9ed053d68db9291c410839c4\"\n",
    "), \"Double check your w_wrt_world!\"\n",
    "assert (\n",
    "    hash_numpy(w_wrt_camera) == \"6248a1dcfe0c8822ba52527f68f7f98955584277\"\n",
    "), \"Double check your w_wrt_camera!\"\n",
    "assert (\n",
    "    hash_numpy(c_wrt_camera) == \"d3399b7262fb56cb9ed053d68db9291c410839c4\"\n",
    "), \"Double check your c_wrt_camera!\"\n",
    "assert (\n",
    "    hash_numpy(c_wrt_world) == \"a4c525cd853a072d96cade8b989a9eaf1e13ed3d\"\n",
    "), \"Double check your c_wrt_world!\"\n",
    "\n",
    "print(\"Looks correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220f414",
   "metadata": {},
   "source": [
    "### 1.1 (b) World â‡¨ Camera Transforms (15 points)\n",
    "\n",
    "Derive the homogeneous transformation matrix needed to convert a point expressed with respect to the world frame $W$ in the camera frame $C$.\n",
    "\n",
    "**Discuss the rotation and translation terms in this matrix and how you determined them, then implement it in `camera_from_world_transform()`** in `cameras.py` file.\n",
    "\n",
    "We've also supplied a set of `assert` statements below to help you check your work.\n",
    "\n",
    "---\n",
    "\n",
    "*Hint #1:*\n",
    "With rotation matrix $R \\in \\mathbb{R}^{3\\times 3}$ and translation vector $t \\in \\mathbb{R}^{3\\times 1}$, you can write transformations as $4 \\times 4$ matrices: \n",
    "$$\n",
    "\\begin{bmatrix}{x_C} \\\\ {y_C} \\\\ {z_C} \\\\ 1\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    R & t \\\\\n",
    "    \\vec{0}^\\top & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}{x_W} \\\\ {y_W} \\\\ {z_W} \\\\ 1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "*Hint #2: Remember our 2D transformation matrix for rotations in the $xy$ plane.*\n",
    "\n",
    "$$\\begin{bmatrix}{x}' \\\\ {y}'\\end{bmatrix} = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta)\\end{bmatrix} \\begin{bmatrix}{x} \\\\ {y}\\end{bmatrix}$$ \n",
    "\n",
    "To apply this to 3D rotations, you might think of this $xy$ plane rotation as holding the $z$ coordinate constant, since that's the axis you're rotating around, and transforming the $x$ and $y$ coordinates as described in the 2D formulation:\n",
    "\n",
    "$$\\begin{bmatrix}{x}' \\\\ {y}' \\\\ {z}'\\end{bmatrix}  = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) & 0 \\\\ \\sin(\\theta) & \\cos(\\theta) & 0 \\\\ 0 & 0 & 1\\end{bmatrix} \\begin{bmatrix}{x} \\\\ {y} \\\\ {z}\\end{bmatrix}$$\n",
    "\n",
    "(Alternatively you could simply take the rotation matrix from the [Wikipedia](https://en.wikipedia.org/wiki/Rotation_matrix) page)\n",
    "\n",
    "*Hint #3: In a homogeneous transform, the translation is applied after the rotation.*\n",
    "\n",
    "As a result, you can visualize the translation as an offset in the output frame.\n",
    "\n",
    "The order matters! You'll end up with a different transformation if you translate and then rotate versus if you rotate first and then translate with the same offsets. In lecture 2 we discussed a formulation for a combinated scaling, rotating, and translating matrix (in that order), which can be a useful starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaabfea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Your response here:** Write your answer in this markdown cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1bc66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your answer against 1.1!\n",
    "from part1.cameras import camera_from_world_transform\n",
    "\n",
    "T_camera_from_world = camera_from_world_transform()\n",
    "\n",
    "# Check c_wrt_camera against T_camera_from_world @ w_wrt_world\n",
    "w_wrt_camera_computed = (T_camera_from_world @ np.append(w_wrt_world, 1.0))[:3]\n",
    "print(f\"w_wrt camera: expected {w_wrt_camera}, computed {w_wrt_camera_computed}\")\n",
    "assert np.allclose(\n",
    "    w_wrt_camera, w_wrt_camera_computed\n",
    "), \"Error! (likely bad translation)\"\n",
    "print(\"Translation components look reasonable!\")\n",
    "\n",
    "# Check w_wrt_camera against T_camera_from_world @ c_wrt_world\n",
    "c_wrt_camera_computed = (T_camera_from_world @ np.append(c_wrt_world, 1.0))[:3]\n",
    "print(f\"c_wrt camera: expected {c_wrt_camera}, computed {c_wrt_camera_computed}\")\n",
    "assert np.allclose(\n",
    "    c_wrt_camera, c_wrt_camera_computed\n",
    "), \"Error! (likely bad rotation)\"\n",
    "print(\"Rotation components looks reasonable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d29452",
   "metadata": {},
   "source": [
    "### 1.1 (c) Preserving Edge Orientations (Geometric Intuition) (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86588a6",
   "metadata": {},
   "source": [
    "Under the translation and rotation transformation from world coordinates to camera coordinates, which, if any, of the edges of the square retain their orientation and why? \n",
    "\n",
    "For those that change orientation, how do they change? (e.g. translation x,y,z and rotation in one of our planes). \n",
    "\n",
    "A sentence or two of geometric intuition is sufficient for each question, such as reasoning about the orientation of the edges and which axes we're rotating and translating about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4affde",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Your response here:** Write your answer in this markdown cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9ce11",
   "metadata": {},
   "source": [
    "### 1.1 (d) Visualization (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899746a",
   "metadata": {},
   "source": [
    "Implement **`apply_transform()`** to help us apply a homogeneous transformation to a batch of points.\n",
    "\n",
    "Then, run the cell below to start visualizing our frames and the world square in PyPlot!\n",
    "\n",
    "Using your code, we can animate a GIF that shows the transition of the square from its position in world coordinates to a new position in camera coordinates. We transform the perspective continuously from the world coordinate system to the camera coordinate system. Analogous to a homogeneous transform, you can see that we first rotate to match the orientation of the camera coordinate system, then translate to match the position of the camera origin. \n",
    "\n",
    "If you want to see how the animation was computed or if you want to play around with its configuration, then check out **`animate_transformation`** in **`utils.py`**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9754815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from part1.cameras import apply_transform\n",
    "from part1.utils import (\n",
    "    animate_transformation,\n",
    "    configure_ax,\n",
    "    plot_frame,\n",
    "    plot_square,\n",
    ")\n",
    "\n",
    "# Vertices per side of the square\n",
    "N = 2\n",
    "\n",
    "# Compute vertices corresponding to each side of the square\n",
    "vertices_wrt_world = np.concatenate(\n",
    "    [\n",
    "        np.vstack([np.zeros(N), np.linspace(1, 2, N), np.ones(N)]),\n",
    "        np.vstack([np.zeros(N), np.ones(N) + 1, np.linspace(1, 2, N)]),\n",
    "        np.vstack([np.zeros(N), np.linspace(2, 1, N), np.ones(N) + 1]),\n",
    "        np.vstack([np.zeros(N), np.ones(N), np.linspace(1, 2, N)]),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Visualize our rotation!\n",
    "animate_transformation(\n",
    "    \"transformation.gif\",\n",
    "    vertices_wrt_world,\n",
    "    camera_from_world_transform,\n",
    "    apply_transform,\n",
    ")\n",
    "\n",
    "import IPython.display\n",
    "\n",
    "with open(\"transformation.gif\", \"rb\") as file:\n",
    "    display(IPython.display.Image(file.read()))\n",
    "\n",
    "# Uncomment to compare to staff animation\n",
    "# with open(\"solution_transformation.gif\", \"rb\") as file:\n",
    "#     display(IPython.display.Image(file.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a5565",
   "metadata": {},
   "source": [
    "## 1.2 Camera Intrinsics & Vanishing Points (12 points)\n",
    "\n",
    "In a pinhole camera, lines that are parallel in 3D rarely remain parallel when projected to the image plane. Instead, parallel lines will meet at a **vanishing point**:\n",
    "\n",
    "<img alt=\"vanishing point\" src=\"part1/images/vanishing_point.jpg\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c11a44",
   "metadata": {},
   "source": [
    "### 1.2 (a) Homogeneous coordinates (6 points)\n",
    "\n",
    "Consider a line that is parallel to a world-space direction vector in the set $\\{ d \\in \\mathbb{R}^3 : d^\\top d = 1\\}$. Show that the image coordinates $v$ of the vanishing point can be be written as $v = KRd$.\n",
    "\n",
    "*Hints:*\n",
    "- As per the lecture slides, $K$ is the camera calibration matrix and $R$ is the camera extrinsic rotation.\n",
    "- As in the diagram above, the further a point on a 3D line is from the camera origin, the closer its projection will be to the line's 2D vanishing point.\n",
    "- Given a line with direction vector $d$, you can write a point that's infinitely far away from the camera via a limit: $\\lim_{\\alpha \\to \\infty} \\alpha d$.\n",
    "- The 3D homogeneous coordinate definition is:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    x & y & z & w\n",
    "\\end{bmatrix}^\\top\n",
    "\\iff\n",
    "\\begin{bmatrix}\n",
    "    x/w & y/w & z/w & 1\n",
    "\\end{bmatrix}^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c63565",
   "metadata": {},
   "source": [
    "\n",
    "**You answer here:** Write your answer in this markdown cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb8d1a",
   "metadata": {},
   "source": [
    "---\n",
    "_Note: Calibration from vanishing points_\n",
    "\n",
    "_Let $d_0, d_1, \\dots$ represent directional vectors for 3D lines in a scene, and $v_0, v_1, \\dots$ represent their corresponding vanishing points._\n",
    "\n",
    "_Consider the situtation when these lines are orthogonal:_\n",
    "$$d_i^\\top d_j = 0, \\text{for each} i \\neq j,$$\n",
    "_then we also have_\n",
    "\n",
    "$$\n",
    "(K^{-1}v_i)^\\top(K^{-1}v_j) = 0, \\text{for each } i \\neq j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4729a32",
   "metadata": {},
   "source": [
    "### 1.2 (b) Short Response (6 points)\n",
    "\n",
    "Respond to the following using bullet points:\n",
    "\n",
    "- Assuming square pixels and no skew, how many vanishing points with mutually orthogonal directions do we now need to solve for our camera's focal length and optical center?\n",
    "\n",
    "- Assuming square pixels and no skew, how many vanishing points with mutually orthogonal directions do we now need to solve for our camera's focal length when the optical center is known?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cad34b",
   "metadata": {},
   "source": [
    "\n",
    "**You answer here:** Write your answer in this markdown cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fae6be",
   "metadata": {},
   "source": [
    "## 1.3 Intrinsic Calibration (41 points)\n",
    "\n",
    "Using the vanishing point math from above, we can solve for a camera matrix $K$.\n",
    "\n",
    "First, let's load in an image. To make life easier for you, we've hand labeled a set of coordinates on it that we'll use to compute vanishing points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3249ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and annotated points; note that:\n",
    "# > Our image is a PIL image type; you can convert this to NumPy with `np.asarray(img)`\n",
    "# > Points are in (x, y) format, which corresponds to (col, row)!\n",
    "img = Image.open(\"part1/images/pressure_cooker.jpg\")\n",
    "print(f\"Image is {img.width} x {img.height}\")\n",
    "points = np.array(\n",
    "    [\n",
    "        [270.0, 327.0],  # [0]\n",
    "        [356.0, 647.0],  # [1]\n",
    "        [610.0, 76.0],  # [2]\n",
    "        [706.0, 857.0],  # [3]\n",
    "        [780.0, 585.0],  # [4]\n",
    "        [1019.0, 226.0],  # [5]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Visualize image & annotated points\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.imshow(img)\n",
    "ax.scatter(points[:, 0], points[:, 1], color=\"white\", marker=\"x\")\n",
    "for i in range(len(points)):\n",
    "    ax.annotate(\n",
    "        f\"points[{i}]\",\n",
    "        points[i] + np.array([15.0, 5.0]),\n",
    "        color=\"white\",\n",
    "        backgroundcolor=(0, 0, 0, 0.15),\n",
    "        zorder=0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c6d2d",
   "metadata": {},
   "source": [
    "### 1.3 (a) Finding Vanishing Points (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae30d0d",
   "metadata": {},
   "source": [
    "In 2D, notice that a vanishing point can be computing by finding the intersection of two lines that we know are parallel in 3D.\n",
    "\n",
    "To find the vanishing points in the image, implement **`intersection_from_lines()`**.\n",
    "\n",
    "Then, run the cell below to check that it's working.\n",
    "\n",
    "Note that later parts of this homework will fail if you choose the side face instead of the front face for producing the leftmost vanishing point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a398c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from part1.cameras import intersection_from_lines\n",
    "\n",
    "# Python trivia: the following two assert statements are the same.\n",
    "# > https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists\n",
    "# > https://numpy.org/doc/stable/reference/arrays.indexing.html#integer-array-indexing\n",
    "assert np.allclose(\n",
    "    intersection_from_lines(points[0], points[1], points[4], points[0],),\n",
    "    points[0],\n",
    ")\n",
    "assert np.allclose(intersection_from_lines(*points[[0, 1, 4, 0]]), points[0])\n",
    "print(\"Looks correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b60c689",
   "metadata": {},
   "source": [
    "To use the constraint we derived above, we need to find vanishing points that correspond to three orthogonal direction vectors.\n",
    "\n",
    "Populate `v0_indices`, `v1_indices`, and `v2_indices`, then run the cell below to compute `v`.\n",
    "\n",
    "You should be able to get an output that looks like this (color ordering does not matter):\n",
    "\n",
    "![vanishing point reference](part1/images/vanishing_point_reference.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce55cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select points used to compute each vanishing point\n",
    "#\n",
    "# Each `v*_indices` list should contain four integers, corresponding to\n",
    "# indices into the `points` array; the first two ints define one line and\n",
    "# the second two define another line.\n",
    "v0_indices = None\n",
    "v1_indices = None\n",
    "v2_indices = None\n",
    "\n",
    "### YOUR CODE HERE\n",
    "pass\n",
    "### END YOUR CODE\n",
    "\n",
    "# Validate indices\n",
    "assert (\n",
    "    len(v0_indices) == len(v1_indices) == len(v2_indices) == 4\n",
    "), \"Invalid length!\"\n",
    "for i, j, k in zip(v0_indices, v1_indices, v2_indices):\n",
    "    assert type(i) == type(j) == type(k) == int, \"Invalid type!\"\n",
    "\n",
    "# Compute vanishing points\n",
    "v = np.zeros((3, 2))\n",
    "v[:, :2] = np.array(\n",
    "    [\n",
    "        intersection_from_lines(*points[v0_indices]),\n",
    "        intersection_from_lines(*points[v1_indices]),\n",
    "        intersection_from_lines(*points[v2_indices]),\n",
    "    ]\n",
    ")\n",
    "assert v.shape == (3, 2)\n",
    "\n",
    "# Display image\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.imshow(img)\n",
    "\n",
    "# Display annotated points\n",
    "ax.scatter(points[:, 0], points[:, 1], color=\"white\")\n",
    "\n",
    "# Visualize vanishing points\n",
    "colors = [\"red\", \"green\", \"blue\"]\n",
    "for indices, color in zip((v0_indices, v1_indices, v2_indices), colors):\n",
    "    ax.axline(*points[indices[:2]], zorder=0.1, c=color, alpha=0.4)\n",
    "    ax.axline(*points[indices[2:]], zorder=0.1, c=color, alpha=0.4)\n",
    "ax.scatter(v[:, 0], v[:, 1], c=colors)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1161c72",
   "metadata": {},
   "source": [
    "### 1.3 (b) Computing Optical Centers (10 points)\n",
    "\n",
    "Next, implement **`optical_center_from_vanishing_points()`** to compute the 2D optical center from our vanishing points. Then, run the cell below to compute a set of optical center coordinates from our vanishing points.\n",
    "\n",
    "*Hint:* Property 3 from [1] may be useful. (Try connecting to Stanford campus network otherwise the paper link might not work for you.)\n",
    "> [1] Caprile, B., Torre, V. **Using vanishing points for camera calibration.** *Int J Comput Vision 4, 127â€“139 (1990)*. https://doi.org/10.1007/BF00127813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da326970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from part1.cameras import optical_center_from_vanishing_points\n",
    "\n",
    "optical_center = optical_center_from_vanishing_points(v[0], v[1], v[2],)\n",
    "\n",
    "assert np.allclose(np.mean(optical_center), 583.4127277436276)\n",
    "assert np.allclose(np.mean(optical_center ** 2), 343524.39942528843)\n",
    "print(\"Looks correct!\")\n",
    "\n",
    "# Display image\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.imshow(img)\n",
    "\n",
    "# Display optical center\n",
    "ax.scatter(*optical_center, color=\"yellow\")\n",
    "ax.annotate(\n",
    "    \"Optical center\",\n",
    "    optical_center + np.array([20, 5]),\n",
    "    color=\"white\",\n",
    "    backgroundcolor=(0, 0, 0, 0.5),\n",
    "    zorder=0.1,\n",
    ")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d97e2c",
   "metadata": {},
   "source": [
    "### 1.3 (c) Computing Focal Lengths (10 points)\n",
    "\n",
    "Consider two vanishing points corresponding to orthogonal directions, and the constraint from above:\n",
    "\n",
    "$$\n",
    "(K^{-1}v_0)^\\top(K^{-1}v_1) = 0, \\text{for each } i \\neq j\n",
    "$$\n",
    "\n",
    "Derive an expression for computing the focal length when the optical center is known, then implement **`focal_length_from_two_vanishing_points()`**.\n",
    "\n",
    "When we assume square pixels and no skew, recall that the intrinsic matrix $K$ is:\n",
    "\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "    f & 0 & c_x \\\\\n",
    "    0 & f & c_y \\\\\n",
    "    0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "*Hint:* Optional, but this problem maybe be simpler if you factorize $K$ as:\n",
    "\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "    1 & 0 & c_x \\\\\n",
    "    0 & 1 & c_y \\\\\n",
    "    0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    f & 0 & 0 \\\\\n",
    "    0 & f & 0 \\\\\n",
    "    0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "When working with homogeneous coordinates, note that the lefthand matrix is a simple translation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from part1.cameras import focal_length_from_two_vanishing_points\n",
    "\n",
    "# If your implementation is correct, these should all be ~the same\n",
    "f = focal_length_from_two_vanishing_points(v[0], v[1], optical_center)\n",
    "print(f\"Focal length from v0, v1: {f}\")\n",
    "f = focal_length_from_two_vanishing_points(v[1], v[2], optical_center)\n",
    "print(f\"Focal length from v1, v2: {f}\")\n",
    "f = focal_length_from_two_vanishing_points(v[0], v[2], optical_center)\n",
    "print(f\"Focal length from v0, v2: {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff03a0",
   "metadata": {},
   "source": [
    "### 1.3 (d) Comparison to EXIF data (5 points)\n",
    "\n",
    "To validate our focal length computation, one smoke test we can run is compare it to parameters supplied by the camera manufacturer.\n",
    "\n",
    "In JPEG images, these parameters and other metadata are sometimes stored using [EXIF](https://en.wikipedia.org/wiki/Exif) tags that are written when the photo is taken. Run the cell below to read & print some of this using the Python Imaging Library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d872cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL.ExifTags import TAGS\n",
    "\n",
    "# Grab EXIF data\n",
    "exif = {TAGS[key]: value for key, value in img._getexif().items()}\n",
    "\n",
    "# Print subset of keys\n",
    "print(f\"EXIF data for {img.filename}\\n=====\")\n",
    "for key in (\n",
    "    \"DateTimeOriginal\",\n",
    "    \"FocalLength\",\n",
    "    \"GPSInfo\",\n",
    "    \"Make\",\n",
    "    \"Model\",\n",
    "):\n",
    "    print(key.ljust(25), exif[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a86b22e",
   "metadata": {},
   "source": [
    "From above, we see that the focal length of our camera system is **`4.3mm`**.\n",
    "\n",
    "Focal lengths are typically in millimeters, but all of the coordinates we've worked with thus far have been in pixel-space. Thus, we first need to convert our focal length from pixels to millimeters.\n",
    "\n",
    "Try to visualize this conversion, then implement **`physical_focal_length_from_calibration()`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05edf32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from part1.cameras import physical_focal_length_from_calibration\n",
    "\n",
    "# Length across sensor diagonal for SM-G970U (Galaxy S10e)\n",
    "# > https://en.wikipedia.org/wiki/Samsung_CMOS\n",
    "sensor_diagonal_mm = 7.06\n",
    "\n",
    "# Length across image diagonal\n",
    "image_diagonal_pixels = np.sqrt(img.width ** 2 + img.height ** 2)\n",
    "\n",
    "f_mm = physical_focal_length_from_calibration(\n",
    "    f, sensor_diagonal_mm, image_diagonal_pixels,\n",
    ")\n",
    "print(f\"Computed focal length:\".ljust(30), f_mm)\n",
    "\n",
    "error = np.abs(f_mm - 4.3) / 4.3\n",
    "print(\"Calibration vs spec error:\".ljust(30), f\"{error * 100:.2f}%\")\n",
    "assert 0.06 < error < 0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194cbccc",
   "metadata": {},
   "source": [
    "### 1.3 (e) Analysis (6 points)\n",
    "\n",
    "If everything went smoothly, your computed focal length should only deviate from the manufacturer spec by ~6.8%.\n",
    "\n",
    "Aside from manufacturing tolerances, name two or more other possible causes for this error, then discuss the limitations of this calibration method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477927d1",
   "metadata": {},
   "source": [
    "**You answer here:** Write your answer in this markdown cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7b752b",
   "metadata": {},
   "source": [
    "## <font color='blue'>Part 2: Panorama Stitching (70 points)</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e91e1c",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "\n",
    "Panorama stitching is an early success of computer vision. Matthew Brown and David G. Lowe published a famous [panoramic image stitching paper](http://matthewalunbrown.com/papers/ijcv2007.pdf) in 2007. Since then, automatic panorama stitching technology has been widely adopted in many applications such as Google Street View, panorama photos on smartphones,\n",
    "and stitching software such as Photosynth and AutoStitch.\n",
    "\n",
    "In this part of the project, we will detect and match keypoints from multiple images to build a single panoramic image. This will involve several tasks:\n",
    "1. Compare two sets of descriptors coming from two different images and find matching keypoints.\n",
    "2. Given a list of matching keypoints, use the least-squares method to find the affine transformation matrix that maps points in one image to another.\n",
    "3. Use RANSAC to give a more robust estimate of the affine transformation matrix. <br>\n",
    "   Given the transformation matrix, use it to transform the second image and overlay it on the first image, forming a panorama.\n",
    "4. Blend panorama images together to remove blurry regions of overlapping images.\n",
    "5. Stich multiple panorama images together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a73f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Setup\n",
    "import numpy as np\n",
    "from skimage import filters\n",
    "from skimage.feature import corner_peaks\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eceee0a",
   "metadata": {},
   "source": [
    "### 2.1 Matching Keypoints (10 points)\n",
    "\n",
    "You are given a set of keypoints in two images (obtained by running the Harris corner detector). The question we want to answer is: How can we determine which pairs of keypoints come from the same 3D points projected on the two different images? In order to *match* the detected keypoints, we must come up with a way to *describe* the keypoints based on their local appearance. Generally, each region around detected keypoint locations is converted into  a fixed-size vector called a *descriptor*. \n",
    "\n",
    "We have implemented a simple descriptor function for you, where each keypoint is described by the normalized intensity of a small patch around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed3af40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from part2.panorama import harris_corners\n",
    "\n",
    "img1 = imread('part2/uttower1.jpg', as_gray=True)\n",
    "img2 = imread('part2/uttower2.jpg', as_gray=True)\n",
    "\n",
    "# Detect keypoints in two images\n",
    "keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "\n",
    "print(\"Keypoints 1 shape = \", keypoints1.shape)\n",
    "print(\"Keypoints 2 shape = \", keypoints2.shape)\n",
    "\n",
    "# Display detected keypoints\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.scatter(keypoints1[:,1], keypoints1[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 1')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2)\n",
    "plt.scatter(keypoints2[:,1], keypoints2[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c2899",
   "metadata": {},
   "source": [
    "Implement the **`match_descriptors`** function to find good matches in two sets of descriptors. First, calculate Euclidean distance between all pairs of descriptors from image 1 and image 2. Then use this to determine if there is a good match: for each descriptor in image 1, if the distance to the closest descriptor in image 2 is significantly (by a given factor) smaller than the distance to the second-closest, we call it a match. The output of the function is an array where each row holds the indices of one pair of matching descriptors.\n",
    "\n",
    "*Checking your answer*: you should see an identical matching of keypoints as the solution, but the precise colors of each line will change with every run of keypoint matching so colors do not need to match.\n",
    "\n",
    "*Optional ungraded food for thought*: Think about why this method of keypoint matching is not commutative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aea9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from part2.panorama import simple_descriptor, match_descriptors, describe_keypoints\n",
    "from part2.utils import plot_matches\n",
    "\n",
    "# Set seed to compare output against solution\n",
    "np.random.seed(131)\n",
    "\n",
    "patch_size = 5\n",
    "    \n",
    "# Extract features from the corners\n",
    "desc1 = describe_keypoints(img1, keypoints1,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "desc2 = describe_keypoints(img2, keypoints2,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "\n",
    "print(\"Desc1 shape = \", desc1.shape)\n",
    "print(\"Desc2 shape = \", desc2.shape)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "matches = match_descriptors(desc1, desc2, 0.7)\n",
    "\n",
    "# Plot matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "ax.axis('off')\n",
    "plt.title('Matched Simple Descriptor')\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, matches)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('part2/solution_simple_descriptor.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Matched Simple Descriptor Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb1bea",
   "metadata": {},
   "source": [
    "### 2.2 Transformation Estimation (20 points)\n",
    "\n",
    "We now have a list of matched keypoints across the two images. We will use this to find a transformation matrix that maps points in the second image to the corresponding coordinates in the first image. In other words, if the point $p_1 = [y_1,x_1]$ in image 1 matches with $p_2=[y_2, x_2]$ in image 2, we need to find an affine transformation matrix $H$ such that\n",
    "\n",
    "$$\n",
    "\\tilde{p_2}H = \\tilde{p_1},\n",
    "$$\n",
    "\n",
    "where $\\tilde{p_1}$ and $\\tilde{p_2}$ are homogenous coordinates of $p_1$ and $p_2$.\n",
    "\n",
    "Note that it may be impossible to find the transformation $H$ that maps every point in image 2 exactly to the corresponding point in image 1. However, we can estimate the transformation matrix with least squares. Given $N$ matched keypoint pairs, let $X_1$ and $X_2$ be $N \\times 3$ matrices whose rows are homogenous coordinates of corresponding keypoints in image 1 and image 2 respectively. Then, we can estimate $H$ by solving the least squares problem,\n",
    "\n",
    "$$\n",
    "X_2 H = X_1\n",
    "$$\n",
    "\n",
    "Implement **`fit_affine_matrix`** in `panorama.py`\n",
    "\n",
    "*-Hint: read the [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html) about np.linalg.lstsq*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5df1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from part2.panorama import fit_affine_matrix\n",
    "\n",
    "# Sanity check for fit_affine_matrix\n",
    "\n",
    "# Test inputs\n",
    "a = np.array([[0.5, 0.1], [0.4, 0.2], [0.8, 0.2]])\n",
    "b = np.array([[0.3, -0.2], [-0.4, -0.9], [0.1, 0.1]])\n",
    "\n",
    "H = fit_affine_matrix(b, a)\n",
    "\n",
    "# Target output\n",
    "sol = np.array(\n",
    "    [[1.25, 2.5, 0.0],\n",
    "     [-5.75, -4.5, 0.0],\n",
    "     [0.25, -1.0, 1.0]]\n",
    ")\n",
    "\n",
    "error = np.sum((H - sol) ** 2)\n",
    "\n",
    "if error < 1e-20:\n",
    "    print('Implementation correct!')\n",
    "else:\n",
    "    print('There is something wrong.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00474282",
   "metadata": {},
   "source": [
    "After checking that your `fit_affine_matrix` function is running correctly, run the following code to apply it to images.\n",
    "Images will be warped and image 2 will be mapped to image 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from part2.utils import get_output_space, warp_image\n",
    "\n",
    "# Extract matched keypoints\n",
    "p1 = keypoints1[matches[:,0]]\n",
    "p2 = keypoints2[matches[:,1]]\n",
    "\n",
    "# Find affine transformation matrix H that maps p2 to p1\n",
    "H = fit_affine_matrix(p1, p2)\n",
    "\n",
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "print(\"Output shape:\", output_shape)\n",
    "print(\"Offset:\", offset)\n",
    "\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 Warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 Warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b60e5",
   "metadata": {},
   "source": [
    "Next, the two warped images are merged to get a panorama. Your panorama may not look good at this point, but we will later use other techniques to get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two images\n",
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.title('Fit-Affine Panorama')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('part2/solution_fit_affine_panorama.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Fit-Affine Panorama Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1294483",
   "metadata": {},
   "source": [
    "### 2.3 RANSAC (20 points)\n",
    "Rather than directly feeding all our keypoint matches into ``fit_affine_matrix`` function, we can instead use RANSAC (\"RANdom SAmple Consensus\") to select only \"inliers\" to use for computing the transformation matrix.\n",
    "\n",
    "The steps of RANSAC are:\n",
    "1. Select random set of matches\n",
    "2. Compute affine transformation matrix\n",
    "3. Find inliers using the given threshold\n",
    "4. Repeat and keep the largest set of inliers (use >, i.e. break ties by whichever set is seen first)\n",
    "5. Re-compute least-squares estimate on all of the inliers\n",
    "    \n",
    "In this case, use Euclidean distance between matched points as a measure of inliers vs outliers.\n",
    "\n",
    "Implement **`ransac`** in `panorama.py`, run through the following code to get a panorama. You can see the difference from the result we get without RANSAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525334d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from part2.panorama import ransac\n",
    "\n",
    "# Set seed to compare output against solution image\n",
    "np.random.seed(131)\n",
    "\n",
    "H, robust_matches = ransac(keypoints1, keypoints2, matches, threshold=1)\n",
    "print(\"Robust matches shape = \", robust_matches.shape)\n",
    "print(\"H = \\n\", H)\n",
    "\n",
    "# Visualize robust matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, robust_matches)\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Matches')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('part2/solution_ransac.png'))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Matches Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048db598",
   "metadata": {},
   "source": [
    "We can now use the tranformation matrix $H$ computed using the robust matches to warp our images and create a better-looking panorama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fd5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612dde50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two images\n",
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Panorama')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('part2/solution_ransac_panorama.png'))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Panorama Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254febf",
   "metadata": {},
   "source": [
    "### 2.4 Better Image Merging (10 points)\n",
    "You will notice the blurry region and unpleasant lines in the middle of the final panoramic image. Using a very simple technique called linear blending, we can smooth out a lot of these artifacts from the panorama.\n",
    "\n",
    "Currently, all the pixels in the overlapping region are weighted equally. However, since the pixels at the left and right ends of the overlap are very well complemented by the pixels in the other image, they can be made to contribute less to the final panorama.\n",
    "\n",
    "Linear blending can be done with the following steps:\n",
    "    1. Define left and right margins for blending to occur between\n",
    "    2. Define a weight matrix for image 1 such that:\n",
    "        - From the left of the output space to the left margin the weight is 1\n",
    "        - From the left margin to the right margin, the weight linearly decrements from 1 to 0\n",
    "    3. Define a weight matrix for image 2 such that:\n",
    "        - From the right of the output space to the right margin the weight is 1\n",
    "        - From the left margin to the right margin, the weight linearly increments from 0 to 1\n",
    "    4. Apply the weight matrices to their corresponding images\n",
    "    5. Combine the images \n",
    "\n",
    "In **`linear_blend`** in `panorama.py` implement the linear blending scheme to make the panorama look more natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78becc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from part2.panorama import linear_blend\n",
    "\n",
    "img1 = imread('part2/uttower1.jpg', as_gray=True)\n",
    "img2 = imread('part2/uttower2.jpg', as_gray=True)\n",
    "\n",
    "# Set seed to compare output against solution\n",
    "np.random.seed(131)\n",
    "\n",
    "# Detect keypoints in both images\n",
    "ec1_keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                              threshold_rel=0.05,\n",
    "                              exclude_border=8)\n",
    "ec1_keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                              threshold_rel=0.05,\n",
    "                              exclude_border=8)\n",
    "\n",
    "print(\"EC1 keypoints1 shape = \", ec1_keypoints1.shape)\n",
    "print(\"EC1 keypoints2 shape = \", ec1_keypoints2.shape)\n",
    "\n",
    "# Extract features from the corners\n",
    "ec1_desc1 = describe_keypoints(img1, ec1_keypoints1,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=16)\n",
    "ec1_desc2 = describe_keypoints(img2, ec1_keypoints2,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=16)\n",
    "\n",
    "print(\"EC1 desc1 shape = \", ec1_desc1.shape)\n",
    "print(\"EC1 desc2 shape = \", ec1_desc2.shape)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "ec1_matches = match_descriptors(ec1_desc1, ec1_desc2, 0.7)\n",
    "\n",
    "H, robust_matches = ransac(ec1_keypoints1, ec1_keypoints2, ec1_matches, threshold=1)\n",
    "print(\"Robust matches shape = \", robust_matches.shape)\n",
    "print(\"H = \\n\", H)\n",
    "\n",
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "print(\"Output shape:\", output_shape)\n",
    "print(\"Offset:\", offset)\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Merge the warped images using linear blending scheme\n",
    "merged = linear_blend(img1_warped, img2_warped)\n",
    "\n",
    "plt.imshow(merged)\n",
    "plt.axis('off')\n",
    "plt.title('Linear Blend')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('part2/solution_linear_blend.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Linear Blend Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea49cf",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.5 Stitching Multiple Images (10 points)\n",
    "Implement **`stitch_multiple_images`** in `panorama.py` to stitch together an ordered chain of images.\n",
    "\n",
    "Given a sequence of $m$ images ($I_1, I_2,...,I_m$), take every neighboring pair of images and compute the transformation matrix which converts points from the coordinate frame of $I_{i+1}$ to the frame of $I_{i}$. Then, select a reference image $I_{ref}$, which is the first or left-most image in the chain. We want our final panorama image to be in the coordinate frame of $I_{ref}$.\n",
    "\n",
    "You do **not** need to use linear blending for this problem: it's not included in the solution so the autograder does not expect it.\n",
    "\n",
    "*-Hint:*\n",
    "- If you are confused, you may want to review the Linear Algebra slides on how to combine the effects of multiple transformation matrices.\n",
    "- The inverse of transformation matrix has the reverse effect. Please use [`numpy.linalg.inv`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html) function whenever you want to compute matrix inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d7eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from part2.panorama import stitch_multiple_images\n",
    "\n",
    "# Set seed to compare output against solution\n",
    "np.random.seed(131)\n",
    "\n",
    "# Load images to be stitched\n",
    "ec2_img1 = imread('part2/yosemite1.jpg', as_gray=True)\n",
    "ec2_img2 = imread('part2/yosemite2.jpg', as_gray=True)\n",
    "ec2_img3 = imread('part2/yosemite3.jpg', as_gray=True)\n",
    "ec2_img4 = imread('part2/yosemite4.jpg', as_gray=True)\n",
    "\n",
    "imgs = [ec2_img1, ec2_img2, ec2_img3, ec2_img4]\n",
    "\n",
    "# Stitch images together\n",
    "panorama = stitch_multiple_images(imgs, desc_func=simple_descriptor, patch_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db68981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize final panorama image\n",
    "plt.imshow(panorama)\n",
    "plt.axis('off')\n",
    "plt.title('Stiched Images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ebe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imread('part2/solution_stitched_images.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Stiched Images Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52884d7",
   "metadata": {},
   "source": [
    "## <font color='blue'>Part 3: Exploration (15 points) </font>\n",
    "Weâ€™ve explored some of the basic approaches in geometry, now it is your turn to explore on your own! The goal here is to get you started working on using CV on your own. Weâ€™ve provided a list of resources and topic suggestions for inspiration and starting point, but feel free to deviate towards your interests!\n",
    "\n",
    "**Libraries:**\n",
    "\n",
    "Feel free to explore the following libraries and use functions that are helpful for your choice of exploration.\n",
    "- [OpenCV](https://opencv.org/): Provides a wide range of functionalities for real-time computer vision, such as image processing, video capture, and object detection, making it ideal for projects involving visual data analysis.\n",
    "- [Kornia](https://kornia.github.io/): A differentiable computer vision library for PyTorch, consists of a set of routines and differentiable modules to solve generic computer vision problems.\n",
    "- [Pytorch 3D](https://pytorch3d.org/): An extension of PyTorch designed for 3D data processing.\n",
    "\n",
    "**Exploration Idea Inspirations:**\n",
    "\n",
    "Check out bonus slides from lecture 4: Pinhole Camera Model for more details on the ideas.\n",
    "\n",
    "- Monocular 3D: do we see with our eyes or brain?\n",
    "- Other Camera Models: peek beyond the pinhole\n",
    "- Lenses: shining some light on this important piece\n",
    "- Vanishing Points++: infinitely more to reveal!\n",
    "- Calibration++: multi-plane, distortion, vanishing points \n",
    "- Single View Metrology: measure things from 2D\n",
    "\n",
    "\n",
    "**_We encourage you to start early on this part, and stop by OH if you have questions regarding your exploration idea and implementation._**\n",
    "\n",
    "_This section is intentionally open-ended, and any honest efforts towards exploration will be awarded full credit, with exceptional projects can receiving up to 3% extra credit. We encourage you to use other libraries and available resources on the internet (please do!), but please cite and external resources that you reference._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c9ea4",
   "metadata": {},
   "source": [
    "### Deliverables\n",
    "#### Write-up: \n",
    "A brief explanation of what you explored, how you went about implementing it, and how it relates to geometry. Please attach any images, figures, etc. If your writeup is long or includes many images/figures/equations, you may also upload to drive and include a link to your work. Please also explicitly cite your sources in the write-up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3cccae",
   "metadata": {},
   "source": [
    "**Your answer here:** Write your write-up in this markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee622917",
   "metadata": {},
   "source": [
    "#### Code:\n",
    "Please include any code that you wrote here. If your code contains other files, please make sure to also include them in the submission. If you have a lot of code, you may also upload to drive and include a link to your work.\n",
    "\n",
    "If you choose to do a mathematical proof or calculation, no need for code. Please specify and state so explicitly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b8602",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
