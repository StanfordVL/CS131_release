{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "This assignment covers methods for panorama stitching such as descriptor matching, transformation estimation, and RANSAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Setup\n",
    "import numpy as np\n",
    "from skimage import filters\n",
    "from skimage.feature import corner_peaks\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Panorama Stitching\n",
    "Panorama stitching is an early success of computer vision. Matthew Brown and David G. Lowe published a famous [panoramic image stitching paper](http://matthewalunbrown.com/papers/ijcv2007.pdf) in 2007. Since then, automatic panorama stitching technology has been widely adopted in many applications such as Google Street View, panorama photos on smartphones,\n",
    "and stitching software such as Photosynth and AutoStitch.\n",
    "\n",
    "In this assignment, we will detect and match keypoints from multiple images to build a single panoramic image. This will involve several tasks:\n",
    "1. Compare two sets of descriptors coming from two different images and find matching keypoints.\n",
    "2. Given a list of matching keypoints, use the least-squares method to find the affine transformation matrix that maps points in one image to another.\n",
    "3. Use RANSAC to give a more robust estimate of the affine transformation matrix. <br>\n",
    "   Given the transformation matrix, use it to transform the second image and overlay it on the first image, forming a panorama.\n",
    "4. Blend panorama images together to remove blurry regions of overlapping images.\n",
    "5. Stich multiple panorama images together.\n",
    "\n",
    "There's a lot of material to get hands-on with so we recommend starting early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Running This Notebook\n",
    "\n",
    "Make sure to run each Part from its begining to ensure that you compute all of the dependencies of your current question and don't crossover variables with the same name from other questions. So long as you run each Part from its beginning, you can run the Parts in any order.\n",
    "\n",
    "When assembling your PDF, we recommend running all cells in order from the top of the notebook to prevent any of these discontinuity errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Matching Keypoints (10 points)\n",
    "\n",
    "You are given a set of keypoints in two images (obtained by running the Harris corner detector). The question we want to answer is: How can we determine which pairs of keypoints come from the same 3D points projected on the two different images? In order to *match* the detected keypoints, we must come up with a way to *describe* the keypoints based on their local appearance. Generally, each region around detected keypoint locations is converted into  a fixed-size vector called a *descriptor*. \n",
    "\n",
    "We have implemented a simple descriptor function for you, where each keypoint is described by the normalized intensity of a small patch around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from panorama import harris_corners\n",
    "\n",
    "img1 = imread('uttower1.jpg', as_gray=True)\n",
    "img2 = imread('uttower2.jpg', as_gray=True)\n",
    "\n",
    "# Detect keypoints in two images\n",
    "keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                          threshold_rel=0.05,\n",
    "                          exclude_border=8)\n",
    "\n",
    "print(\"Keypoints 1 shape = \", keypoints1.shape)\n",
    "print(\"Keypoints 2 shape = \", keypoints2.shape)\n",
    "\n",
    "# Display detected keypoints\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.scatter(keypoints1[:,1], keypoints1[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 1')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2)\n",
    "plt.scatter(keypoints2[:,1], keypoints2[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 Matching Descriptors (10 points)\n",
    "Implement the **`match_descriptors`** function to find good matches in two sets of descriptors. First, calculate Euclidean distance between all pairs of descriptors from image 1 and image 2. Then use this to determine if there is a good match: for each descriptor in image 1, if the distance to the closest descriptor in image 2 is significantly (by a given factor) smaller than the distance to the second-closest, we call it a match. The output of the function is an array where each row holds the indices of one pair of matching descriptors.\n",
    "\n",
    "*Checking your answer*: you should see an identical matching of keypoints as the solution, but the precise colors of each line will change with every run of keypoint matching so colors do not need to match.\n",
    "\n",
    "*Optional ungraded food for thought*: Think about why this method of keypoint matching is not commutative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import simple_descriptor, match_descriptors, describe_keypoints\n",
    "from utils import plot_matches\n",
    "\n",
    "# Set seed to compare output against solution\n",
    "np.random.seed(131)\n",
    "\n",
    "patch_size = 5\n",
    "    \n",
    "# Extract features from the corners\n",
    "desc1 = describe_keypoints(img1, keypoints1,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "desc2 = describe_keypoints(img2, keypoints2,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=patch_size)\n",
    "\n",
    "print(\"Desc1 shape = \", desc1.shape)\n",
    "print(\"Desc2 shape = \", desc2.shape)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "matches = match_descriptors(desc1, desc2, 0.7)\n",
    "\n",
    "# Plot matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "ax.axis('off')\n",
    "plt.title('Matched Simple Descriptor')\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, matches)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_simple_descriptor.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Matched Simple Descriptor Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Transformation Estimation (20 points)\n",
    "\n",
    "We now have a list of matched keypoints across the two images. We will use this to find a transformation matrix that maps points in the second image to the corresponding coordinates in the first image. In other words, if the point $p_1 = [y_1,x_1]$ in image 1 matches with $p_2=[y_2, x_2]$ in image 2, we need to find an affine transformation matrix $H$ such that\n",
    "\n",
    "$$\n",
    "\\tilde{p_2}H = \\tilde{p_1},\n",
    "$$\n",
    "\n",
    "where $\\tilde{p_1}$ and $\\tilde{p_2}$ are homogenous coordinates of $p_1$ and $p_2$.\n",
    "\n",
    "Note that it may be impossible to find the transformation $H$ that maps every point in image 2 exactly to the corresponding point in image 1. However, we can estimate the transformation matrix with least squares. Given $N$ matched keypoint pairs, let $X_1$ and $X_2$ be $N \\times 3$ matrices whose rows are homogenous coordinates of corresponding keypoints in image 1 and image 2 respectively. Then, we can estimate $H$ by solving the least squares problem,\n",
    "\n",
    "$$\n",
    "X_2 H = X_1\n",
    "$$\n",
    "\n",
    "Implement **`fit_affine_matrix`** in `panorama.py`\n",
    "\n",
    "*-Hint: read the [documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html) about np.linalg.lstsq*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import fit_affine_matrix\n",
    "\n",
    "# Sanity check for fit_affine_matrix\n",
    "\n",
    "# Test inputs\n",
    "a = np.array([[0.5, 0.1], [0.4, 0.2], [0.8, 0.2]])\n",
    "b = np.array([[0.3, -0.2], [-0.4, -0.9], [0.1, 0.1]])\n",
    "\n",
    "H = fit_affine_matrix(b, a)\n",
    "\n",
    "# Target output\n",
    "sol = np.array(\n",
    "    [[1.25, 2.5, 0.0],\n",
    "     [-5.75, -4.5, 0.0],\n",
    "     [0.25, -1.0, 1.0]]\n",
    ")\n",
    "\n",
    "error = np.sum((H - sol) ** 2)\n",
    "\n",
    "if error < 1e-20:\n",
    "    print('Implementation correct!')\n",
    "else:\n",
    "    print('There is something wrong.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking that your `fit_affine_matrix` function is running correctly, run the following code to apply it to images.\n",
    "Images will be warped and image 2 will be mapped to image 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_output_space, warp_image\n",
    "\n",
    "# Extract matched keypoints\n",
    "p1 = keypoints1[matches[:,0]]\n",
    "p2 = keypoints2[matches[:,1]]\n",
    "\n",
    "# Find affine transformation matrix H that maps p2 to p1\n",
    "H = fit_affine_matrix(p1, p2)\n",
    "\n",
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "print(\"Output shape:\", output_shape)\n",
    "print(\"Offset:\", offset)\n",
    "\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 Warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 Warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the two warped images are merged to get a panorama. Your panorama may not look good at this point, but we will later use other techniques to get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two images\n",
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.title('Fit-Affine Panorama')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_fit_affine_panorama.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Fit-Affine Panorama Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 RANSAC (20 points)\n",
    "Rather than directly feeding all our keypoint matches into ``fit_affine_matrix`` function, we can instead use RANSAC (\"RANdom SAmple Consensus\") to select only \"inliers\" to use for computing the transformation matrix.\n",
    "\n",
    "The steps of RANSAC are:\n",
    "1. Select random set of matches\n",
    "2. Compute affine transformation matrix\n",
    "3. Find inliers using the given threshold\n",
    "4. Repeat and keep the largest set of inliers (use >, i.e. break ties by whichever set is seen first)\n",
    "5. Re-compute least-squares estimate on all of the inliers\n",
    "    \n",
    "In this case, use Euclidean distance between matched points as a measure of inliers vs outliers.\n",
    "\n",
    "Implement **`ransac`** in `panorama.py`, run through the following code to get a panorama. You can see the difference from the result we get without RANSAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import ransac\n",
    "\n",
    "# Set seed to compare output against solution image\n",
    "np.random.seed(131)\n",
    "\n",
    "H, robust_matches = ransac(keypoints1, keypoints2, matches, threshold=1)\n",
    "print(\"Robust matches shape = \", robust_matches.shape)\n",
    "print(\"H = \\n\", H)\n",
    "\n",
    "# Visualize robust matches\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, robust_matches)\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Matches')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_ransac.png'))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Matches Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the tranformation matrix $H$ computed using the robust matches to warp our images and create a better-looking panorama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped)\n",
    "plt.title('Image 1 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped)\n",
    "plt.title('Image 2 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two images\n",
    "merged = img1_warped + img2_warped\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = (img1_mask * 1.0 +  # Multiply by 1.0 for bool -> float conversion\n",
    "           img2_mask)\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "normalized = merged / np.maximum(overlap, 1)\n",
    "plt.imshow(normalized)\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Panorama')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_ransac_panorama.png'))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Robust Panorama Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Better Image Merging (10 points)\n",
    "You will notice the blurry region and unpleasant lines in the middle of the final panoramic image. Using a very simple technique called linear blending, we can smooth out a lot of these artifacts from the panorama.\n",
    "\n",
    "Currently, all the pixels in the overlapping region are weighted equally. However, since the pixels at the left and right ends of the overlap are very well complemented by the pixels in the other image, they can be made to contribute less to the final panorama.\n",
    "\n",
    "Linear blending can be done with the following steps:\n",
    "    1. Define left and right margins for blending to occur between\n",
    "    2. Define a weight matrix for image 1 such that:\n",
    "        - From the left of the output space to the left margin the weight is 1\n",
    "        - From the left margin to the right margin, the weight linearly decrements from 1 to 0\n",
    "    3. Define a weight matrix for image 2 such that:\n",
    "        - From the right of the output space to the right margin the weight is 1\n",
    "        - From the left margin to the right margin, the weight linearly increments from 0 to 1\n",
    "    4. Apply the weight matrices to their corresponding images\n",
    "    5. Combine the images \n",
    "\n",
    "In **`linear_blend`** in `panorama.py` implement the linear blending scheme to make the panorama look more natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import linear_blend\n",
    "\n",
    "img1 = imread('uttower1.jpg', as_gray=True)\n",
    "img2 = imread('uttower2.jpg', as_gray=True)\n",
    "\n",
    "# Set seed to compare output against solution\n",
    "np.random.seed(131)\n",
    "\n",
    "# Detect keypoints in both images\n",
    "ec1_keypoints1 = corner_peaks(harris_corners(img1, window_size=3),\n",
    "                              threshold_rel=0.05,\n",
    "                              exclude_border=8)\n",
    "ec1_keypoints2 = corner_peaks(harris_corners(img2, window_size=3),\n",
    "                              threshold_rel=0.05,\n",
    "                              exclude_border=8)\n",
    "\n",
    "print(\"EC1 keypoints1 shape = \", ec1_keypoints1.shape)\n",
    "print(\"EC1 keypoints2 shape = \", ec1_keypoints2.shape)\n",
    "\n",
    "# Extract features from the corners\n",
    "ec1_desc1 = describe_keypoints(img1, ec1_keypoints1,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=16)\n",
    "ec1_desc2 = describe_keypoints(img2, ec1_keypoints2,\n",
    "                           desc_func=simple_descriptor,\n",
    "                           patch_size=16)\n",
    "\n",
    "print(\"EC1 desc1 shape = \", ec1_desc1.shape)\n",
    "print(\"EC1 desc2 shape = \", ec1_desc2.shape)\n",
    "\n",
    "# Match descriptors in image1 to those in image2\n",
    "ec1_matches = match_descriptors(ec1_desc1, ec1_desc2, 0.7)\n",
    "\n",
    "H, robust_matches = ransac(ec1_keypoints1, ec1_keypoints2, ec1_matches, threshold=1)\n",
    "print(\"Robust matches shape = \", robust_matches.shape)\n",
    "print(\"H = \\n\", H)\n",
    "\n",
    "output_shape, offset = get_output_space(img1, [img2], [H])\n",
    "print(\"Output shape:\", output_shape)\n",
    "print(\"Offset:\", offset)\n",
    "\n",
    "# Warp images into output sapce\n",
    "img1_warped = warp_image(img1, np.eye(3), output_shape, offset)\n",
    "img1_mask = (img1_warped != -1) # Mask == 1 inside the image\n",
    "img1_warped[~img1_mask] = 0     # Return background values to 0\n",
    "\n",
    "img2_warped = warp_image(img2, H, output_shape, offset)\n",
    "img2_mask = (img2_warped != -1) # Mask == 1 inside the image\n",
    "img2_warped[~img2_mask] = 0     # Return background values to 0\n",
    "\n",
    "# Merge the warped images using linear blending scheme\n",
    "merged = linear_blend(img1_warped, img2_warped)\n",
    "\n",
    "plt.imshow(merged)\n",
    "plt.axis('off')\n",
    "plt.title('Linear Blend')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('solution_linear_blend.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Linear Blend Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 5 Stitching Multiple Images (10 points)\n",
    "Implement **`stitch_multiple_images`** in `panorama.py` to stitch together an ordered chain of images.\n",
    "\n",
    "Given a sequence of $m$ images ($I_1, I_2,...,I_m$), take every neighboring pair of images and compute the transformation matrix which converts points from the coordinate frame of $I_{i+1}$ to the frame of $I_{i}$. Then, select a reference image $I_{ref}$, which is the first or left-most image in the chain. We want our final panorama image to be in the coordinate frame of $I_{ref}$.\n",
    "\n",
    "You do **not** need to use linear blending for this problem: it's not included in the solution so the autograder does not expect it.\n",
    "\n",
    "*-Hint:*\n",
    "- If you are confused, you may want to review the Linear Algebra slides on how to combine the effects of multiple transformation matrices.\n",
    "- The inverse of transformation matrix has the reverse effect. Please use [`numpy.linalg.inv`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html) function whenever you want to compute matrix inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import stitch_multiple_images\n",
    "\n",
    "# Set seed to compare output against solution\n",
    "np.random.seed(131)\n",
    "\n",
    "# Load images to be stitched\n",
    "ec2_img1 = imread('yosemite1.jpg', as_gray=True)\n",
    "ec2_img2 = imread('yosemite2.jpg', as_gray=True)\n",
    "ec2_img3 = imread('yosemite3.jpg', as_gray=True)\n",
    "ec2_img4 = imread('yosemite4.jpg', as_gray=True)\n",
    "\n",
    "imgs = [ec2_img1, ec2_img2, ec2_img3, ec2_img4]\n",
    "\n",
    "# Stitch images together\n",
    "panorama = stitch_multiple_images(imgs, desc_func=simple_descriptor, patch_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize final panorama image\n",
    "plt.imshow(panorama)\n",
    "plt.axis('off')\n",
    "plt.title('Stiched Images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imread('solution_stitched_images.png'))\n",
    "plt.axis('off')\n",
    "plt.title('Stiched Images Solution')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
